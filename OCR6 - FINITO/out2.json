{
    "1 - Introduction to RL and MDPs.pdf": "Reinforcement Learning\nAn Introduction\n\nInstructor\nHussam ATOUI\n●\nSoftware Engineer at Valeo \nCréteil (France) since Nov 2022\n●\nPhD-Cifre RENAULT & \nGrenoble-Alpes University \n(2019-2022)\n●\nSpecialities: Automated Driving, \nReinforcement Learning, \nAutomatic Control, Optimization\n\nInstructor\nVictor MORAND\nmorand@isir.upmc.fr\n●\nPhD Student at ISIR (Sorbonne - \nCNRS)\n○\nExplaining how LLMs manipulate \nKnowledge\n○\ntowards AIs that know what they \nknow\n●\nAlso out of a School of Engineering !\nFeel free to reach out at the end of our \nsessions ! \n\nCourse Content\n1.\nIntroduction to Reinforcement Learning\n2.\nMarkov Decision Processes (MDPs)\n3.\nPolicy and Value Functions\n4.\nDynamic Programming (DP) for RL\n5.\nModel-Free methods\n6.\nValue Function Approximation\n7.\nPolicy-Gradient and Actor-Critic Methods\n8.\nDeep RL\n9.\nTP Project\n\nIntroduction to Reinforcement \nLearning (RL)\n\nSupervised Learning\nReinforcement Learning\nUnsupervised Learning\nTrain with labeled data\nTrain with unlabeled data\nClustering\nTrain with environment \nexperience\nRegression\nClassiﬁcation\n6\nTypes of Learning\n\nSupervised Learning \nModel\nInputs:\nFeatures / States\nPredicted Outputs:\nValue/ Class\nTraining target:\nTarget Output\n●\nError: Target Output - Predicted Output\n●\nObjective: Minimize the error between the target and the predicted output\n7\nSupervised Learning\n\nSupervised Learning\n\nReinforcement Learning\nReinforcement \nLearning Model\nInputs:\nFeatures / States\nPredicted Outputs:\nActions\nEvaluation:\nRewards / Penalties\n●\nError: Awards - Penalties \n●\nObjective: Maximize the awards and decrease penalties as much as possible\n9\n\nReinforcement Learning\n\nExamples of Rewards [1]\n●\nFly stunt manoeuvres in a helicopter\n○\n+ve reward for following desired trajectory\n○\n−ve reward for crashing\n●\nManage an investment portfolio\n○\n+ve reward for each $ in bank\n●\nControl a power station\n○\n+ve reward for producing power\n○\n−ve reward for exceeding safety thresholds\n●\nMake a humanoid robot walk\n○\n+ve reward for forward motion\n○\n−ve reward for falling over\n●\nPlay many different Atari games better than humans\n○\n+/−ve reward for increasing/decreasing score\n11\n\nAgent and Environment\nActions   A(t)\nObservations   O(t) \nRewards   R(t) \nAgent\nEnvironment\nAt step t: \nThe Agent:\n●\nReceives O(t)\n●\nReceives R(t)\n●\nExecutes A(t)\nThe Environment:\n●\nReceives A(t)\n●\nEmits O(t+1)\n●\nEmits R(t+1)\nt++\n12\n\nFully Observable Environment\nObservations   O(t) \nAgent\nEnvironment\n●\nEnvironment observations = Agent \nstate\n●\nThis is assumed in Markov Decision \nProcess (MDP)\n13\n\nPartially Observable Environment\nObservations   O(t) \nAgent\nEnvironment\n●\nEnvironment observations ≠ Agent state\n○\nA drone navigating a forest only sees \nnearby obstacles.\n○\nA healthcare agent observes patient \nsymptoms but not the underlying \ndisease.\n○\nA self-driving car detects nearby \nvehicles but not hidden pedestrians.\n○\nA weather forecasting model \nobserves recent conditions but not \nfuture patterns\n●\nThis is called Partially Observable Markov \nDecision Process (POMDP)\n(Missing info)\n14\n\nReinforcement Learning\n15\nAgent: The system that takes \nactions to be trained.\nEnvironment: The external \nsystem with which the agent \ninteracts.\nState: The information \nrequired by the agent to take \nan action. This info is observed \nfrom the environment.\nAction: The decision or \nmove that the agent makes \nat a particular state\nReward: Feedback received \nby the agent to evaluate the \ntaken action under a certain \nstate.\nGeneral Architecture\n\n16\nReinforcement Learning\n\nPolicy\nRL Agent\nA policy deﬁnes the agent’s behavior in the environment. It \nrepresents a mapping from states to actions, for example:\n●\nDeterministic policy:                   , \nwhere the action a is chosen directly based on state s.\n●\nStochastic policy:                                                   ,\nwhere the policy gives the probability of taking action \na given state s.\n17\n\nValue Function\nRL Agent\nA value function \n●\nestimates the expected future reward \n●\nassesses the quality of states, helping to determine the best actions to \ntake. \nFor example, the state value under policy 𝜋 is given by:\nThis equation expresses the expected sum of discounted rewards starting from \nstate 𝑠.\n18\n\nValue Function\nRL Agent\nA value function \n●\nestimates the expected future reward \n●\nassesses the quality of states, helping to determine the best actions to \ntake. \nFor example, the state value under policy 𝜋 is given by:\nThis equation expresses the expected sum of discounted rewards starting from \nstate 𝑠.\n19\n\nValue Function\nRL Agent\n20\nγ∈[0,1]:\n●\nIf γ=0, the agent focuses solely on immediate rewards.\n●\nIf γ=1, future rewards are valued equally to immediate rewards.\n\nModel\nRL Agent\nA model forecasts the environment's next state and expected reward:\n●\n𝑃 represents the probability of the next state given the current state and \naction:\n●\n𝑅 represents the expected immediate reward given the current state and \naction:\n21\n\nStates, Actions, Rewards\nExample: Maze [1]\n●\nStates: Agent’s location\n●\nActions: Right, Left, Up, Down\n●\nRewards: -1 per time-step\n22\n\nPolicy\nExample: Maze [1]\nArrows represent policy π(s) for \neach state s\n23\n\nExample: Maze [1]\nNumbers represent value           of \neach state s\n24\n\nDifferent Types\nRL Agents\n➔\nValue-based:\n◆\nNo Policy\n◆\nValue Function\n➔\nPolicy-based:\n◆\nPolicy\n◆\nNo Value Function\n➔\nActor-Critic:\n◆\nPolicy\n◆\nValue Function\n➔\nModel-free:\n◆\nPolicy and/or Value Function\n◆\nNo Model\n➔\nModel-based:\n◆\nPolicy and/or Value Function\n◆\nModel\n25\n\nMarkov Decision Processes (MDPs)\n\nMarkov Process\n●\nA Markov Process is a memoryless process where the future state depends only \non the current state and not on any past states.\n●\nFormally, a Markov Process is a tuple: M=(S,P)\nWhere:\n●\nS: A ﬁnite set of states.\n●\nP: Transition probabilities between states, deﬁned as:\n27\n\nThe Markov Property\n●\nMarkov property: Future depends only on the present, not past states\n●\nSimpliﬁes state transition modeling\n28\n\n●\nA Markov Reward Process is a Markov Process with added rewards.\n●\nIt is represented as a tuple: MR=(S,P,R,γ)\nWhere:\n●\nR(s): Reward function providing the expected reward at each state s, \n●\nγ: Discount factor, controlling the importance of future rewards.\n29\nMarkov Reward Process\n\nCumulative Reward - Gain\nMarkov Reward Process\n●\nCumulative Reward G(t) : Expected cumulative reward from state s\n30\n●\n(State-)Value Function v(s) : Expected state-value of state s\n\nState-Value Function\nBellman Equation\n●\nThe state-value function can be presented as an immediate reward and future reward \nas follows:\nPROOF?\n31\n\nProof\nBellman Equation\nStochastic Eq.\n?\n32\n\nExample\n\nExample: Student MRP (P, S, R) [1] \n34\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n35\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n36\n\nExercise\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n38\n?\n\nExample of Bellman’s equation\nExample: Student MRP (P, S, R) [1]  \n39\n\nMRP → MDP\n(P, S, R) → (P, S, A, R)\n\nMarkov Decision Process (MDP)\n A Markov decision process is a 4-tuple (S, A, P, R):\nNote: A ﬁnite MDP is an MDP with ﬁnite state, action, and reward \nsets. Much of the current theory of reinforcement learning is \nrestricted to ﬁnite MDPs.\n●\nStates (S): Describe environment situations\n●\nActions (A): Choices available to the agent\n●\nRewards (R): Immediate feedback for actions\n●\nTransition Probabilities (P): Likelihood of reaching a \nnew state\n41\n\nState Transitions - Policy\nMarkov Decision Process\n●\nTransition probability: P(s′∣s,a)\n●\nModels probability of moving to s′ from s after action a\n42\n\nReward Function and Policy\nMarkov Decision Process\n●\nReward function R(s,a): Immediate feedback\n●\nPositive rewards encourage actions; negative prevent actions\n●\nDeterministic policy:                   , \nwhere the action a is chosen directly based on state s.\n●\nStochastic policy:                                                   ,\nwhere the policy gives the probability of taking action a given \nstate s.\n43\n\nPolicies\nMarkov Decision Process\n●\nDeterministic policy: \n                  \nwhere the action a is chosen directly based on state s.\n●\nStochastic policy:         \n                                          \nwhere the policy gives the probability of taking action a given state s.\n44\n\nExample: Student MDP (P, S, A, R) [1]  \n45\n\nValue Functions\nMarkov Decision Process\n●\nState-value function : Expected cumulative reward from state s under policy π\n46\n●\nAction-value function: Expected reward of taking action a in state s under policy π\n\nState-Value Function\nMarkov Decision Process\n47\n\nBellman Expectation Equation\n48\n●\nState-value function : Expected cumulative reward from state s under policy π\n●\nAction-value function: Expected reward of taking action a in state s under policy π\n\nBellman Expectation Equation [1]\n49\n\nBellman Expectation Equation [1]\n50\n\nBellman Expectation Equation [1]\n51\n\nBellman Expectation Equation [1]\n52\n\nExercise\n\nExample: Student MDP\n54\n?\n\nExample: Student MDP\n55\n\nState-Value and Action-Value Functions\nBellman Optimality\n●\nThe optimal state-value function\n●\nOptimal action-value function\n56\n\nExercise: Optimal State-Value Function [1]\n57\n\nExercise: Optimal Action-Value Function [1]\n58\n\nFind an Optimal Policy\n●\nAn optimal policy π∗ can be determined by selecting actions that \nmaximize the optimal action-value function q∗(s,a). The optimal policy π∗\n(a∣s) is deﬁned as:\n●\nFor any MDP, there is always a deterministic optimal policy. If q∗(s,a) is \nknown, we can directly derive the optimal policy from it.\n59\n\nExercises\n\nExercise 1: Understanding Policies\nQuestion:\nLet S={s1,s2} be a set of two states and A={a1,a2} be a set of two actions. Suppose a \nstochastic policy π is deﬁned as follows:\n1.\nWhat is the probability of taking action a2  in state s1  under this policy?\n2.\nIf the agent is in state s2 , what is the probability of taking action a1  under this \npolicy?\n61\n\nExercise 1: Understanding Policies\nSolution:\n1.\nThe probability of taking action a2  in state s1  is given directly by π(a2 |s1 )=0.3\n2.\nThe probability of taking action a1  in state s2  is given by π(a1 ∣s2 )=0.4\n62\n\nExercise 2: State-Value Function\nQuestion:\nConsider a simple MDP with two states s1  and s2  and a single action a with the \nfollowing reward structure:\n●\nStarting from s1  and taking action a, the agent moves to s2  with a reward of 5.\n●\nStarting from s2  and taking action a, the agent stays in s2  and receives a \nreward of 3.\nAssuming a discount factor γ=0.9 and a deterministic policy where action a is \nalways taken, compute the value of each state v(s1 ) and v(s2 ).\n63\n\nExercise 2: State-Value Function\nSolution:\nThe Bellman equation for the value of each state s is:\n1.\nFor s2 :\nSolving for v(s2 ) → v(s2)=30\n2.\nFor s1 :\nThus, v(s1)=32 and v(s2)=30.\n64\n\nExercise 3: Action-Value Function\nQuestion:\nUsing the same MDP setup as in Exercise 2, calculate the action-value q(s1 ,a) and \nq(s2 ,a) for each state-action pair.\n65\n\nExercise 3: Action-Value Function\nSolution:\nThe Bellman equation for the action-value function is:\nUsing the state values calculated in Exercise 2:\n66\n\nExercise 4: Bellman Optimality Equation\nQuestion:\nSuppose we have an MDP with three states S={s1,s2,s3} and two actions A={a1,a2}. \nThe reward function and transitions are given below:\n●\nFrom s1  taking a1  leads to s2  with reward 4.\n●\nFrom s1  taking a2  leads to s3  with reward 2.\n●\nFrom s2  taking a1  leads to s3  with reward 5.\n●\nFrom s3  taking a1  or a2  leads back to s3  with reward 3.\nAssuming a discount factor γ=0.9, write the Bellman optimality equation for v∗(s1 ).\n67\n\nExercise 4: Bellman Optimality Equation\n68\nSolution:\nThe Bellman optimality equation for the state-value function is:\nSubstituting the rewards:\nTo solve this, we would need the values of v*(s2) and v*(s3), which can be calculated \nrecursively by applying the Bellman optimality equation to each state.\n\nExercise 4: Bellman Optimality Equation\n69\nSolution:\nThe optimal values for each state are: \n●\nv*(s1) = 32.8\n●\nv*(s2) = 32\n●\nv*(s3) = 30\n\nExercise 5: Optimal Policy Derivation\nQuestion:\nIf the optimal action-value function q∗(s,a) for some state s is given by:\n●\nq∗(s,a1 )=12\n●\nq∗(s,a2 )=10\nWhat is the optimal policy π∗(a∣s)?\n70\n\nExercise 5: Optimal Policy Derivation\nSolution:\nThe optimal policy π∗(a∣s) chooses the action that maximizes q∗(s,a).\nSo:\n \nThus, the optimal policy is to always choose action a1  in state s, since q∗(s,a1 )>q∗\n(s,a2 ).\n71\n\nExploration & Exploitation\n\nExploration vs. Exploitation\n●\nIn RL, the agent faces a dilemma between:\n○\nExploration: Trying new actions to discover valuable \noutcomes. (can be harmful…)\n○\nExploitation: Choosing actions that have yielded high \nrewards in the past.\n73\n●\nGoal: Balance exploration and exploitation to maximize rewards over time.\n●\nChallenge: Too much exploration can delay achieving rewards, while too \nmuch exploitation can lead to suboptimal long-term results.\n\nExploration: Discovering New Opportunities\n●\nExample 1 - A robot navigating a maze:\n○\nThe robot tries unfamiliar paths to locate shorter routes or more valuable \nrewards.\n●\nExample 2 - A recommendation system:\n○\nOccasionally recommends new, lesser-known products to a user to \nlearn their interests.\n●\nBeneﬁt: Exploration can uncover higher rewards that aren’t immediately \nobvious.\n74\n\nExploitation: Leveraging Known Information\n●\nExample 1 - A trading agent:\n○\nSelects stocks it has previously identiﬁed as proﬁtable, prioritizing \nconsistency over discovering new options.\n●\nBeneﬁt: Exploitation capitalizes on known successes, ensuring steady rewards.\n●\nExample 2 - A game-playing AI:\n○\nRepeats a high-reward move (e.g., a chess opening) that has led to victories \nin past games.\n75\n\nAny Questions ? \nDon’t hesitate to contact me\nmorand@isir.pmc.fr\n\nAny Questions ? \nContact us !\nhussam.atoui@valeo.com\n\nReferences\n78\n[1] David Silver, Lectures on Reinforcement Learning, 2015\n[2] Reinforcement Learning and Advanced Deep Learning (Sorbonne) - Olivier \nSigaud\n[3] Sutton, R. S. and Barto, A. G. (2018), Reinforcement Learning: An Introduction \n(Second edition). MIT Press\n \nOlivier Sigaud Youtube Channel\n",
    "19Thales.pdf": "1 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \n𝐴𝐵′\n𝐴𝐵    =    𝐴𝐶′\n𝐴𝐶    =    𝐵′𝐶′\n𝐵𝐶 \nTHÉORÈME DE THALÈS \n Tout le cours en vidéo : https://youtu.be/puuHhlf0jAQ \n \n \nThalès serait né autour de 625 avant J.C. à Milet en Asie Mineure (actuelle Turquie). Considéré comme \nl'un des sept sages de l'Antiquité, il est à la fois mathématicien, ingénieur, philosophe et homme d'Etat \nmais son domaine de prédilection est l'astronomie.  \nIl aurait prédit avec une grande précision l'éclipse du soleil du 28 mai de l'an - 585. Ce n'est peut-être \nqu'une légende, Thalès en explique cependant le phénomène. \nCurieusement, le fameux théorème de Thalès n'a pas été découvert par Thalès. Il était déjà connu \navant lui des babyloniens et ne fut démontré qu'après lui par Euclide d'Alexandrie. \n \n \n \n \nPartie 1 : Le théorème de Thalès « version triangles emboîtés » (Rappel) \n \nAnimation : http://www.maths-et-tiques.fr/telech/Thales4.ggb \n \nLE THÉORÈME DE THALÈS \n \nSoit deux triangles 𝐴𝐵𝐶 et 𝐴𝐵’𝐶’, tels que : \n𝐴, 𝐵, 𝐵’ et 𝐴, 𝐶, 𝐶’ sont alignés. \n \nSi (𝐵’𝐶’)//(𝐵𝐶) \n \nalors :  \n!\"!\n!\" = \n!#!\n!# =\n\"!#!\n\"#  \n \n  \n \nComment retenir le théorème de Thalès ? \n \n𝐴𝐵𝐶 et 𝐴𝐵’𝐶’ sont deux triangles en situation de Thalès : ils ont un sommet commun 𝐴, et deux côtés \nparallèles (𝐵’𝐶’) et (𝐵𝐶). \nUn triangle est un « agrandissement » de l’autre. Ils ont donc des côtés deux à deux proportionnels.  \nOn obtient la formule de Thalès : \n \n        Le petit triangle 𝐴𝐵’𝐶’ \n \n        Le grand triangle 𝐴𝐵𝐶 \n \n \n \n \n       \n           1ers côtés             2èmes côtés              3èmes côtés \n \n \nSavoir utiliser : http://www.maths-et-tiques.fr/telech/thales_ecrire.pdf \n \n \n \n\n2 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \nMéthode : Calculer une longueur à l’aide du théorème de Thalès  \n \n Vidéo https://youtu.be/zP16D2Zrv1A \n \nSur la figure ci-dessous, les triangles 𝐵𝐶𝐹 et 𝐵𝐷𝐸 sont tels que (𝐶𝐹) et (𝐷𝐸) sont parallèles.  \nCalculer : a) 𝐵𝐸    b) 𝐵𝐷 \nDonner la valeur exacte et éventuellement l’arrondi au dixième. \n \n \n \n \n \n \n \n \n \n \n \n \nCorrection \n \na) Les triangles 𝐵𝐶𝐹 et 𝐵𝐷𝐸 sont en situation de Thalès car (𝐶𝐹) // (𝐷𝐸), donc : \n \n                   𝐵𝐶\n𝐵𝐷= 𝐵𝐹\n𝐵𝐸= 𝐶𝐹\n𝐷𝐸 \n \n                   4\n𝐵𝐷= 4,5\n𝐵𝐸= 3\n7 \n \n                               4,5\n𝐵𝐸= 3\n7 \nSoit : 𝐵𝐸 =  4,5 × 7 ∶3 = 10,5   \n \nb) On a :  \n$\n\"% =\n$,'\n\"( =\n)\n*  \n               \n                               4\n𝐵𝐷= 3\n7 \nSoit : 𝐵𝐷= 4 × 7 : 3 = \n+,\n)  (Valeur exacte) \n                                       » 9,3 (Valeur arrondie) \n \n \n \n \n \n \n \nE \nD \nC \nB \nF \n7 \n3 \n4,5 \n4 \n× \n: \n\n3 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \nPartie 2 : Le théorème de Thalès « version papillon » \n \nAnimation : http://www.maths-et-tiques.fr/telech/Thales.ggb \n \n \n \nLE THÉORÈME DE THALÈS \n \nSoit deux triangles 𝐴𝐵𝐶 et 𝐴𝐵’𝐶’, tels que : \n𝐴, 𝐵, 𝐵’ et 𝐴, 𝐶, 𝐶’ sont alignés. \n \nSi (𝐵’𝐶’)//(𝐵𝐶) \n \nalors :  \n!\"!\n!\" = \n!#!\n!# =\n\"!#!\n\"#  \n \n \n \n \n \n \n \n \nMéthode : Calculer une longueur à l’aide du théorème de Thalès  \n \n Vidéo https://youtu.be/cq3wBbXYB4A  \n \nLes triangles 𝐵𝐴𝐸 et 𝐵𝐷𝐶 sont tels que les droites  \n(𝐴𝐸) et (𝐶𝐷) sont parallèles. \nOn donne : 𝐵𝐸= 2 𝑐𝑚, 𝐵𝐷= 5 𝑐𝑚, et 𝐶𝐷= 6 𝑐𝑚. \nCalculer 𝐴𝐸.  \n \n \nCorrection \nLes triangles 𝐵𝐴𝐸 et 𝐵𝐷𝐶 sont en situation de Thalès car (𝐴𝐸) et (𝐶𝐷) sont parallèles, donc : \n \n     𝐵𝐴\n𝐵𝐶= 𝐵𝐸\n𝐵𝐷= 𝐴𝐸\n𝐶𝐷 \n \n     𝐵𝐴\n𝐵𝐶= 2\n5 = 𝐴𝐸\n6  \n   \n          2\n5 = 𝐴𝐸\n6  \n \nEt donc 𝐴𝐸= 6 × 2 : 5 = 2,4 𝑐𝑚.  \n \nActivités de groupe : Le paradoxe de Lewis Carroll \nhttp://www.maths-et-tiques.fr/telech/L_CARROLL.pdf \n \nC’ \nB’ \nA \nB \nC \nE \nD \nC \n \nB \nA \n\n4 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \n \n \nDes hauteurs inaccessibles \nhttp://www.maths-et-tiques.fr/telech/haut_inacc.pdf \nhttp://www.maths-et-tiques.fr/index.php/expositions-deleves/hauteurs-inaccessibles \n \n \n \nPartie 3 : La réciproque du théorème de Thalès \n \nAnimation : http://www.maths-et-tiques.fr/telech/RThales.ggb \n \n \nLA RÉCIPROQUE DU THÉORÈME DE THALÈS \n \n \n     Si les points 𝐴, 𝐵, 𝐵’ sont alignés dans  \n     le même ordre que les points 𝐴, 𝐶, 𝐶’  \n     et \n!\"!\n!\" =\n!#!\n!#  \n                      \n                        alors (𝐵’𝐶’)//(𝐵𝐶) \n         \n       Thalès de Milet (-624 ; -546) \n \nVersion « triangles emboités » \n \n \nVersion « papillon » \n \n \n \n \n \n \n \n \n \n \n \n \n \nMéthode : Démontrer que deux droites sont parallèles  \n Vidéo https://youtu.be/uaPicwUSQz0 \n \nSur la figure ci-contre, les points 𝐴, 𝐶, 𝐸 sont alignés et les  \npoints 𝐵, 𝐶, 𝐷 sont également alignés dans le même ordre. \nLes droites (𝐴𝐵) et (𝐷𝐸) sont-elles parallèles ?  \n \n \n \n \nA \nB’ \nB \nC’ \nC \nC’ \nB’ \nA \nB \nC \nB \nC \n \nD \nE \nA \n3 \n4,5 \n6 \n4 \n\n5 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \nCorrection \n● D’une part : \n#!\n#( =\n)\n$ = 0,75 \n● D’autre part : \n#\"\n#% =\n$,'\n- = 0,75 \n \nDonc : \n#!\n#( =\n#\"\n#% \nDe plus les points 𝐴, 𝐶, 𝐸 sont alignés dans le même ordre que les points 𝐵, 𝐶, 𝐷. \nD’après la réciproque du théorème de Thalès, on peut conclure que les droites (𝐴𝐵) et (𝐷𝐸) \nsont parallèles. \n \nMéthode : Démontrer que deux droites ne sont pas parallèles \n \n Vidéo https://youtu.be/ovlhagzONlw \n \n                                         \n                                                            Les droites (𝑃𝑅) et (𝐷𝐸) sont-elles parallèles ? \n \n \n                                          \n \n \n \nCorrection \n \n• D’une part : \n#.\n#% =\n$\n- ≈0,67 \n• D’autre part : \n#/\n#( =\n+,'\n$ = 0,625 \nDonc :  \n#.\n#% ≠\n#/\n#(\n \nOn ne peut pas utiliser la réciproque du théorème de Thalès. \n(𝑃𝑅) et (𝐷𝐸) ne sont pas parallèles. \n \n \n \nLors d’un voyage en Egypte, Thalès de Milet (-624 ; -546) aurait mesuré la hauteur de la pyramide de \nKheops par un rapport de proportionnalité avec son ombre. \nCitons : « Le rapport que j’entretiens avec mon ombre est le même que celui que la pyramide entretient \navec la sienne. » \nPar une relation de proportionnalité, il obtient la hauteur de la pyramide grâce à la longueur de son \nombre. \nL'idée ingénieuse de Thalès est la suivante : « A l’instant où mon ombre sera égale à ma taille, l'ombre de \nla pyramide sera égale à sa hauteur. » \n \n \nHors du cadre de la classe, aucune reproduction, même partielle, autres que celles prévues à l'article L 122-5 du code de \nla propriété intellectuelle, ne peut être faite de ce site sans l'autorisation expresse de l'auteur. \nwww.maths-et-tiques.fr/index.php/mentions-legales \n \n",
    "h1-2-les-regimes-totalitaires.pdf": "H1-2 LES REGIMES TOTALITAIRES  \nA LA RECHERCHE D’UNE NOUVELLE GEOPOLITIQUE EUROPEENNE \n \nIntroduction : La Première Guerre mondiale, par la brutalisation des sociétés, est un bouleversement profond pour \nles pays européens. Si en Russie, les révolutions aboutissent à la mise en place progressive d’un régime communiste, \nla Première Guerre mondiale fragilise les démocraties. Par ses traités, elle fait naître en Allemagne et en Italie des \nmouvements politiques antidémocratiques violents qui prospèrent sur les difficultés économiques et la montée du \ncommunisme. Des régimes que l’on qualifiera plus tard de totalitaires se mettent en place en Russie, en Italie et en \nAllemagne. \nProblématique : Pourquoi qualifier les régimes russe, allemand et italien de totalitaires et comment ont-ils fait \nbasculer l’Europe et le monde dans la guerre ?  \nI. \nDes racines et des pratiques communes. \nA. \nDes régimes issus de la brutalisation des sociétés. \nL’impact de la Grande Guerre : Le régime communiste qui s’installe progressivement en Russie est né dans le contexte \nde la Première Guerre mondiale, avec le soulèvement de la population et des mutineries qui aboutissent à la chute du \nTsar Nicolas II, remplacé par un gouvernement provisoire qui est à son tour renversé par un coup d’état construit par \nles Bolcheviks, mouvement minoritaire communiste dirigé par Lénine (octobre 1917). En Italie, Benito Mussolini prend \nles rênes du PNF, parti national fasciste dans un pays qui se sent trahi par la « victoire mutilée » et réclame les terres \nirredente qu’elle n’a pas obtenu à l’issue des traités de paix. Alors que les grèves se multiplient, Mussolini s’appuie sur \nles Squadre ou chemises noires (groupe paramilitaire majoritairement composé de soldats démobilisés qui mènent \ndes opérations violentes) pour créer un climat de violence favorable à sa nomination à la tête de l’Italie (marche sur \nRome en octobre 1922). En Allemagne, la République de Weimar, née en 1918, est considérée par les partis politiques \nd’extrême droite comme responsable de la défaite et de l’humiliation de Versailles. \nLe poids de la crise économique : L’autre racine commune à ces régimes totalitaires est la misère. En Russie, la \nPremière Guerre mondiale, puis la guerre civile qui oppose les communistes aux « Blancs » entre 1917 et 1922, créé \nles conditions nécessaires à l’établissement d’un régime violent dans un pays déjà marqué par une grande misère. La \ncrise économique très grave que connaît l’Italie après la Première Guerre mondiale est aussi un facteur expliquant \nl’arrivée au pouvoir de Mussolini. Mais c’est en Allemagne que le poids de la crise économique est le plus fort dans la \nmise en place du régime totalitaire. Si le NSDAP, le parti nazi est présent dès les années 20 en Allemagne, il reste \nmarginal et ne connaît pas de succès politiques majeurs avant l’arrivée de la crise économique de 1929 en Allemagne \nqui créé les conditions favorables à la nomination d’Hitler comme chancelier en janvier 1933, dans un contexte de \nviolence extrême organisée par les SA (sturmabteilung, organisation paramilitaire issue du parti nazi) et de lutte avec \nleurs principaux opposants, le parti communiste allemand. \nB. \nUne société contrôlée. \nUn culte du chef organisé par une propagande puissante : Dans les régimes totalitaires, la place du chef est centrale \net un véritable culte se met en place autour de ces guides, de ces leaders qu’il faut écouter et suivre aveuglément. De \nmanière différente, ils occupent chacun l’espace médiatique grâce à une propagande savamment orchestrée par des \nhommes comme Joseph Goebbels, le ministre allemand de l’information et de la Propagande. Le Duce, le Vojd ou le \nFührer sont omniprésents, ne se trompent jamais et se sacrifient pour leur nation, invitant les populations à faire de \nmême. De grandes manifestations sportives ou politiques sont organisées, mettant en scène cette adhésion populaire \ncomme à Nuremberg ou bien encore lors des festivités de la révolution d’octobre sur la Place Rouge à Moscou. \nDes sociétés encadrées : Dans les états totalitaires, l’individu doit s’effacer au nom de la construction d’une société \nnouvelle, d’un homme nouveau. Chaque moment de la vie sociale est encadré par une organisation issue du parti \nunique : les organisations de jeunesse (Jeunesses hitlériennes, Balilla et Avant-gardistes, Pionniers et Komsomols), le \n\ntravail (Front du travail, soviet), la vie sociale et politique au sein du parti qui devient un ascenseur social (PCUS, parti \nfasciste, parti nazi). En Allemagne, l’organisation Kraft durch Freude (la Force par la joie) organise les temps libres et \nles vacances des travailleurs.  L’ensemble de la vie politique est politisé et les résistances restent faibles, malgré \ncertaines tentatives au sein des milieux catholiques en Italie et en Allemagne et orthodoxes en URSS. \n \nC. \nUne économie dirigée. \nLe dirigisme d’état allemand et italien : En Italie, Mussolini lance des programmes ambitieux pour faire de son pays \nune grande puissance industrielle et agricole autour de l’assèchement des terres insalubres. Avec le contrecoup de la \ncrise économique, le dirigisme italien s’accentue. L’IRI (institut pour la reconstruction industrielle) est créé en 1933 et \ncontrôle une large part de l’industrie italienne. En Allemagne, le même schéma de politiques de grands travaux se met \nen place, mise en lumière par une propagande intensive. L’autre chemin choisi par l’Allemagne et l’Italie est celui de \nl’autarcie, c’est-à-dire d’une économie fermée. La bataille de l’emploi n’est gagnée en Allemagne que dans la cadre du \nréarmement (en 1939, les 2/3e du revenu national sont consacrés au réarmement) et au prix d’une baisse importante \ndu pouvoir d’achat et d’un pillage industriel des pays annexés. Mais le recul du chômage explique en partie l’adhésion \nde la population aux régimes totalitaires. \nLa politique économique de Staline : En 1929, Staline lance son pays dans une grande réforme économique : il s’agit \nde faire de l’URSS une grande puissance industrielle. Pour y arriver, l’État nationalise l’ensemble de l’économie et fixe \ndes objectifs de production (planification) à l’industrie lourde. Dans un pays largement agricole, l’agriculture est \nsacrifiée pour financer l’industrialisation forcée. La propriété privée est supprimée et les terres sont regroupées dans \ndes fermes collectives d’État (sovkhozes). La désorganisation totale du monde agricole entraîne des famines, \nattribuées aux koulaks (paysans opposés à la collectivisation) mais le régime glorifie les nouveaux héros comme \nStakhanov, mineur qui aurait produit 14 fois plus que les objectifs. \nII. \nDes idéologies différentes qui légitiment la violence. \nA. \nLe socialisme soviétique. \nLe socialisme soviétique : Le parti bolchevik qui prend le pouvoir à partir de 1917 sous la tutelle de Lénine s’appuie \nsur le communisme. Il s’agit d’une idéologie qui vise à la création d’une société égalitaire sans classe. Pour y parvenir, \nles ouvriers (les prolétaires) doivent faire la révolution et imposer des réformes : c’est la dictature du prolétariat. Pour \nles soviétiques, le communisme doit s’établir dans le monde entier et l’URSS doit aider à la mise en place d’une \nrévolution mondiale. C’est le rôle du Komintern, organisation internationale communiste dont le siège est à Moscou.  \nStaline, qui s’empare progressivement du pouvoir en 1927, pousse à la naissance de l’Homme nouveau. \nLa Terreur stalinienne au cœur de l’idéologie : Tous ceux qui s’opposent au pouvoir sont considérés comme des \nennemis de la classe ouvrière. Le pouvoir soviétique met en place dès 1918 des premiers camps dans lesquels il s’agit \nde « rééduquer » par le travail. Ce sont en fait des camps de travail forcé administré par le Goulag. Staline, qui arrive \nau pouvoir en éliminant ses opposants, s’appuie sur la police politique, le NKVD, pour traquer ceux qu’il appelle « les \nennemis de l’intérieur », en fait tous ceux qui s’opposent, même et surtout au sein du Parti Communiste. Entre 1936 \net 1938, la Grande Terreur s’abat sur l’URSS. La violence d’état devient systématique, des objectifs sont fixés dans les \nprovinces. Entre 1,5 et 2 millions de personnes sont arrêtées, condamnées à mort (750 000) ou envoyées dans des \ncamps. Les principaux opposants à Staline sont jugés en public pendant les Procès de Moscou, et sont généralement \ncondamnés à la peine de mort.  \nB. \nLe fascisme italien. \nDéfinir le fascisme : Avec les lois fascistissimes de 1925-1926, Mussolini met en place les outils nécessaires à \nl’encadrement de la société. Le fascisme se base sur une double référence au passé glorieux de l’Italie (L’Empire \nromain) et sur la volonté de construire un état moderne autour de son chef. Il s’appuie donc sur un fort nationalisme \n\net un rejet de la démocratie et du communisme. La culture de guerre, visible dans l’encadrement militaire de la société, \ntout comme dans la propagande après la guerre en Éthiopie, est un élément central du fascisme. Le fascisme intègre \nde manière incomplète un racisme d’état, affirmant la supériorité du peuple italien. En 1938, une série de lois \nantijuives complètent la définition du fascisme.  \nUne violence d’état antidémocratique : Si le nombre de victimes du régime de Mussolini n’est pas comparable avec \nl’Allemagne et l’URSS, l’Italie est un état policier dans lequel les opposants politiques sont systématiquement \npourchassés. Après l’assassinat de l’opposant Matteotti (1924) et les lois fascistissimes, Mussolini utilise la violence \npour asseoir son autorité. L’OVRA, police politique mise en place en 1927, traque les opposants politiques, notamment \nles communistes. Arrêtés, ils sont jugés et condamnés à mort ou à la déportation dans les îles Lipari. \nC. \nLe nazisme. \nLe racisme comme base idéologique : Développée par Hitler dans son ouvrage Mein Kampf, rédigé en 1924-1925, le \nnazisme se définit comme une révolution sociale qui doit permettre à la race aryenne de conserver sa supériorité. Il \nentend donc lutter contre tout ce qui pourrait affaiblir la race aryenne (handicapés, homosexuels…). Pour Hitler, les \nJuifs sont les principaux responsables de l’affaiblissement et de la défaite de 1918 (« coup de poignard dans le dos »), \nassociés aux communistes. L’antisémitisme est donc un fondement central du nazisme et les Juifs sont la cible du \nnazisme : boycott des magasins juifs, Lois de Nuremberg (1935) les privant de la nationalité allemande et interdisant \nles mariages entre Juifs et citoyens allemands, ou bien encore les mesures de 1938 leur interdisant d’exercer un \nnombre importants de métiers. \nLa violence comme outil idéologique : Hitler confisque le pouvoir grâce à la violence (incendie du Reichstag en février \n1933) et fait du NSDAP le seul parti autorisé. Devenu Reichsführer en 1934 à la mort du Président Hindenburg, Hitler \nmet en place de nombreux outils pour orchestrer la violence nazie : la Gestapo, police politique, arrête les opposants \ntandis que les SA puis les SS sont chargés de l’application violente des mesures nazies. Ainsi, utilisant comme prétexte \nun attentat à Paris contre un représentant nazi, les dirigeants nazis appellent les Allemands à se venger. C’est le début \ndu pogrom (mot russe signifiant la persécution des Juifs) orchestré par les SS. Dans la nuit du 9 au 10 novembre 1938, \nappelée la Nuit de Cristal, les magasins juifs sont détruits, les synagogues incendiées et les Juifs sont déportés dans les \ncamps de concentration dont certains ont ouvert dès 1933. \nIII. \nLe rêve d’un nouvel ordre européen. \nA. \nLe culte de la guerre. \nLa guerre au centre des idéologies : Dans les régimes totalitaires, le fonctionnement de base de la société est celui de \nl’embrigadement c’est-à-dire de la militarisation des organisations civiles : le port de l’uniforme dans les groupes \nparamilitaires (SS, squadre) et les organisations de jeunesse en témoignent. Dans les régimes fasciste et nazi, la guerre \nest un but en soi, un projet de société qui permettra de créer une nouvelle société. En URSS, la guerre est d’abord \ncelle contre les ennemis intérieurs. Mais elle est aussi glorifiée par le sacrifice pour assurer la victoire du communisme, \ncomme lors de la guerre civile entre 1917 et 1922 durant laquelle les forces européennes aidèrent les troupes \nblanches. La guerre est donc pour l’URSS nécessaire comme moyen de survie. \nLe refus de l’ordre international : L’un des enjeux centraux du régime nazi est de venger l’affront de Versailles en \nredonnant à l’Allemagne une place centrale en Europe. Il faut donc, par la guerre, se venger de la France et du Royaume \nUni. Mais il faut surtout pour Hitler donner à l’Allemagne « un espace vital » suffisant pour sa population par une \ncolonisation de l’Europe de l’Est et donc la disparition de la Pologne, de la Russie…L’Italie a une position plus ambigüe \nvis-à-vis de l’ordre international. Si elle quitte la SDN, elle cherche tout de même à rester proche de la France et du \nRoyaume Uni. Mais, devant les réticences occidentales, l’Italie opère un rapprochement avec l’Allemagne pour former \nl’Axe Rome-Berlin (1936), le nationalisme italien devant nécessairement passer par des annexions et des guerres. \nL’URSS prône la révolution communiste mondiale et le renversement des régimes bourgeois. Ses relations avec les \n\nautres puissances mondiales sont marquées par un forte défiance même si elle se rapproche de la France (adhésion à \nla SDN en 1934, accords avec la France). \nB. \nL’Espagne, lieu d’affrontement des totalitarismes \nUn pays déchiré par une guerre civile : Les élections de février 1936 voient la victoire en Espagne d’un Front Populaire \nregroupant les forces de gauche, dont les communistes (comme en France). Une partie de l’armée, basée au Maroc \nespagnol, se soulève sous la direction du Général Franco qui prend la tête des nationalistes contre les Républicains. La \nguerre dure trois ans et se termine par la victoire des nationalistes et la fuite en France des Républicains espagnols \n(600 000 morts). \n Un lieu d’affrontement idéologique : Si malgré la demande espagnole, la France de Léon Blum refuse d’intervenir \ndans le conflit et essaie d’imposer un embargo sur les armes, les régimes totalitaires s’engagent dans le conflit. L’URSS, \npar le biais du Komintern, organise et arme les Brigades Internationales qui réunit les volontaires du monde entier qui \ns’engagent aux côtés des Républicains espagnols. De leur côté, les régimes allemand et italien envoient des troupes et \ndu matériel pour aider les nationalistes de Franco, créant une alliance idéologique. Ils vont profiter de ce conflit pour \nexpérimenter leurs troupes et leur matériel comme lors du bombardement du village de Guernica par l’aviation \nallemande (1937) ou celui de Barcelone par l’aviation italienne (1938). \nC. \nLa marche vers la guerre. \nLa faiblesse des démocraties européennes : l’exemple espagnol montre qu’en Angleterre et en France, les \ngouvernements et les opinions publiques, traumatisées par la Première Guerre mondiale, sont très largement \npacifistes et en faveur de politiques d’apaisement et de renoncement. Dès 1936, face à la réoccupation militaire de la \nRhénanie, interdite par le Traité de Versailles, l’absence de réponse ferme franco-britannique illustre cette faiblesse. \nLorsque Hitler revendique les Sudètes, une région de la Tchécoslovaquie en 1938, Chamberlain (Royaume Uni) et \nDaladier (France) préfèrent sacrifier une allié militaire pour éviter de faire basculer l’Europe dans la guerre. C’est \nl’esprit de Munich (nom de la ville où a eu lieu la conférence). Même si la France a conscience que la guerre est proche, \nle Royaume Uni espère encore une conférence sur la paix. \nLes coups de force nazis et italiens : Profitant de l’absence de réaction des grandes puissances mondiales, Hitler, après \navoir réarmé son pays, se lance dans une politique d’expansion. En mars 1938, il réalise l’annexion de l’Autriche \nl’Anschluss au nom de la « Grande Allemagne » qui doit réunir tous les populations germanophones sous l’autorité \nnazie. En Septembre 1938, les Sudètes, province majoritairement germanophone en Tchécoslovaquie sont annexées \npar l’Allemagne avec l’accord de la France et du Royaume Uni (conférence de Munich). En mars 1939, Hitler s’empare \ndu reste de la Tchécoslovaquie et commence à revendiquer une partie de la Pologne. La France et le Royaume Uni \ncomprennent que la guerre est inévitable. Lorsque l’Allemagne, après avoir signé un pacte avec l’URSS se partageant \nla Pologne, se lance dans la guerre le 1e septembre 1939, les puissances occidentales se lancent à contre cœur dans la \nSeconde Guerre mondiale. \nConclusion : L’URSS, l’Italie fasciste et l’Allemagne nazie sont donc des régimes nés de la brutalisation des sociétés à \nl’issue des deux évènements du premier XXe siècle : la Première Guerre mondiale et la crise de 1929. S’appuyant sur \nun parti unique et rejetant les fondements démocratiques, ces trois régimes ont mis en place des méthodes de \ngouvernement communes pour atteindre des objectifs idéologiques différents : une dictature du prolétariat en URSS, \nun nationalisme guerrier en Italie et un nationalisme raciste et expansionniste en Allemagne. Ces trois pays ont eu en \ncommun de vouloir mettre en place une nouvelle géopolitique dans l’Europe des années 30, avec, comme moyen \nultime d’y parvenir la guerre face à des démocraties traumatisées par les sacrifices de la Grande Guerre. \n",
    "Memoire_M1___Efrei.pdf": "Floware\nVision\nJourney in Edge AI Computer Vision Application for Traﬀic Analysis.\nNoé Breton\nSupervised by Julian Garbiso, Léonard Benedetti\nEFREI Paris Panthéon-Assas Université\n-\nFloware\nAugust 2023\n------>\nJune 2024\n\nAbstract\nThis memoir explores the development and application of Edge AI computer vision technology for traﬀic analysis through\nFloware Vision.\nThe integration of computer vision, artificial intelligence, and the Internet of Things transformed various\nindustries, transportation for instance.\nThis project highlights Edge AI’s real-time data processing capabilities to address\nmobility challenges and optimize traﬀic flow. Floware Vision, using Nvidia’s Edge AI solutions and YOLO models, processes\nvideo inputs from multiple cameras to detect and track objects, providing essential data for traﬀic management. This approach\nnot only enhances real-time decision-making but also maintains data privacy by performing computations locally. The memoir\ndescribes the architecture of Floware Vision, explaining the video processing pipeline, model integration, and the deployment\nstrategies used. It aims to enhance the importance of modularity, real-time eﬀiciency, and compatibility in the system’s design.\nAdditionally, it justifies the implementation of functional programming and microservices architecture to meet these requirements.\nPerformance metrics and case studies comparing different YOLO models are presented to demonstrate the system’s effectiveness.\nThe memoir concludes with a review of the future work needed to further upgrade Floware Vision and its potential impact on\nurban mobility.\n\nCONFIDENTIALITY\nTHIS REPORT IS INTENDED SOLELY FOR THE EVALUATORS AND MAY NOT BE DISCLOSED TO OTHER INDIVID-\nUALS, WHETHER INTERNAL OR EXTERNAL TO THE INSTITUTION. THE DOCUMENT MUST NOT BE RETAINED\nBEYOND THE PERIOD STRICTLY NECESSARY FOR ITS EVALUATION.\n\nPlagiarism\nI, Noé BRETON, hereby certify that I am the author of this Memoire, and I conducted the research myself. I confirm that this\nMemoire has not been previously submitted for any other degree. Any statement taken from the work of another person (with\nor without minor changes) and cited in this report is enclosed in quotation marks, and proper and precise references have been\nprovided for such citations. I am aware that plagiarism can result in the invalidation of this Memoire and, in severe cases, lead\nto expulsion from the University. I also aﬀirm that, except for the duly acknowledged citations, this report represents my own\nwork.\n\nAcknowledgements\nI would like to thank Floware, Julian Garbiso, and Mathieu Lafarge for allowing me to build my experience in their company\non interesting projects. Additionally, I would like to extend my thanks to all my collaborators, especially Ali, Bond, and Enea,\nfor their support and sympathy.\n\nContents\n1\nIntroduction\n1\n2\nFloware: Edge AI for Traﬀic Analysis\n1\n2.1\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n2.2\nTeam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n2.3\nMissions Organisation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.4\nPartners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.4.1\nAcademical Partners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.4.2\nEconomic Partners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.4.3\nCommercial Partner\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.4.4\nComercial Partner Archetype\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.5\nLocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.6\nSome Competitors\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.7\nServices\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.7.1\nFloware Vision\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.7.2\nFloware Core (In development)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.7.3\nFloware Autopilot (In development)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.8\nProject Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nEdge AI: Definition and Context\n5\n3.1\nIntroduction to Edge Computing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nWhy using Edge AI ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.3\nApplications of Edge AI\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4\nReal Time Computer Vision and the YOLO approach\n6\n4.1\nIntroduction to computer vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.2\nHow YOLO Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3\nYOLO’s Iterations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3.1\nYOLOv1-v4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3.2\nYOLOv5-v8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.4\nComparison with Competitor Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.5\nYOLO limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.6\nApplication of YOLO in traﬀic detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5\nNvidia Jetson: The Chosen Hardware\n10\n5.1\nIntroduction to Nvidia Jetson[1] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5.2\nTechnical Specifications\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5.3\nApplications in Floware Vision\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n6\nFocus On Nvidia Deepstream, the complexity worth the performance ?\n12\n6.1\nOverview of Nvidia Deepstream . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6.2\nFeatures and Capabilities\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6.3\nPerformance vs. Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n7\nFloware Vision: Review of the Existing\n12\n\n7.1\nExisting States\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n7.1.1\nInputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n7.1.2\nOutputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n7.1.3\nKey Features\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n7.1.4\nArchitectures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n7.1.5\nVision Processing Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n7.1.6\nModel Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.2\nExisting Deployment Process\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.3\nLimitations, Issues & Challenges\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.3.1\nStructure Issue\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.3.2\nSecurity Issues\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n7.3.3\nOptimization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n7.3.4\nDeployement issue\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n7.3.5\nBugs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n7.4\nFeedback and Improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n8\nFloware Vision: Development\n19\n8.1\nImplementation Choice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n8.2\nImprovement On The Existing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n8.3\nAI Model Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n8.4\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n8.4.1\nJustification for C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n8.4.2\nDesign & Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n9\nFloware Vision: Deployment\n24\n9.1\nDeployment Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n9.2\nIntroduction to Azure IoT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n9.3\nCI/CD: Actual Deployment Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n10 Results and Discussion\n26\n10.1 Floware Vision Performance Metrics\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n10.2 Case Studies : Yolo8s vs Yolo8n performance\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n10.3 Output Analysis: the end side of the pipeline\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n10.3.1 Visualization of Flows\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n10.3.2 Transition Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n10.3.3 Activity Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n10.3.4 Post-Alerting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n10.3.5 Queue Length Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n10.3.6 Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n11 Conclusion\n30\n\nFloware Vision: Journey in Edge AI Computer Vision Application\nDevelopment for Traﬀic Analysis\nNoé Breton\n \n \n16 July 2024\n1\nIntroduction\nThe convergence of computer vision, artificial intelligence, and the Internet of Things has revolutionized various industries like\ntransportation, security, and home automation. Among these advancements, Edge AI has stepped up as a powerful and eﬀicient\nsolution for providing real-time data processing and decision-making capabilities directly at the source of data recording. Traﬀic\nanalysis has benefited from these advancements. Indeed, the issue of mobility flows1 represents a major challenge for many\nentities, whether public or private. According to a study of the Institute for Transport & Economics Technische Universit¨at\nDresde, Traﬀic congestion typically leads to an increase in fuel consumption of about 80% while travel time can increase by up\nto a factor of 4[2].\nUnderstanding and optimizing movements within a territory is essential for improving transportation eﬀiciency, reducing con-\ngestion, and contributing to the transition toward more sustainable modes of transportation. In this context, the use of Edge AI\nsensors is really powerful as it can bring real-time processing and alerting by local computing and complex analysis by embedded\nAI software without privacy-threatening data transfers to the cloud. That is the path Floware has chosen. Floware seeks to\nprovide useful information and innovative solutions to these entities over the long term, using Nvidia Edge AI solution and\nYolo.\nWhat role does Edge AI software play in traﬀic analysis ?\nAfter providing some context by presenting the company and defining key notions in the field, with a significant state-of-the-\nart section on our utilized technology, my memoir will illustrate my contributions to Floware’s activities, particularly on the\nembedded Edge AI software: FLWR-Vision. This approach will provide a comprehensive overview of Floware’s activities, from\nthe sensor to the data.\n2\nFloware: Edge AI for Traﬀic Analysis\n2.1\nOverview\nFloware is a startup founded in 2023 in Palaiseau, specializing in analyzing mobility flows. It aims to bring innovative technologies\nfor optimizing and de-carbonizing mobility over the long term. Relying on a network of proprietary sensors and a software chain\nstrengthened by artificial intelligence, Floware aims to provide precise, secure, and anonymized data by controlling the entire\ndata acquisition chain, from software to hardware, usable for a wide range of applications cases.\n2.2\nTeam\nFounded by Julian Garbiso (Telecom Paris Tech, IMT Atlantique), PhD in Computer Science and Networks, and Mathieu\nLafarge (Science Po, ESTP), specialized in Urban Planning and Civil Engineering, Floware currently employs one trainee\n(myself) with the title of Edge AI and Cloud Services Developer. To strengthen our workforce and refocus my activity on\nsensor work, a recruitment campaign has been launched for two positions in Data Analysis and the Internet of Things. We now\nhave a Senior Edge IA Engineer, Enea de Bolivier, (INP Grenoble, Kyoto University), an Industrial + UX/UI Designer, Daniel\nEsperben (Universidad Nacional de La Plata), and 3 interns. Ali Choucair (Université de Strasbourg), Mecatronics Engineer\nIntern, Bond Zhang (Polytechnique), Iot intern and Mathias (Epita), data scientist intern.\n1Mobility flows refer to the movements and displacements of people or vehicles within a given space.\n1\n\nFigure 1: Floware Organisational Chart\n2.3\nMissions Organisation\n• Enea\n– Manage Interns and trainees in all fields before new Seniors are hired.\n– Lead the embedded software part of Floware, including FLWR-Vision.\n• Ali & Bond\n– Works on operational device deployment and hardware setup.\n– Research and development tasks on hardware (Motorized Camera, GPS feature, log writings)\n• David\n– Design new device case improving temperature, weight, and size.\n– Design Floware websites.\n• Mathias\n– Work on data science client missions: transform, and create a visualization of the data created from the sensor.\n2.4\nPartners\n2\nThis section outlines Floware’s key partnerships, including academic, economic, and commercial collaborators. These partner-\nships play a crucial role in enhancing Floware’s capabilities and market reach.\n2.4.1\nAcademical Partners\nFloware collaborates with several academic institutions to develop their technological expertise.\n2Informations transmitten by Julian Garbiso\n2\n\n2.4.1.1\nInstitut Vedecom\n3\nFloware is an aﬀiliate of Vedecom Institute and is therefore a natural partner for R&D in the mobile sector. This partnership\nbrings significant benefits in technical support, networking, and branding. Floware aims to create a mutually beneficial dynamic\nfor both organizations by jointly responding to project proposals and tenders and developing a common communication and\nmarketing strategy.\n2.4.1.2\nÉcole Polytechnique (X-UP / X-TECH)\nFloware was founded as a company as part of the X-UP acceleration program at the Ecole Polytechnique. Through the school’s\ninvolvement in the company, the relationship has grown into a long-term partnership. Floware’s oﬀices are within X-TECH,\nallowing the company to continue benefiting from this valuable ecosystem’s technological developments. The Drahi X-Novation\nCenter also houses a FabLab, providing a conducive environment for Floware to develop and test its sensors on-site.\n2.4.1.3\nUniversité Gustave Eiffel\nWe are working with Mahdi Zargayouna, Deputy Director of the GRETTIA Laboratory, on Traﬀic Modeling and Simulation.\nThe Sci-Ty program is currently developing a Technology Maturation and Transfer project.\n2.4.2\nEconomic Partners\nFloware’s economic partners enhance the scalability of the business and support our business strategy, sales, and marketing.\n2.4.2.1\nHEC\nFloware joined the HEC incubator in September 2023 and getting support in business strategy, sales, and marketing.\n2.4.2.2\nMoove Lab [4]\nFloware is part of batch number 11 starting from September 2023. Its goal is to use this opportunity as a platform to access\nthe ecosystem related to the mobility sector, including industry players and venture capital investors in the mobility market,\nnotably ViaID, which runs the program with Mobilians.\n2.4.2.3\nLeonard (Groupe Vinci)[5]\nFloware is part of batch number 11 starting from September 2023. Its goal is to use this opportunity as a platform to access\nthe ecosystem related to the mobility sector, including industry players and venture capital investors in the mobility market,\nnotably ViaID, which runs the program with Mobilians.\n2.4.2.4\nNextmove [6]\nFloware adheres to the Nextmove competitiveness cluster to benefit from its rich ecosystem, its network of member cities and\nterritories, and its commitment to fostering a mutually beneficial partnership.\n2.4.3\nCommercial Partner\nFloware’s commercial partnerships focus on either implementing our sensor on their territory, providing useful data, or both.\n2.4.3.1\nEcomesure [7]\nEcomesure is developing an IoT solution to measure air quality. Floware and Ecomesure are working together on EcoFlow,\nselected by Paris&Co as an innovative metropolitan area. The tool will monitor and warn about vehicle emissions to support\nlow-emission zone policies in European cities.\n3French Institute for Energy Transition dedicated to road mobility [3]\n3\n\n2.4.3.2\nEPAPS (Etablissement d’aménagement de Paris Saclay)\nFloware’s first major partner commissioned them to conduct a large-scale proof of concept. EPAPS helped Floware to contract\nwith EPAPS and DiRIF and collaborate with the Ecole Polytechnique to conduct decarbonization experiments.\n2.4.3.3\nLa Fabrique des Mobilités [8]\nFloware is working with FabMob to develop Tracemob, an app that tracks travel intentions. They plan to conduct a joint citizen\ntravel survey in Noisy-le-Grand, combining sensor data and citizen voluntary travel information.\n2.4.4\nComercial Partner Archetype\n• Local collectivities (E.g, traﬀic data usage to optimize public transportation routes)\n• A developer/promoter (E.g, real estate developers planning new residential areas based on traﬀic patterns)\n• A mobility operator (E.g, bus companies adjusting schedules based on real-time traﬀic conditions)\n• A construction industry actor (E.g, construction firms coordinating work schedules to minimize traﬀic disruptions)\n• An engineering firm (E.g, firms designing new road networks with the help of advanced traﬀic modeling)\n2.5\nLocation\n• École Polytechnique Palaiseau - Drahi-X Novation Center\n• Station F - Open Space Moove Lab, Paris 13th\n• Leonard:Paris - Coworking space, Paris 11th\n2.6\nSome Competitors\n• Wintics[9]: Develops AI-based software for traﬀic management, parking optimization and public transport monitoring\nto improve urban mobility and reduce congestion.\n• ALYCE[10]: Provides insights into traﬀic patterns, pedestrian flows, and public transport usage, as well as mobility\ndata and analytics to optimize infrastructure and services.\n• UPcity[11]: Provides urban planning and smart city management tools, including traﬀic simulation and infrastructure\nplanning, to support eﬀicient and sustainable urban development.\n• Eurovia[12]: VINCI Group subsidiary focuses on transport infrastructure construction and urban development, integrat-\ning IoT and AI to achieve sustainable and resilient infrastructure.\n• CDVIA[13]: Provides advanced traﬀic management systems with real-time data analytics and machine learning to\noptimize traﬀic flows, reduce congestion, and improve traﬀic safety.\n2.7\nServices\nAs mentioned earlier, Floware offers an innovative solution for analyzing mobility flows for private and public territorial operators.\nTheir philosophy is centered on privacy-by-design (privacy protection rooted in robust design) and focuses on eﬀiciency and\nautonomy. Floware aims to control every aspect of the data pipeline, from the acquisition of data to its processing and analysis.\nFloware divides its services into three products :\n4\n\nFigure 2: Floware Solutions\n2.7.1\nFloware Vision\nThe embedded software that collects data and performs the first processing, powered by AI, is the subject of this memoir. You\ncan find more details in the FLWR-Vision focused section here.\n2.7.2\nFloware Core (In development)\nSaaS platform for data analysis and visualization on the cloud. Production of mobility models and simulations tailored to\nspecific use cases.\n2.7.3\nFloware Autopilot (In development)\nFloware’s AI co-pilot uses Larges language models to analyze data and manage cloud software. This tool provides clients with\nvisualizations, reports, simulations, and customized insights. It improves strategic decision-making in mobility. Floware aims\nto be the top provider of these services with a scalable business model.\n2.8\nProject Management\nFloware chose Agile management[14] for developing their solution, with long-term tasks and urgent, short-term tasks for clients’\nneeds.\n3\nEdge AI: Definition and Context\n3.1\nIntroduction to Edge Computing\nAfter the deployment of the World Wide Web in 1989 by Tim Berners-Lee, and the invention of web servers, web browsers, and\nHTML, data processing started to switch from the local machine to a server. Berners-Lee noticed some issues: in the future,\nwhen many devices are connected to the internet and if all the data is processed by a group of centralized servers, congestion\nproblems tend to appear, causing bugs and crashes for users. A decentralization of computation was needed. Akamai[15] was\none of the first companies to introduce this concept in 1989. By using multiple networks physically closer to users and devices,\nlatency can be reduced, costs minimized, and network connectivity improved. Imagine a festival with thousands of people inside.\nIf there were only one big food stand, toilet, or bar, it would be a matter of time before the whole stand becomes bloated and\nnearly unreachable. Splitting the big stand into smaller ones, located in diverse places, and closer to the festival-goers that can\nget their food quickly and eﬀiciently.\nWith time, edge computing extended to the content-delivering usage from other applications, and now defines every case where\nrunning a computer program delivers a quick response to where the request is made. Edge Computing is not the same concept\n5\n\nas the Internet Of Things (IoT), because this network of physical objects is not obligated to process the data, and can send\ntheir data to the cloud for processing, whereas edge computing focuses on local processing.\nEdge Artificial intelligence (Edge AI)[16] is one of the derivate uses of edge computing. It refers to deploying models and running\nmachine learning tasks directly close to the device location instead of a distant cloud. The data is stored and processed at the\nsame place, before an optional sending to a cloud for retaining or deploying purposes.\n3.2\nWhy using Edge AI ?\nCloud AI and Distributed AI are also popular solutions, but for some use cases, Edge AI brings distinct advantages, particularly\nfor small structures operating in public spaces.\nOperating at the device level removes the need to send data across the network, reducing the risk of data leaks or breaches[17].\nDecentralization decreases the threat of major data breaches because data is not stored in a single location.\nAdditionally,\nprocessing sensitive private data locally allows for retaining only the analysis output or encrypted information, minimizing\nprivacy issues and improving security.\nAdditionally, on a latency aspect, by the edge computing properties mentioned above, the data is immediately processed\nwithout server traveling delay.\nThis reduced latency and local processing enables instantaneous task treatment and permits an alerting system. For instance,\na Computer Vision IoT device can recognize a person searched by the authorities and immediately alert them in real-time.\nFewer requests to a distant server also mean decreased bandwidth usage, with all the cost reductions it induces. reduced\nbandwidth and cloud treatment enhance the scalability of edge solutions. Indeed, increasing the number of devices doesn’t\nsignificantly increase the cloud resources needs because each device processes its data independently.\n3.3\nApplications of Edge AI\nEdge AI can be applied to numerous domains.\nIndeed, edge AI devices like sensors, drones, and cameras can be used in\nagriculture to determine the health state of the soil, and infestation in real-time, allowing the farmer to act quickly, and\nreducing his work time[18].\nFloware Sensors are a good example of edge AI utilization in Traﬀic Flux Analysis. Processing data directly from a camera\nstream lightens the data sent to the server while reducing the amount of sensitive information traveling to the server. Real-time\nprocessing enables an alerting process if congestion is detected, for instance.\nIn another case, a device with many diagnostic sensors can provide an immediate diagnosis with confidentiality. More softly,\nhealth monitoring with noninvasive edge devices like Fitbit[19].\n4\nReal Time Computer Vision and the YOLO approach\n4.1\nIntroduction to computer vision\n”Computer vision is a field of computer science that focuses on enabling computers to identify and understand objects and\npeople in images and videos. Like other types of AI, computer vision seeks to perform and automate tasks by replicating human\ncapabilities. In this case, computer vision seeks to replicate both the way humans see, and the way humans make sense of what\nthey see”[20]\nComputer vision regroups techniques and tools that enable computers to understand, interpret, and process information from\nimages and videos. To understand an image, a computer translates different aspects of it into an array of numbers. These\naspects can represent the contrast or color of each pixel, and the intensity of luminosity or the grey level, for example.\nComputer vision existed before the rise of Deep learning and is still used in its traditional form for simple tasks, like edge or\nshape detection, and integrated into more complex processes. Convolutional Neural Networks, for instance, still use filters and\nshape detection in their process.\nIt was significantly transformed by two major developments: the invention of convolutions neural networks in 1980[21] and\nthe rise of GPU Computing in the 2000s. But because of limited computer resources, it could not be used then. The latter\nwas required as originally computing power was not suﬀicient to run CNN. In 2014, the CNN AlexNet trained on the dataset\nImageNet dominated the image classification field and proved the eﬀiciency of CNN on image classification. A CNN comprises\nlayers that apply matrix operation at each step on the image vector.\nThis method significantly improved the accuracy of\nobject detection and enabled the handling of massive data volumes.\nUnlike traditional methods requiring manual feature\nengineering, Deep Learning algorithms can automatically learn relevant features from data, thus simplifying the development\nprocess significantly improving the accuracy of object detection, and enabling the handling of massive data volumes.\n6\n\nDespite the exponential growth of computer capacity, there were still performance issues, CNNs were requiring too many\nresources to run in real-time. But why it was still the case in the 2010s? Let’s explain how real-time computer vision worked\nbefore 2016. A popular way to process vision object detection was through Faster RCNN (Fast Region-based Convolutional\nNeural Network). RCNN divides the image into smaller regions instead of grids, up to 2000 regions, and then passes these\nregions into a pre-trained AlexNet and a feature map, for each region before a regression.\nFigure 3: RCNN\nDetermining 2000 regions took too much time for real-time (47 seconds per frame), therefore fast RCNN was created, and this\nnetwork established the region from the feature maps, without the need to feed the network 2000 regions directly.\nFigure 4: Fast RCNN\nFaster RCNN improved the process by suppressing the selective search algorithm for regions and let a network predict them\ninstead.\nFigure 5: Faster RCNN\nDespite the optimization improvements of these region-based models, it still wasn’t enough for real-time use. To address these\nlimitations, the YOLO[22] (You Only Look Once) approach was developed.\n7\n\n4.2\nHow YOLO Works\nWhile RCNNs do not look at the complete image and process regions, YOLO chooses a different approach by processing one\nframe at once, using only one network. The network predicts bounding boxes and class probabilities directly from full images\nin one evaluation.\nFigure 6: The Model\nYOLO divides an image into an SxS grid of elements, and for each grid cell, it detects if an object is present through a multi-layer\nneural network. In the case of detection, it determines the center of the object (x, y), its height, width, and a class confidence\nscore. The output is an object/tensor of dimension SxSx(B*5 + C).\nFigure 7: The YOLO Detection System.\n4.3\nYOLO’s Iterations\nYOLO has seen several versions, each improving after each iteration in terms of accuracy, speed, and capabilities. Here is a\nbrief overview of the key iterations and advancements:\n4.3.1\nYOLOv1-v4\nYOLOv1, which was first presented by Redmon et al. in 2016, transformed object detection by approaching it as a single\nregression problem. It enabled real-time detection by processing images on a GPU at 45 frames per second. In contrast to\nRCNN models, which were accurate but slow, YOLOv1 provided a noticeable speed boost.\nYOLOv2, or YOLO9000, was established in 2017. Anchor boxes, batch normalization, and a brand-new network architecture\nknown as Darknet-19 were all included. With over 9000 object classes detected, YOLO9000 improved speed and accuracy.\nYOLOv3 in 2018, included detection at three different scales along with Darknet-53, an enriched architecture with residual\nconnections. This version improved precision and recall while handling both small and large objects.\nYOLOv4, released in 2020 combined new methods like PANet and SAM blocks with CSPDarknet53 as the backbone to optimize\nspeed and accuracy. It enhanced the mean Average Precision (mAP) while retaining real-time detection.\n4.3.2\nYOLOv5-v8\nYOLOv5, created by Ultralytics in 2020, was designed with deployment and usability in mind. Offering multiple versions suited\nto varying computational budgets, from mobile devices to server-grade GPUs, it was optimized for production environments.\nYOLOv6, which was first introduced by Li et al. in 2022, was designed for industrial use. Its Rep-PAN neck, anchor-free\ndetection method, and EﬀicientRep backbone optimized performance for particular industrial scenarios.\n8\n\nYOLOv7, published in 2022, combined novel model architectures with cutting-edge learning strategies.\nIt created ”bag-of-\nfreebies” methods to boost accuracy without raising inference costs, and it raised the bar for real-time object detection. YOLOv8,\nlaunched in 2023, kept improving performance and usability. It included mosaic data augmentation, anchor-free detection, and\na decoupled head.\nWith its effective real-time detection capabilities, YOLOv8 is especially well-suited for edge AI devices.\nBecause of their smaller size and higher framerate, YOLOv8 in its small (YOLOv8s) and nano (YOLOv8n) versions are used\non Floware embedded computer vision.\n4.4\nComparison with Competitor Models\nFigure 8: Error Analysis: Fast R-CNN vs. YOLO : Despite R-CNN having more correct detection, Yolo compensated\nby better performances\nFigure 9: YOLO Framerate compared to other state-of-the-art object detectors [23]\n4.5\nYOLO limitations\nWhile YOLO is a powerful object detection algorithm, it suffers from some limitations:\nYOLO has several limitations that affect its performance in certain applications. One major limitation is the detection of\nsmall objects within large images due to its grid-based approach, which can lead to inaccurate predictions. Additionally, YOLO\nmay struggle with objects that are too close to each other, causing overlapping bounding boxes and reduced detection accuracy.\nThe algorithm also tends to underperform in scenarios with significant variations in object scale, orientation, and occlusion.\nYOLO can also be less precise than real-time detectors such as Faster R-CNN, even though this issue is compensated by its\nreal-time processing capabilities. YOLO does not include tracking functionality natively. These limitations must be considered\nwhen applying YOLO to traﬀic detection tasks, where high accuracy and reliability are crucial.\n9\n\n4.6\nApplication of YOLO in traﬀic detection\nDespite its limitations and coupled with tracking algorithms, YOLO has been effectively applied in traﬀic detection systems\ndue to its real-time processing capabilities. Its speed allows for quick identification of vehicles, pedestrians, and other objects,\nmaking it suitable for dynamic traﬀic environments. YOLO can be integrated with edge AI hardware to analyze video streams\nfrom traﬀic cameras, processing data for traﬀic management and monitoring. By optimizing the model and fine-tuning it for\nspecific traﬀic scenarios, YOLO can achieve satisfactory performance levels, contributing to eﬀicient traﬀic flow and incident\ndetection (see figure 12).\n5\nNvidia Jetson: The Chosen Hardware\n5.1\nIntroduction to Nvidia Jetson[1]\nThe Nvidia Jetson is a family of hardware designed for edge and embedded computing. These devices are engineered to eﬀiciently\nrun AI and machine learning models, using GPU computing and Nvidia technology, such as the DeepStream SDK, enabling\ndevelopers to create AI systems at the edge. This makes them ideal for various applications, including autonomous machines,\nIoT devices, and edge computing solutions. Nvidia offers different versions of Jetson with varying prices and functionalities:\nJetson Nano, Jetson Orin, and Xavier listed in ascending order of capabilities and prices. Currently, Floware uses the Nano and\nOrin models due to their scalability and cost-effectiveness. However, Nvidia has discontinued the Jetson Nano due to its aging\nperformance capabilities, forcing Floware to transition to the Jetson Orin.\n5.2\nTechnical Specifications\nFigure 10: Specification comparison between Jetson Nano, Orin and a Raspberry Pi\n10\n\n5.3\nApplications in Floware Vision\nFigure 11: Floware Sensor\nA classic Floware sensor is composed of a Jetson Nano (1) linked to a camera (5), and an Ubertooth antenna [24] (4) to scrape\nBluetooth data. The Jetson Nano does not contain a Wi-Fi card, so we use a Wi-Fi dongle (3) with a Wi-Fi key (3) to connect\nto the network. The Nvidia Jetson Nano is powered by a 12-volt to 5-volt converter(2).\nFigure 12: Jetson Nano Desktop with Floware Vision Running\n11\n\n6\nFocus On Nvidia Deepstream, the complexity worth the performance\n?\nDeepstream is an Nvidia-developed project based on Gstreamer, an open-source pipeline-based framework [25] made for pro-\ncessing media streams. Gstreamer links many stream-processing into complex workflows.\n6.1\nOverview of Nvidia Deepstream\nNvidia DeepStream[26] reuses tasks in a cascade routine but is specialized for multi-sensor (audio, video, and image) AI\nprocessing. DeepStream incorporates deep neural networks and other complex processes such as encoding/decoding, rendering,\nand tracking. GStreamer’s basic pipeline element is still usable, in addition to Nvidia’s created element plugin. DeepStream\ncan be used on multiple devices, personal computers with Ubuntu installed, or with Nvidia edge devices, such as Nvidia Jetson,\nused for Floware Sensor.\n6.2\nFeatures and Capabilities\n• Multi-Stream Processing: Capable of managing multiple video streams at once, ensuring eﬀicient use of resources.\n• AI Models Integration: Allows modular integration with multiple pre-trained AI models for detection, classification,\nand segmentation.\n• Hardware Acceleration: Permits full integration with Nvidia graphics card acceleration. GPU enables fast parallel\ncomputation, improving performance. Every element of the pipeline is GPU computed.\n• Scalability: Adaptable for deployment on various platforms, from edge devices to cloud environments, supporting scalable\nsolutions.\n• Customizability: Offers a flexible pipeline architecture that enables easy fine-tuning of complex processes to fit various\napplication needs.\n• Developer Tools: Includes tools for performance monitoring, debugging, and optimization to assist developers in fine-\ntuning their applications.\n• Compatibility: Deepstream is adaptable on many platforms, from edge devices to cloud environments.\n6.3\nPerformance vs. Complexity\nSetting up and optimizing DeepStream requires a deep knowledge of the Nvidia ecosystem, including CUDA and TensorRT.\nCustomizing and fitting a pipeline for our needs can require more development and testing effort. Why would we use DeepStream\nfor a computer vision task instead of a basic Python script using Ultralytics[27] and OpenCV on GPU? Besides the fact that\nthe Jetson Nano was built to use DeepStream, the customizability and scalability arguments, an OpenCV application is never\nentirely running on the GPU, such as the rendering of videos and bounding boxes. In contrast, a DeepStream pipeline runs\nentirely on the GPU, allowing for overall better performance.\n7\nFloware Vision: Review of the Existing\nFloware Vision, as discussed in the first part of the report, is the embedded software used in the Floware sensor. Floware Vision\naims to centralize all the functionalities and AI processes needed for the output data, using Deepstream and Ubertooth as their\nmain dependencies to perform computation and processing on the edge. Floware’s needs include object detection, tracking, and\nclassification (not in real-time but very regularly).\nAdditionally, Ubertooth is used to capture a fragment of the Bluetooth address (LAP) of the Bluetooth devices emitting packets\nin the detection range of the sensor, anonymizing it on the fly, helping in the continuity of detection between sensors. For some\nclients, relay control is required. As an example, local authorities may need to trigger lights when a person passes by during\nthe night but not for a vehicle. This requires a relay control in the object detection pipeline. If the sensor camera angle is wide,\ntriggering should be limited to a particular zone, such as a pedestrian crossing. This application needs a Region Of Interest\n(ROI) filtering to limit detection to a small zone, necessitating an ROI drawing tool and an encoding convention (either on the\nedge or in a preprocessing step).\nThis example highlights that the sensor has to be adapted to different use cases. Modularity is crucial to fine-tune each sensor\naccording to the client’s needs while responding to privacy needs.\nAs many sensors can be deployed simultaneously, they must be as failure-proof as possible. Floware needs to incorporate features\nthat ensure reliability, and if a failure occurs, it is essential to understand why and how it happened.\n12\n\n7.1\nExisting States\nFloware Vision is a composite application, using a corpus of Python scripts distributed across multiple files.\nIt manages\nBluetooth data, computer vision data, but also some intern information like power and disk usage data.\n7.1.1\nInputs\n• Video Stream via USB camera.\n• Bluetooth data via Ubertooth antenna.\n• Disk usage.\n• Date and time.\n7.1.2\nOutputs\nFloware Vison sends It’s output to servers (virtual machines) on the Scaleway or Azure clouds.\n7.1.2.1\nVision Detection Data\nSensor output data is in the form of a CSV or Parquet file, divided by variable periods, with the following columns: ['Frame_number',\n'ObjectID', 'ClassID', 'Confidencelevel', 'BoundingBoxTop', 'BoundingBoxLeft', 'BoundingBoxWidth', 'BoundingBoxHeight',\n'Datetime']\n• ObjectID: corresponds to a unique identifier for a unique object.\n• ClassID: corresponds to the type of object among these categories:\n('car', 'truck', 'bike', 'motorbike', 'person')\n• BoundingBox_: corresponds to the characteristics of the detected object’s frame.\n• Confidencelevel: corresponds to the YOLO class confidence.\nEach file is dated.\n7.1.2.2\nSnapshot Data\nSnapshot data correspond to frames of the camera stream captured at certain intervals in JPG format. There are two categories\nof snapshots: time and class snapshots. The snapshots taken by class are named according to the detected class in the picture.\nThe purpose of snapshots is to aid in debugging in case of incoherent results, provide an accurate representation of the sensor\nsituation (snapshot time), and build our internal dataset (snapshot class).\n(a) Example of Snapshot Class: truck-20240423165514\n(b) Example of Snapshot Time: snapshot-202405032204s\n13\n\n7.1.2.3\nBluetooth data\nBluetooth data, collected using the Ubertooth[24] device, is sent to either the log or parquet format for further processing. The\ncollected data includes several key fields, which are as follows: ['systime', 'ch', 'LAP', 'err', 'clkn', 'clk_offset',\n's', 'n', 'snr']. Each field represents specific information:\n• systime: The system time when the data was captured.\n• ch: The channel on which the data was captured.\n• LAP: The Lower unique Address Part of the Bluetooth devices address. It’s the main information for FFloware’s Bluetooth\ndata analysis.\n• err: Error information, if any, related to the data capture.\n• clkn: The clock number of the Bluetooth device.\n• clk_offset: The clock offset.\n• s: The signal strength or quality.\n• n: The noise level at the time of data capture.\n• snr: The signal-to-noise ratio, is useful for knowing the clearness of the signal.\nThis structured format ensures that the data can be effectively analyzed and utilized for traﬀic analysis and other applications.\n7.1.2.4\nThe ippadrr case\nFigure 14: Server Side Folder Tree\nThe ipaddr folder is deprecated. It was initially created to regularly send the IP address information of the sensors (on the\ncellular network) to establish SSH connections. However, there were issues bypassing the NAT of the 4g dongle; thus, it was\nabandoned. Eventually, it was repurposed to transfer occasional files, such as syslog files. This functionality should be retained,\nbut it should not be called ipaddr.\n7.1.3\nKey Features\nComputer Vision (FLWR-Vision.py)\n• Implementation of the Computer Vision module can be found in FLWR-Vision.py. This script configures and executes a\nGStreamer pipeline to process video streams. It offers primary and secondary inference, tracking, and analytics features\nusing DeepStream SDK.\n• Additionally, the module contains a scheduler that periodically converts CSV files to Parquet format, enabling eﬀicient\nstorage and analysis of the generated results.\nBluetooth Detection Scrapping (FLWR-UbertoothService.py)\n• Responsible for capturing Bluetooth data using the Ubertooth device which executes the ubertooth-rx command and\nrecords the received data.\n• Periodically (every 10 minutes), the script generates new log files to facilitate easier management and analysis of the data\nby segmenting it based on time intervals.\n14\n\nServer File Sender (FLWR-SyncFoldersDaemon.py)\n• The FLWR-SyncFoldersDaemon.py script synchronizes local folders with remote server folders using the rsync command.\nROI Filtering (drawRoi.py)\n• The drawRoi.py script facilitates the drawing of Regions of Interest (ROIs) for the analytics module.\nLogs to Parquet Converter (FLWR-UbertoothLogsToParquet.py)\n• The FLWR-UbertoothLogsToParquet.py script scrap log files generated by the Ubertooth device, converting them to\nParquet format for optimized storage and analysis.\n• The script parses each log file, extracts the relevant fields and saves the data into Parquet files. After successful conversion,\nthe original log files are deleted to save disk space.\nDisk Usage Control (FLWR-DiskUsageControlDaemon.py)\n• The FLWR-DiskUsageControlDaemon.py script monitors disk usage in specified directories and deletes the oldest files when\nusage exceeds predefined limits.\n• It ensures that the system remains operational by preventing disk space from being filled up, thus avoiding potential\ncrashes or data loss.\nPower Management (FLWR-PowerDaemon.py)\n• The FLWR-PowerDaemon.py script manages the power state of the system, including suspending during night hours and\nrebooting in the morning.\n• It uses a state machine approach to track the current power state and performs actions based on the time of day, ensuring\noptimal power usage.\n15\n\n7.1.4\nArchitectures\nFigure 15: Floware Vision Architecture Representation\nAll the real-time processing is managed by the Analytics class, which saves computer vision results and activates the relay\ndepending on the class detected.\n7.1.5\nVision Processing Pipeline\nAs mentioned, FLWR-Vision uses a Deepstream pipeline to process video flux for object detection, tracking, and analytics. The\npipeline is implemented in the FLWR-Vision.py script and uses the DeepStream SDK, enabling GPU processing throughout the\nentire pipeline.\nPipeline Components\n• Source Element: The pipeline starts with a source element that captures video from available video devices.\nThe\nfunction list_video_devices() is used to list all video devices available on the system.\n• Inference Engine: One of the main elements of the pipeline is the primary inference element. It uses pre-trained models\nlike Yolo to perform object detection and classification.\n• Tracker: The tracking element tracks detected objects across frames. It attributes a unique id for each detected object.\n• Analytics Module: An analytics module processes information output from the tracking and inference elements, per-\nforming analysis if needed.\n• Sink Element: The sink element is placed at the end of the pipeline, which can either display the video on the screen\nor save it to a file, depending on the configuration.\n16\n\nFigure 16: Existing Deepstream Pipeline Representation\nPipeline Elements\nThe usb-cam-source element captures video input from the USB camera, serving as the starting point of the video stream\nin the pipeline. Next, the nvmm_caps element applies properties to the video stream to match the expected format for future\nprocessing, setting up properties like frame rate, resolution, and format. Following this, the convertor_src1 element converts\nthe video stream into a compatible format for the next pipeline elements, ensuring compatibility for hardware-accelerated GPU\nprocessing. Another video conversion is performed by convertor_ssing. After that, the Stream-muxer element concatenates\nmultiple input video streams into a single output stream, which is essential for scenarios where multiple camera feeds are\nprocessed simultaneously. Then, the primary-inference element performs primary inference using a pre-trained deep learning\nmodel, such as YOLO, and is responsible for object detection and classification within the video stream. Configurations for\nYOLO models (YOLOv8s, YOLOv8n, YOLOv5) can be loaded via the config file here. Additionally, the tracker element tracks\ndetected objects across frames, maintaining unique identifiers for each object, and ensuring continuity of object detection over\ntime for one sensor, which is essential for traﬀic detection. The onscreendisplay element then overlays information such as\nbounding boxes and labels on the video stream, useful for visualizing detection and tracking results in real-time, mostly for\ndebugging and maintenance. Following this, the nvvideorenderer element renders the processed video stream to a display,\nproviding real-time visualization of the pipeline output. A sink element, fakesink, does not display the stream visualization\nand is used when visualization is not needed; it is not currently used in production. Lastly, the analytics element performs\nadditional analytics on the inferred data, processing the detection and tracking results to generate insights and metrics. In\nFloware Vision, the analysis part is transmitted to the Analytics class.\n \n17\n\n7.1.5.1\nOverall Flow\n• Video is captured from the USB camera.\n• The video stream is processed through several conversion steps to ensure compatibility.\n• Inference is performed to detect and classify objects.\n• Detected objects are tracked across frames.\n• The results are displayed on-screen or sent to analytics for personalized processing.\n• Additional analytics are performed to generate insights and results are saved.\n7.1.6\nModel Integration\nAt the primary inference level, YOLOv8s is used by default and pretrained for 640x480 images. Labels used (see Vision Data\nPart is tuned for traﬀic purposes), for performance purposes YOLO8n can be used for improving framerate (see Floware Vision\nPerformance Metrics).\n7.2\nExisting Deployment Process\nSystemd services are used to maintain scripts to run even after failure.\n7.2.0.1\nJetson Nano\nOn the Jetson Nano, Floware uses an SD card already flashed that contains Floware Vision from a previous Floware Sensor,\nthen copies bit by bit data on another sd card to avoid most of the failure that can happen on a regular copy. The new sd\ncard is then inserted in a new jetson. Bit-by-bit copy is a long process and can be time-consuming, it also does not allow any\nautomatic Floware Vision Update.\n7.2.0.2\nJetson Orin\nPrevious deployment on Jetson Orin is slightly different because it’s a ”recently” acquired hardware. Floware chose an inde-\npendent developer to convert floware Vision to Orin. The independent developer created docker images that contain every\ndependency that Floware Vision requires. By pulling the docker images in a ”virgin” Orin with Floware Vision on it, we can\nlaunch the application inside the docker. I was far from being finished at this state.\n7.3\nLimitations, Issues & Challenges\nChecking the Floware Vision codebase revealed that a lot of angles of improvement were possible.\n7.3.1\nStructure Issue\nFloware Vision’s codebase doesn’t have code documentation, making it very diﬀicult to understand the purpose of each func-\ntionality. Besides the lack of documentation, there are many unused files with no purpose in production, including various sh,\ntxt, and py files.\nAdditionally, the class used in the code has issues that make it hard to read. The main script, FLWR-Vision.py, is not structured\nwith methods or classes, which is a major issue for modularity. If we wanted to add elements to the pipeline, such as a second\nclassifier, or fix any bugs, every feature could be impacted by small modifications.\nEven if a configuration file rigs this script, it still lacks modularity, and many features are hardcoded. For instance, we cannot\nchoose the video device or the path of the configuration file for the tracker and the primary inference; only the model name is\nin the configuration. The script uses an if-else structure with hard coded paths inside the code:\npgie = Gst . ElementFactory . make( ” nvinfer ” , ”primary−inference ” )\ni f not pgie :\nsys . stderr . write ( ”␣Unable␣to␣create ␣pgie ␣\\n” )\ni f\n(c_app [ ’app ’ ] [ ’MODEL’ ] == ’YOLOV8N’ ) :\npgie . set_property ( ’ config−f i l e −path ’ , ” cfg /YOLOV8N. txt ” )\ne l i f\n(c_app [ ’app ’ ] [ ’MODEL’ ] == ’YOLOV8S’ ) :\npgie . set_property ( ’ config−f i l e −path ’ , ” cfg /YOLOV8S. txt ” )\n18\n\ne l i f\n(c_app [ ’app ’ ] [ ’MODEL’ ] == ’YOLOV5N’ ) :\npgie . set_property ( ’ config−f i l e −path ’ , ” cfg /YOLOV5N. txt ” )\ne l i f\n(c_app [ ’app ’ ] [ ’MODEL’ ] == ’YOLOV5S’ ) :\npgie . set_property ( ’ config−f i l e −path ’ , ” cfg /YOLOV5S. txt ” )\nElse :\nprint ( ’Model␣Not␣found ’ )\n7.3.2\nSecurity Issues\nThe previously reported structural issue also brings major security concerns. The script responsible for synchronization with the\nserver has the server IP hardcoded. Additionally, there were two scripts with the same code but with different IPs for Scaleway\nand Azure, indicating a lack of modularity. The authentication keys are stored in the main Floware Vision repository without\nany protection. If an intruder accesses one of the Floware Jetson devices, they will also gain access to our servers. The Docker\nimage of the application was publicly pulled from the personal DockerHub of a freelance developer.\n7.3.3\nOptimization\nFloware Vision on Jetson Nano runs at 8 fps, while on Jetson Orin it runs at 30 fps. The low resolution and frame rate can\nreduce the precision of the measurements. There is also a size optimization issue; the Docker image for the Jetson Orin version\nof the application is 16 gigabytes, which is unusually large as it runs directly on the disk, indicating that the application is not\nproperly containerized.\n7.3.4\nDeployement issue\nThe deployment method on the Jetson Nano does not permit eﬀicient software updating on the deployed sensor. For example,\nwe used the TeamViewer remote file transfer system to update a dozen sensors. An operation like this usually takes one and a\nhalf days, and many issues can occur during the process, such as ownership problems with the executable file or file conversion\nissues if the source computer is running Windows. Additionally, there was no online repository for the application, such as\nGitHub or GitLab, nor any version control.\n7.3.5\nBugs\nSome bugs were still in production and slowed the maintenance and the post-process offset:\n• Permission issue: The server key was not always accessible for the system’s service due to an ownership issue.\n• Files sending issue: On the server side, files were sent as soon they were written inside, so a lot of data was missing, they\nwere also sent before the conversion to parquet, sometimes with only an error output inside.\n• As the video device name was hardcoded in the source pipeline element (/dev/video0), the video device was not always\nat this location, and a manual reboot was required.\n7.4\nFeedback and Improvements\nAll these issues highlight the need for structuring and organization of a work pipeline so that each fix and addition can be reviewed\nand homogenized. Besides bug fixes and performance improvements, an effort will be made in modularity and compatibility\nto ensure eﬀicient debugging and adaptability for various missions, particularly in AI detection. Indeed, some missions require\nfine-tuning in detection, such as work vehicle detection, license plate detection, or even speed detection. The future development\nand deployment process of Floware Vision will be considered in this way.\n8\nFloware Vision: Development\nBefore any refactoring, correcting the failure-causing bugs was the priority. First, we created a bash script that deployed the\nsystem’s services as the IoT user, eliminating permission issues. In the file-sending system services, we chose to ignore the last\nfile created and exclude any logs and CSV files. The video device selection was no longer hardcoded; we retrieved a list of all\ndevices and selected the first one from the sorted list.\n19\n\n(a) Bluetooth Data before fixes\n(b) Bluetooth Data after fixes\nThe Floware Vision development and refactorization process is driven by three constraints: real-time eﬀiciency, modularity,\nand compatibility. The first decision we had to make was to choose a programming paradigm that combined these three\ncharacteristics.\nObject-oriented programming (OOP) has been used since the 1960s in software development and was the default option\nin the 1990s, the principal paradigm to learn if you wanted to be a good software developer. However, OOP has slightly lost\npopularity over time, even if in some cases, it is still the obvious choice. The purpose of OOP is to divide services into objects\nwith internal properties, allowing for easy addition or modification of features without impacting the rest of the code.\nWhen applied strictly, developers realize that OOP suffers from numerous problems, particularly in real-time and IoT devel-\nopment. Indeed, it can lead to hard-to-read code and increase the debugging and maintenance time, with a deep hierarchy\ndue to inheritance and abstraction properties. Multiple abstraction layers can lead to overhead in limited resource environ-\nments, due to the need to manage multiple objects. Mutable state objects is a significant problem in real-time processing,\ncomplicating concurrency; in multithreading, for example, it can lead to data leaks and data access issues.\nIn Floware Vision (particularly the computer vision part), we didn’t find any use cases that would require a mutable internal\nstate. These are the main reasons for our choice not to use OOP in the main part of Floware Vision. Additionally, how do you\ndefine objects objectively? What would happen if an object fits into multiple categories? (See The “Platypus” Effect[28]).These\nquestions are not necessary and would make Floware lose time and money.\nAnother programming paradigm that has gained popularity in the last decade is functional programming[29]. While OOP\nfocuses on interacting or communicating with objects, in functional programming the emphasis is on transforming them[28].\nThe focus in functional programming is to pass an object, data, or variables into a function that returns a new object, data, or\nvariables representing the transformed input, without altering some internal state in any way. This is achieved using immutable\nobjects and ”pure” functions as described previously.\nThis lack of a mutable internal state is recognized as a great way to achieve parallel computation because it avoids concurrency\nand accessibility issues. It makes the code more readable and debugging easier. Pure functions also significantly improve\nreliability in data streams because they ensure consistent output for the same input. Furthermore, the code is less bloated with\ncomplex hierarchies and inheritance, resulting in shorter, less complex code that enhances maintainability. This simplicity and\nthe reduced failure that induced is are researched in IoT programming [30]. Our sensors needed a paradigm that fitted best our\nconstraint. And functional programming is this paradigm.\n8.1\nImplementation Choice\nWe noticed quickly that Floware Vision is an aggregate of functionalities that didn’t communicate; the computer vision script\ndoes not interact with the Bluetooth script, and the sender system gets the data from the folder directly. It is the same for\nthe power management and disk usage services. The application was already composite because each ”service” was launched\nindependently, but it wasn’t implicit.\nBesides the functional programming choice as a programming paradigm, a microservices4 architecture was chosen.\nBy its\ncomposite structure, it facilitates the debugging and updating of each service, but also the deployment. Indeed, teamwork is\nimproved as each member of the team can access a service and upgrade it independently. Services are faster to build, test, and\n4Microservices - also known as the microservice architecture - is an architectural style that structures an application as a collection of\nservices[31]\n20\n\ndeploy5.\nEven though functional programming is the main choice, we don’t exclude the use of OOP or procedural programming when it\nis the most appropriate choice. For instance, some services may need different implementations (conversion purposes or sending\nto different servers in different formats using different libraries). In this case, using a Strategy Pattern[32] in OOP can be useful,\nor we can also functionally translate this pattern, using custom-typed functions as parameters in another function.\n8.2\nImprovement On The Existing\nAfter the bug fixes, we started by refactoring every Python script of Floware Vision to translate procedural computation into\na corpus of functions and integrate a configuration JSON file for each script, replacing hardcoded information and mutable\nobject attributes in certain scripts like Analytics. We also removed all the unused files. And organized the file hierarchy by type\n(models, configuration files, scripts, and services).\nHere are some of the improvements in the Python version of the application:\n• Added Python documentation to all functions and a general README.\n• Removed unused files.\n• Refactored the Sync-folder service to use a JSON config file, allowing for the selection of the desired server.\n• Automatically select the video device in the Vision service.\n• Bugs Fixes\n• Fusion of the jetson nano orin, and dGpu application, it can now run on every device.\nFloware is also required to modify the Deepstream pipeline of the computer vision to add more classifiers depending on the\nspecificities of each mission, which is easier to do since the refactoring of the script.\n5More details in the deployment section\n21\n\n8.3\nAI Model Integration\nFigure 18: Deepstream Pipeline\nCompared to the previous existing pipeline, we can notice the addition of an optional secondary and third classifier.\nIt is\ncurrently used for license plate recognition. Using custom Licence plate Detection and text Recognition models provided by\nNvidia[33]\n22\n\nFigure 19: Testing Of Licence Plate Recognition feature\n8.4\nFloware Vision Bedrock6: The C++ Version\nWe made limited changes to the Python app to ensure reduced deployment and maintenance time, but we knew Python was\nnot the best option for IoT programming where we needed to be closer to the metal7 in an optimized way. C++ remains one\nof the most used and fastest languages.\n8.4.1\nJustification for C++\nNvidia Deepstream and Gstreamer are C++ libraries. By using the Deepstream Python bindings, we are adding an intermediate\nlayer that is not necessary. Using C++ natively improves performance and also simplifies the deployment process and the edge\ndevice. Not having to install Python bindings[34] also improves compatibility between devices by avoiding any issues caused by\nPython or pip, but also reduces the bload of IoT. The fewer elements we have to care about, the fewer issues will happen.\nC++ is natively typed and supports both OOP and functional programming[35].\nIt contains every library needed for our\nservices (even if our architecture permits to use of different languages depending on the service) like Azure Iot, Deepstream,\nand Ubertooth. We choose the classic C++ naming convention.\n6A well chosen name.\n7Closer To The Hardware\n23\n\n8.4.2\nDesign & Architecture\nFigure 20: C++ Floware Vision Bedrock Architecture\nThe architecture graph shows how the data is processed from left to right, providing a clearer view of the microservices\narchitecture. Each service is contained with its libraries or configuration files. Notably, the convert-to-parquet service, which was\npreviously (see 15) implemented in two different scripts—FLWR-Bluetooth for Bluetooth data and hardcoded in the FLWR-Vision\nScript for computer vision data—is now an independent service allowing multiple conversion methods. Similarly, the sync folder\nservice has been improved. Floware decided to abandon Virtual Machine storage in favor of using Azure Blob Storage[36] for\ncost eﬀiciency, management, and scalability. By using the Azure library inside the application, we need to ensure the possibility\nof multiple implementations to facilitate future migrations to other services. Floware seeks European partners. We could have\nthe need, in the future, to use a European cloud instead of Microsoft Azure.\n9\nFloware Vision: Deployment\nFloware possesses 46 sensors, and deploying and updating them eﬀiciently is crucial for maintaining our reputation with clients.\nAs mentioned above, the initial deployment and debugging method used TeamViewer[37] file transfer system. This method was\nnot scalable because it was not designed for this purpose. Deploying 10 sensors required two people to spend one and a half\ndays. Even before refactoring the legacy code, our main challenge was to find scalable and eﬀicient ways to deploy and update\nFloware Vision.\n9.1\nDeployment Strategies\nSetting up a Jetson from scratch can be time-consuming due to the number of dependencies to install, such as Gstreamer,\nDeepstream, and Python bindings. Noticing the significant time loss during this initial setup, we decided to use a container\nsystem. This approach allows us to include all dependencies inside a container and then run it to execute any application stored\ninside a pre-setup environment. We chose to use Docker containers[38] for this purpose.\nWhere to store the created container? Many solutions exist, like Azure Container Registry, GitHub Container Registry, or\n24\n\nDockerHub.\nWe chose Azure Container Registry for its capacity (GitHub Container Registry is restricted to 500MB per\ncontainer) and for practicality, as we already have a startup Azure Subscription.\nBy modifying the Dockerfile8 and refactoring the application, we reduced the container image size from 16 gigabytes to 3.4\ngigabytes.\nWith this container, we have an object we can easily pull and run. The next challenge was scalability: how to automate the\ndeployment process?\n9.2\nIntroduction to Azure IoT\nAccording to Microsoft:\n”IoT Hub is a managed cloud-hosted service that acts as a central message hub for bi-\ndirectional communication between your IoT solution and the devices it manages.” [39]. Azure’s key feature is Device\nManagement; it simplifies the process of adding new devices to the Azure IoT Hub and also allows the monitoring of their\nstatus or performance while updating device firmware over the air. Our main issue with TeamViewer was scalability; Azure\nIoT can manage hundreds of devices simultaneously. Additionally, the security of our devices is improved by having unique\ncredentials per device and managing permissions for each device group with custom user creation.\nEach device can then be recorded as an Azure Edge module and deployed with our Docker images directly from our Azure\nPortal or Visual Studio Code.\n9.3\nCI/CD: Actual Deployment Pipeline\nIf we use Git, a good deployment pipeline also means a good versioning pipeline. We chose to create a Floware GitHub repository\nto store every project the company has. We defined strict conventions to ensure the most secure deployment process. First, we\ndefined the production branch—the one that contains the version of the code we deploy. We protect this branch to avoid any\narbitrary other branch merging or committing without a review. If we want to add a modification to the code, we must create\na new branch with a name reflecting its purpose (e.g., fix/ubertooth, feature/parquetconversion, update/syncfolder).\nWhen the fix or feature update is implemented, the user should create a pull request9 into a development branch. The senior\ndevelopers then review the modifications and decide whether to merge them. When a new version of the code is finished in the\ndevelopment branch, the content is transmitted to a stash branch for testing and debugging purposes, so developers can still\nadd new modifications to the development branch.\nWhen every feature is tested, we merge the code into production. We can now create a GitHub Release from this production\nbranch with a version tag, representing the version of the application (for instance, flwr vision v1.2.3).\n8The file that contains the build instruction of the docker image\n9A request to add these modifications into another branch\n25\n\nFigure 21: Deployment Pipeline representation\nAfter this release, we create a GitHub Action pipeline triggered at every new release that takes place in a Linux arm64\nenvironment (same as a Jetson) that contains a list of automated instructions. Here, the GitHub Action connects to Azure\nContainer Registry with secretly stored login credentials10. The Docker image is then built and pushed to ACR with the version\nas a container tag. We can then deploy the images to all or a fraction of Azure IoT Edge devices and receive metrics and status\nupdates to ensure the deployment goes right.\n10\nResults and Discussion\n10.1\nFloware Vision Performance Metrics\nFloware Vision can use two performance metrics sender, deepstream perf for the framerate of floware vision, and jtop, a software\nthat extracts every property of a jetson in real-time, it can be useful for the creation of failure prediction models, or create\nperformance benchmarks on the different Floware vision version.\n10.2\nCase Studies : Yolo8s vs Yolo8n performance\nYOLOv8s[41] and YOLOv8n are designed for different computational capacities and use cases.\nYOLOv8s is optimized for\na balance between speed and accuracy, making it suitable for applications where moderate hardware resources are available.\nYOLOv8n, on the other hand, is designed for scenarios where minimal computational resources are required, prioritizing speed\nand eﬀiciency. Having a latency issue on Floware Vision, it’s important to know which model has the best accuracy for resources\ntaken.\n10See GitHub Secrets[40]\n26\n\n• Launching Floware Vision\n– YOLOv8s FPS: 8.8 - 9.2\n– YOLOv8n FPS: 17.2 - 17.9\n• One person detection\n– YOLOv8s FPS: 7.6 - 8.4\n– YOLOv8n FPS: 13.0 - 15.6\n27\n\n• End detection\n– YOLOv8s FPS: 7.6 - 8.4\n– YOLOv8n FPS: 13.0 - 15.6\n• Person & license plate detection\n– YOLOv8s FPS: 5.7 - 8.8\n– YOLOv8n FPS: 11.0 - 15.3\nDespite greater GPU usage, we observe better framerate on YOLOv8n and reduced CPU usage, indicated by a flatter courb\ncompared to YOLOv8s. We still need post-process data analysis to judge if the model size loss justifies the better performance,\nas framerate can affect tracking accuracy.\n10.3\nOutput Analysis: the end side of the pipeline\n10.3.1\nVisualization of Flows\nThe first task is visualization. We aggregate sensor data for one or two days using the panda’s library. We determine the central\npoint of the bounding box, then for each object, we draw a line between each point. The challenge lies in reducing the noise\npoints. The linear interpolation method was then used to obtain clean and viewable trajectories.\nFigure 22: Flux Visualization of sensor FLWR-007\n10.3.2\nTransition Matrix\nThis task was carried out in two parts. First, we need to determine regions of interest (ROIs) for each sensor (there are 15\nsensors, so some task automation is required). Then, for a chosen time step, we determine the number of movements from one\nROI to another for each vehicle class. The required output is a matrix in the following form:\ninterval\ntransition\nnb_transitions\nmode\ncapter_name\nverification\n16/09/23 07:00-07:15\nT1-2\n0\ncar\nFLWR-004\nOK\n16/09/23 07:00-07:15\nT1-3\n5\ncar\nFLWR-004\nOK\n16/09/23 07:15-07:30\nT..-..\n...\n...\nFLWR-00...\nOK\n28\n\n10.3.3\nActivity Diagram\nThe required output is a diagram in the form of a CSV document usable for PowerBi, representing the activity time of a sensor\nover a day, week, or month. The sensor is considered inactive if there is no data for an extended period.\n10.3.4\nPost-Alerting\nWe check the files received on the server side, and if any data is missing, we send a Slack alert to address the issue as quickly as\npossible in the following format: [Mission][Importance Level][Sensor Name][Data Type (Bluetooth or Computer Vision]\nNo DATATPE file received since N days, TIME\nExample of an alert output:\n−−−−−−−−−−−−−−−−−−−−\n[ECOFLOW] [RED] [FLWR−017][BT] No BT f i l e\nreceived\nsince 14 days ,\n19:36:31.251712\n[ECOFLOW] [RED] [FLWR−017][CV] No CV f i l e\nreceived\nsince 14 days ,\n19:38:30.996416\n[ECOFLOW] [RED] [FLWR−021][BT] No BT f i l e\nreceived\nsince\n100 days ,\n13:46:31.340826\n[ECOFLOW] [RED] [FLWR−021][CV] No CV f i l e\nreceived\nsince\n100 days ,\n13:02:31.331053\n[ECOFLOW] [RED] [FLWR−022][BT] No BT f i l e\nreceived\nsince 21 days ,\n2:07:31.396548\n[ECOFLOW] [RED] [FLWR−022][CV] No CV f i l e\nreceived\nsince 21 days ,\n2:05:31.379133\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n10.3.5\nQueue Length Measurement\nFor each ROI, we aim to determine the average distance thresholds traveled by each vehicle to classify them as moving, slowing\ndown, or stopping. Then, we choose a distance for each ROI that corresponds to the queue length and their states over time\nbased on the average mobility states of all vehicles. By cumulating the queue lengths for several ROIs according to the direction\nof traﬀic, we obtain a queue length for each congestion period, presented in a table format with columns for time and queue\nlength.\n10.3.6\nProjection\nThe problem with data analysis on our sensors is that distances are relative to the orientation of the sensor or the road, meaning\nthe distances between points do not correspond to real distances. We project the points from the sensor image onto the satellite\nimage using various research methods, including the RANSAC regression algorithm[42].\nFigure 23: Scatter Plot of Detections on Sensor Images\n29\n\nFigure 24: Scatter Plot Projected onto a Satellite Image of the Sensor Location, Still In Testing Stage\n11\nConclusion\nWriting this memoir made me realize the importance and advantages of Floware’s choice to use Edge AI for traﬀic analysis, due\nto its scalability, data privacy in public spaces, and real-time processing capabilities. Additionally, it highlighted the need for\na robust and maintainable architecture and deployment process to ensure trust and reliability for our clients. The future work\nwill focus on completing our deployment plan in parallel with finishing the development of Floware Vision Bedrock. After that,\nthe next few months will be dedicated to training and testing new computer vision models fine-tuned for specific mission tasks.\nFor instance, as part of our partnership with Vinci, we need to fine-tune a YOLO model for classifying various types of trucks.\nIn the long term, Edge AI will not be the only AI paradigm used. Work on flux simulation model and large language model\nfor interrogating our database. I am confident that the continued development and refinement of our Edge AI solutions will\ncontribute significantly to advancements in traﬀic management and urban mobility. We are confident that our ongoing efforts\nto develop and refine our Edge AI solutions will play a significant role in advancing traﬀic management and urban mobility.\n30\n\nReferences\n[1] “Modules et kits de développement pour systèmes Embedded | NVIDIA Jetson.” https://www.nvidia.com/fr-fr/autonomous-\nmachines/embedded-systems/.\n[2] M. Treiber, A. Kesting, and C. Thiemann, “How Much does Traﬀic Congestion Increase Fuel Consumption and Emissions?\nApplying a Fuel Consumption Model to the NGSIM Trajectory Data,”\n[3] “Institut vedecom website.” available at https://www.vedecom.fr/accueil-2/ite/quisommesnous.\n[4] “Moove Lab – L’accélérateur des startups de la mobilité de Station F.” available at https://moove-lab.com/en/.\n[5] “Leonard, la plate-forme de prospective et d’innovation de VINCI.” https://leonard.vinci.com/.\n[6] “Nextmove : Pôle de compétitivité Européen de la mobilité.” https://nextmove.fr/.\n[7] “Ecomesure.” https://ecomesure.com/en.\n[8] “FabMob France.” https://lafabriquedesmobilites.fr.\n[9] “Wintics • Cityvision, video analysis software for urban stakeholders.” https://wintics.com/en/.\n[10] “Collecte & valorisation des données de mobilité.” https://www.alyce.fr/.\n[11] “Accueil, UPcity.” https://www.up-city.be/.\n[12] “Eurovia.” https://www.eurovia.fr/.\n[13] “Accueil | CDVIA.” https://www.cdvia.fr/.\n[14] S. Denning, “What Is Agile?.” https://www.forbes.com/sites/stevedenning/2016/08/13/what-is-agile/.\n[15] “Cloud Computing, sécurité et réseau de diffusion de contenu (CDN).” https://www.akamai.com/fr.\n[16] “What Is Edge AI? | IBM.” https://www.ibm.com/topics/edge-ai, Aug. 2023.\n[17] K.\nChowdhary,\n“Microsoft\nAzure\nHit\nWith\nThe\nLargest\nData\nBreach\nIn\nIts\nHistory;\nHundreds\nOf\nExecu-\ntive Accounts Compromised.” https://techreport.com/news/microsoft-azure-hit-with-the-largest-data-breach-in-its-history-\nhundreds-of-executive-accounts-compromised/, Feb. 2024.\n[18] “Prospera.ag.” https://prospera.ag.\n[19] “Site\noﬀiciel\nFitbit\n:\ncoachs\nélectroniques\npour\nla\nforme\net\nle\nsport,\net\nbien\nplus\nencore.”\nhttps://www.fitbit.com/global/fr/home.\n[20] “What\nIs\nComputer\nVision?\n|\nMicrosoft\nAzure.”\nhttps://azure.microsoft.com/en-us/resources/cloud-computing-\ndictionary/what-is-computer-vision.\n[21] K. Fukushima, “Neocognitron,” Scholarpedia, vol. 2, p. 1717, Jan. 2007.\n[22] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You Only Look Once: Unified, Real-Time Object Detection,” May\n2016.\n[23] S. Sánchez Hernández, H. Romero, and A. Morales, “A review: Comparison of performance metrics of pretrained models\nfor object detection using the tensorflow framework,” IOP Conference Series: Materials Science and Engineering, vol. 844,\np. 012024, 06 2020.\n[24] “Ubertooth One - Great Scott Gadgets.” https://greatscottgadgets.com/ubertoothone/.\n[25] “GStreamer/gstreamer.” GStreamer GitHub mirrors, July 2024.\n[26] “DeepStream SDK.” https://developer.nvidia.com/deepstream-sdk.\n[27] “Ultralytics: Ultralytics YOLOv8 for SOTA object detection, multi-object tracking, instance segmentation, pose estimation\nand image classification..”\n[28] Talin, “The Rise and Fall of Object Oriented Programming,” Nov. 2018.\n[29] S. Akhmechet, “Functional Programming For The Rest of Us.” https://www.defmacro.org/2006/06/19/fp.html, June 2006.\n[30] T. Hänisch, “A Case Study on Using Functional Programming for Internet of Things Applications,” Athens Journal of\nTechnology & Engineering, vol. 3, pp. 29–38, Feb. 2016.\n[31] “What are microservices?.” http://microservices.io/index.html.\n[32] V. Thennakoon and B. Hettige, A STUDY ON OBJECT-ORIENTED DESIGN PRINCIPLES AND PATTERNS. July\n2022.\n[33] “NVIDIA-AI-IOT/deepstream_lpr_app.” NVIDIA AI IOT, June 2024.\n[34] “NVIDIA-AI-IOT/deepstream_python_apps.” NVIDIA AI IOT, July 2024.\n[35] kexugit,\n“C++\n-\nFunctional-Style\nProgramming\nin\nC++.”\nhttps://learn.microsoft.com/en-us/archive/msdn-\nmagazine/2012/august/c-functional-style-programming-in-c, Jan. 2016.\n31\n\n[36] “Azure Blob Storage | Microsoft Azure.” https://azure.microsoft.com/en-us/products/storage/blobs.\n[37] “TeamViewer - Le logiciel de connectivité à distance.” https://www.teamviewer.com/fr/.\n[38] “Overview of the Docker workshop.” https://docs.docker.com/guides/workshop/, 11:50:25 -0700 -0700.\n[39] leestott,\n“Introduction\n-\nTraining.”\nhttps://learn.microsoft.com/fr-fr/training/modules/introduction-to-iot-hub/1-\nintroduction.\n[40] “Using\nsecrets\nin\nGitHub\nActions.”\nhttps://docs.github.com/_next/data/cjISr1MlIQhzdaVAuo8xo/en/free-\npro-team@latest/actions/security-guides/using-secrets-in-github-actions.json?versionId=free-pro-\nteam%40latest&productId=actions&restPage=security-guides&restPage=using-secrets-in-github-actions.\n[41] Ultralytics, “YOLOv8.” https://docs.ultralytics.com/models/yolov8.\n[42] M. Rezaei, M. Azarmi, and F. M. P. Mir, “3d-net: Monocular 3d object recognition for traﬀic monitoring,” Expert Systems\nwith Applications, vol. 227, p. 120253, 2023.\n32\n"
}