{
    "1 - Introduction to RL and MDPs.pdf": "Reinforcement Learning\nAn Introduction\n\nInstructor\nHussam ATOUI\nâ—\nSoftware Engineer at Valeo \nCrÃ©teil (France) since Nov 2022\nâ—\nPhD-Cifre RENAULT & \nGrenoble-Alpes University \n(2019-2022)\nâ—\nSpecialities: Automated Driving, \nReinforcement Learning, \nAutomatic Control, Optimization\n\nInstructor\nVictor MORAND\nmorand@isir.upmc.fr\nâ—\nPhD Student at ISIR (Sorbonne - \nCNRS)\nâ—‹\nExplaining how LLMs manipulate \nKnowledge\nâ—‹\ntowards AIs that know what they \nknow\nâ—\nAlso out of a School of Engineering !\nFeel free to reach out at the end of our \nsessions ! \n\nCourse Content\n1.\nIntroduction to Reinforcement Learning\n2.\nMarkov Decision Processes (MDPs)\n3.\nPolicy and Value Functions\n4.\nDynamic Programming (DP) for RL\n5.\nModel-Free methods\n6.\nValue Function Approximation\n7.\nPolicy-Gradient and Actor-Critic Methods\n8.\nDeep RL\n9.\nTP Project\n\nIntroduction to Reinforcement \nLearning (RL)\n\nSupervised Learning\nReinforcement Learning\nUnsupervised Learning\nTrain with labeled data\nTrain with unlabeled data\nClustering\nTrain with environment \nexperience\nRegression\nClassiï¬cation\n6\nTypes of Learning\n\nSupervised Learning \nModel\nInputs:\nFeatures / States\nPredicted Outputs:\nValue/ Class\nTraining target:\nTarget Output\nâ—\nError: Target Output - Predicted Output\nâ—\nObjective: Minimize the error between the target and the predicted output\n7\nSupervised Learning\n\nSupervised Learning\n\nReinforcement Learning\nReinforcement \nLearning Model\nInputs:\nFeatures / States\nPredicted Outputs:\nActions\nEvaluation:\nRewards / Penalties\nâ—\nError: Awards - Penalties \nâ—\nObjective: Maximize the awards and decrease penalties as much as possible\n9\n\nReinforcement Learning\n\nExamples of Rewards [1]\nâ—\nFly stunt manoeuvres in a helicopter\nâ—‹\n+ve reward for following desired trajectory\nâ—‹\nâˆ’ve reward for crashing\nâ—\nManage an investment portfolio\nâ—‹\n+ve reward for each $ in bank\nâ—\nControl a power station\nâ—‹\n+ve reward for producing power\nâ—‹\nâˆ’ve reward for exceeding safety thresholds\nâ—\nMake a humanoid robot walk\nâ—‹\n+ve reward for forward motion\nâ—‹\nâˆ’ve reward for falling over\nâ—\nPlay many different Atari games better than humans\nâ—‹\n+/âˆ’ve reward for increasing/decreasing score\n11\n\nAgent and Environment\nActions   A(t)\nObservations   O(t) \nRewards   R(t) \nAgent\nEnvironment\nAt step t: \nThe Agent:\nâ—\nReceives O(t)\nâ—\nReceives R(t)\nâ—\nExecutes A(t)\nThe Environment:\nâ—\nReceives A(t)\nâ—\nEmits O(t+1)\nâ—\nEmits R(t+1)\nt++\n12\n\nFully Observable Environment\nObservations   O(t) \nAgent\nEnvironment\nâ—\nEnvironment observations = Agent \nstate\nâ—\nThis is assumed in Markov Decision \nProcess (MDP)\n13\n\nPartially Observable Environment\nObservations   O(t) \nAgent\nEnvironment\nâ—\nEnvironment observations â‰  Agent state\nâ—‹\nA drone navigating a forest only sees \nnearby obstacles.\nâ—‹\nA healthcare agent observes patient \nsymptoms but not the underlying \ndisease.\nâ—‹\nA self-driving car detects nearby \nvehicles but not hidden pedestrians.\nâ—‹\nA weather forecasting model \nobserves recent conditions but not \nfuture patterns\nâ—\nThis is called Partially Observable Markov \nDecision Process (POMDP)\n(Missing info)\n14\n\nReinforcement Learning\n15\nAgent: The system that takes \nactions to be trained.\nEnvironment: The external \nsystem with which the agent \ninteracts.\nState: The information \nrequired by the agent to take \nan action. This info is observed \nfrom the environment.\nAction: The decision or \nmove that the agent makes \nat a particular state\nReward: Feedback received \nby the agent to evaluate the \ntaken action under a certain \nstate.\nGeneral Architecture\n\n16\nReinforcement Learning\n\nPolicy\nRL Agent\nA policy deï¬nes the agentâ€™s behavior in the environment. It \nrepresents a mapping from states to actions, for example:\nâ—\nDeterministic policy:                   , \nwhere the action a is chosen directly based on state s.\nâ—\nStochastic policy:                                                   ,\nwhere the policy gives the probability of taking action \na given state s.\n17\n\nValue Function\nRL Agent\nA value function \nâ—\nestimates the expected future reward \nâ—\nassesses the quality of states, helping to determine the best actions to \ntake. \nFor example, the state value under policy ðœ‹ is given by:\nThis equation expresses the expected sum of discounted rewards starting from \nstate ð‘ .\n18\n\nValue Function\nRL Agent\nA value function \nâ—\nestimates the expected future reward \nâ—\nassesses the quality of states, helping to determine the best actions to \ntake. \nFor example, the state value under policy ðœ‹ is given by:\nThis equation expresses the expected sum of discounted rewards starting from \nstate ð‘ .\n19\n\nValue Function\nRL Agent\n20\nÎ³âˆˆ[0,1]:\nâ—\nIf Î³=0, the agent focuses solely on immediate rewards.\nâ—\nIf Î³=1, future rewards are valued equally to immediate rewards.\n\nModel\nRL Agent\nA model forecasts the environment's next state and expected reward:\nâ—\nð‘ƒ represents the probability of the next state given the current state and \naction:\nâ—\nð‘… represents the expected immediate reward given the current state and \naction:\n21\n\nStates, Actions, Rewards\nExample: Maze [1]\nâ—\nStates: Agentâ€™s location\nâ—\nActions: Right, Left, Up, Down\nâ—\nRewards: -1 per time-step\n22\n\nPolicy\nExample: Maze [1]\nArrows represent policy Ï€(s) for \neach state s\n23\n\nExample: Maze [1]\nNumbers represent value           of \neach state s\n24\n\nDifferent Types\nRL Agents\nâž”\nValue-based:\nâ—†\nNo Policy\nâ—†\nValue Function\nâž”\nPolicy-based:\nâ—†\nPolicy\nâ—†\nNo Value Function\nâž”\nActor-Critic:\nâ—†\nPolicy\nâ—†\nValue Function\nâž”\nModel-free:\nâ—†\nPolicy and/or Value Function\nâ—†\nNo Model\nâž”\nModel-based:\nâ—†\nPolicy and/or Value Function\nâ—†\nModel\n25\n\nMarkov Decision Processes (MDPs)\n\nMarkov Process\nâ—\nA Markov Process is a memoryless process where the future state depends only \non the current state and not on any past states.\nâ—\nFormally, a Markov Process is a tuple: M=(S,P)\nWhere:\nâ—\nS: A ï¬nite set of states.\nâ—\nP: Transition probabilities between states, deï¬ned as:\n27\n\nThe Markov Property\nâ—\nMarkov property: Future depends only on the present, not past states\nâ—\nSimpliï¬es state transition modeling\n28\n\nâ—\nA Markov Reward Process is a Markov Process with added rewards.\nâ—\nIt is represented as a tuple: MR=(S,P,R,Î³)\nWhere:\nâ—\nR(s): Reward function providing the expected reward at each state s, \nâ—\nÎ³: Discount factor, controlling the importance of future rewards.\n29\nMarkov Reward Process\n\nCumulative Reward - Gain\nMarkov Reward Process\nâ—\nCumulative Reward G(t) : Expected cumulative reward from state s\n30\nâ—\n(State-)Value Function v(s) : Expected state-value of state s\n\nState-Value Function\nBellman Equation\nâ—\nThe state-value function can be presented as an immediate reward and future reward \nas follows:\nPROOF?\n31\n\nProof\nBellman Equation\nStochastic Eq.\n?\n32\n\nExample\n\nExample: Student MRP (P, S, R) [1] \n34\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n35\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n36\n\nExercise\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n38\n?\n\nExample of Bellmanâ€™s equation\nExample: Student MRP (P, S, R) [1]  \n39\n\nMRP â†’ MDP\n(P, S, R) â†’ (P, S, A, R)\n\nMarkov Decision Process (MDP)\n A Markov decision process is a 4-tuple (S, A, P, R):\nNote: A ï¬nite MDP is an MDP with ï¬nite state, action, and reward \nsets. Much of the current theory of reinforcement learning is \nrestricted to ï¬nite MDPs.\nâ—\nStates (S): Describe environment situations\nâ—\nActions (A): Choices available to the agent\nâ—\nRewards (R): Immediate feedback for actions\nâ—\nTransition Probabilities (P): Likelihood of reaching a \nnew state\n41\n\nState Transitions - Policy\nMarkov Decision Process\nâ—\nTransition probability: P(sâ€²âˆ£s,a)\nâ—\nModels probability of moving to sâ€² from s after action a\n42\n\nReward Function and Policy\nMarkov Decision Process\nâ—\nReward function R(s,a): Immediate feedback\nâ—\nPositive rewards encourage actions; negative prevent actions\nâ—\nDeterministic policy:                   , \nwhere the action a is chosen directly based on state s.\nâ—\nStochastic policy:                                                   ,\nwhere the policy gives the probability of taking action a given \nstate s.\n43\n\nPolicies\nMarkov Decision Process\nâ—\nDeterministic policy: \n                  \nwhere the action a is chosen directly based on state s.\nâ—\nStochastic policy:         \n                                          \nwhere the policy gives the probability of taking action a given state s.\n44\n\nExample: Student MDP (P, S, A, R) [1]  \n45\n\nValue Functions\nMarkov Decision Process\nâ—\nState-value function : Expected cumulative reward from state s under policy Ï€\n46\nâ—\nAction-value function: Expected reward of taking action a in state s under policy Ï€\n\nState-Value Function\nMarkov Decision Process\n47\n\nBellman Expectation Equation\n48\nâ—\nState-value function : Expected cumulative reward from state s under policy Ï€\nâ—\nAction-value function: Expected reward of taking action a in state s under policy Ï€\n\nBellman Expectation Equation [1]\n49\n\nBellman Expectation Equation [1]\n50\n\nBellman Expectation Equation [1]\n51\n\nBellman Expectation Equation [1]\n52\n\nExercise\n\nExample: Student MDP\n54\n?\n\nExample: Student MDP\n55\n\nState-Value and Action-Value Functions\nBellman Optimality\nâ—\nThe optimal state-value function\nâ—\nOptimal action-value function\n56\n\nExercise: Optimal State-Value Function [1]\n57\n\nExercise: Optimal Action-Value Function [1]\n58\n\nFind an Optimal Policy\nâ—\nAn optimal policy Ï€âˆ— can be determined by selecting actions that \nmaximize the optimal action-value function qâˆ—(s,a). The optimal policy Ï€âˆ—\n(aâˆ£s) is deï¬ned as:\nâ—\nFor any MDP, there is always a deterministic optimal policy. If qâˆ—(s,a) is \nknown, we can directly derive the optimal policy from it.\n59\n\nExercises\n\nExercise 1: Understanding Policies\nQuestion:\nLet S={s1,s2} be a set of two states and A={a1,a2} be a set of two actions. Suppose a \nstochastic policy Ï€ is deï¬ned as follows:\n1.\nWhat is the probability of taking action a2  in state s1  under this policy?\n2.\nIf the agent is in state s2 , what is the probability of taking action a1  under this \npolicy?\n61\n\nExercise 1: Understanding Policies\nSolution:\n1.\nThe probability of taking action a2  in state s1  is given directly by Ï€(a2 |s1 )=0.3\n2.\nThe probability of taking action a1  in state s2  is given by Ï€(a1 âˆ£s2 )=0.4\n62\n\nExercise 2: State-Value Function\nQuestion:\nConsider a simple MDP with two states s1  and s2  and a single action a with the \nfollowing reward structure:\nâ—\nStarting from s1  and taking action a, the agent moves to s2  with a reward of 5.\nâ—\nStarting from s2  and taking action a, the agent stays in s2  and receives a \nreward of 3.\nAssuming a discount factor Î³=0.9 and a deterministic policy where action a is \nalways taken, compute the value of each state v(s1 ) and v(s2 ).\n63\n\nExercise 2: State-Value Function\nSolution:\nThe Bellman equation for the value of each state s is:\n1.\nFor s2 :\nSolving for v(s2 ) â†’ v(s2)=30\n2.\nFor s1 :\nThus, v(s1)=32 and v(s2)=30.\n64\n\nExercise 3: Action-Value Function\nQuestion:\nUsing the same MDP setup as in Exercise 2, calculate the action-value q(s1 ,a) and \nq(s2 ,a) for each state-action pair.\n65\n\nExercise 3: Action-Value Function\nSolution:\nThe Bellman equation for the action-value function is:\nUsing the state values calculated in Exercise 2:\n66\n\nExercise 4: Bellman Optimality Equation\nQuestion:\nSuppose we have an MDP with three states S={s1,s2,s3} and two actions A={a1,a2}. \nThe reward function and transitions are given below:\nâ—\nFrom s1  taking a1  leads to s2  with reward 4.\nâ—\nFrom s1  taking a2  leads to s3  with reward 2.\nâ—\nFrom s2  taking a1  leads to s3  with reward 5.\nâ—\nFrom s3  taking a1  or a2  leads back to s3  with reward 3.\nAssuming a discount factor Î³=0.9, write the Bellman optimality equation for vâˆ—(s1 ).\n67\n\nExercise 4: Bellman Optimality Equation\n68\nSolution:\nThe Bellman optimality equation for the state-value function is:\nSubstituting the rewards:\nTo solve this, we would need the values of v*(s2) and v*(s3), which can be calculated \nrecursively by applying the Bellman optimality equation to each state.\n\nExercise 4: Bellman Optimality Equation\n69\nSolution:\nThe optimal values for each state are: \nâ—\nv*(s1) = 32.8\nâ—\nv*(s2) = 32\nâ—\nv*(s3) = 30\n\nExercise 5: Optimal Policy Derivation\nQuestion:\nIf the optimal action-value function qâˆ—(s,a) for some state s is given by:\nâ—\nqâˆ—(s,a1 )=12\nâ—\nqâˆ—(s,a2 )=10\nWhat is the optimal policy Ï€âˆ—(aâˆ£s)?\n70\n\nExercise 5: Optimal Policy Derivation\nSolution:\nThe optimal policy Ï€âˆ—(aâˆ£s) chooses the action that maximizes qâˆ—(s,a).\nSo:\n \nThus, the optimal policy is to always choose action a1  in state s, since qâˆ—(s,a1 )>qâˆ—\n(s,a2 ).\n71\n\nExploration & Exploitation\n\nExploration vs. Exploitation\nâ—\nIn RL, the agent faces a dilemma between:\nâ—‹\nExploration: Trying new actions to discover valuable \noutcomes. (can be harmfulâ€¦)\nâ—‹\nExploitation: Choosing actions that have yielded high \nrewards in the past.\n73\nâ—\nGoal: Balance exploration and exploitation to maximize rewards over time.\nâ—\nChallenge: Too much exploration can delay achieving rewards, while too \nmuch exploitation can lead to suboptimal long-term results.\n\nExploration: Discovering New Opportunities\nâ—\nExample 1 - A robot navigating a maze:\nâ—‹\nThe robot tries unfamiliar paths to locate shorter routes or more valuable \nrewards.\nâ—\nExample 2 - A recommendation system:\nâ—‹\nOccasionally recommends new, lesser-known products to a user to \nlearn their interests.\nâ—\nBeneï¬t: Exploration can uncover higher rewards that arenâ€™t immediately \nobvious.\n74\n\nExploitation: Leveraging Known Information\nâ—\nExample 1 - A trading agent:\nâ—‹\nSelects stocks it has previously identiï¬ed as proï¬table, prioritizing \nconsistency over discovering new options.\nâ—\nBeneï¬t: Exploitation capitalizes on known successes, ensuring steady rewards.\nâ—\nExample 2 - A game-playing AI:\nâ—‹\nRepeats a high-reward move (e.g., a chess opening) that has led to victories \nin past games.\n75\n\nAny Questions ? \nDonâ€™t hesitate to contact me\nmorand@isir.pmc.fr\n\nAny Questions ? \nContact us !\nhussam.atoui@valeo.com\n\nReferences\n78\n[1] David Silver, Lectures on Reinforcement Learning, 2015\n[2] Reinforcement Learning and Advanced Deep Learning (Sorbonne) - Olivier \nSigaud\n[3] Sutton, R. S. and Barto, A. G. (2018), Reinforcement Learning: An Introduction \n(Second edition). MIT Press\n \nOlivier Sigaud Youtube Channel\n",
    "19Thales.pdf": "1 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \nð´ðµâ€²\nð´ðµ    =    ð´ð¶â€²\nð´ð¶    =    ðµâ€²ð¶â€²\nðµð¶ \nTHÃ‰ORÃˆME DE THALÃˆS \n Tout le cours en vidÃ©o : https://youtu.be/puuHhlf0jAQ \n \n \nThalÃ¨s serait nÃ© autour de 625 avant J.C. Ã  Milet en Asie Mineure (actuelle Turquie). ConsidÃ©rÃ© comme \nl'un des sept sages de l'AntiquitÃ©, il est Ã  la fois mathÃ©maticien, ingÃ©nieur, philosophe et homme d'Etat \nmais son domaine de prÃ©dilection est l'astronomie.  \nIl aurait prÃ©dit avec une grande prÃ©cision l'Ã©clipse du soleil du 28 mai de l'an - 585. Ce n'est peut-Ãªtre \nqu'une lÃ©gende, ThalÃ¨s en explique cependant le phÃ©nomÃ¨ne. \nCurieusement, le fameux thÃ©orÃ¨me de ThalÃ¨s n'a pas Ã©tÃ© dÃ©couvert par ThalÃ¨s. Il Ã©tait dÃ©jÃ  connu \navant lui des babyloniens et ne fut dÃ©montrÃ© qu'aprÃ¨s lui par Euclide d'Alexandrie. \n \n \n \n \nPartie 1 : Le thÃ©orÃ¨me de ThalÃ¨s Â« version triangles emboÃ®tÃ©s Â» (Rappel) \n \nAnimation : http://www.maths-et-tiques.fr/telech/Thales4.ggb \n \nLE THÃ‰ORÃˆME DE THALÃˆS \n \nSoit deux triangles ð´ðµð¶ et ð´ðµâ€™ð¶â€™, tels que : \nð´, ðµ, ðµâ€™ et ð´, ð¶, ð¶â€™ sont alignÃ©s. \n \nSi (ðµâ€™ð¶â€™)//(ðµð¶) \n \nalors :  \n!\"!\n!\" = \n!#!\n!# =\n\"!#!\n\"#  \n \n  \n \nComment retenir le thÃ©orÃ¨me de ThalÃ¨s ? \n \nð´ðµð¶ et ð´ðµâ€™ð¶â€™ sont deux triangles en situation de ThalÃ¨s : ils ont un sommet commun ð´, et deux cÃ´tÃ©s \nparallÃ¨les (ðµâ€™ð¶â€™) et (ðµð¶). \nUn triangle est un Â« agrandissement Â» de lâ€™autre. Ils ont donc des cÃ´tÃ©s deux Ã  deux proportionnels.  \nOn obtient la formule de ThalÃ¨s : \n \n        Le petit triangle ð´ðµâ€™ð¶â€™ \n \n        Le grand triangle ð´ðµð¶ \n \n \n \n \n       \n           1ers cÃ´tÃ©s             2Ã¨mes cÃ´tÃ©s              3Ã¨mes cÃ´tÃ©s \n \n \nSavoir utiliser : http://www.maths-et-tiques.fr/telech/thales_ecrire.pdf \n \n \n \n\n2 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \nMÃ©thode : Calculer une longueur Ã  lâ€™aide du thÃ©orÃ¨me de ThalÃ¨s  \n \n VidÃ©o https://youtu.be/zP16D2Zrv1A \n \nSur la figure ci-dessous, les triangles ðµð¶ð¹ et ðµð·ð¸ sont tels que (ð¶ð¹) et (ð·ð¸) sont parallÃ¨les.  \nCalculer : a) ðµð¸    b) ðµð· \nDonner la valeur exacte et Ã©ventuellement lâ€™arrondi au dixiÃ¨me. \n \n \n \n \n \n \n \n \n \n \n \n \nCorrection \n \na) Les triangles ðµð¶ð¹ et ðµð·ð¸ sont en situation de ThalÃ¨s car (ð¶ð¹) // (ð·ð¸), donc : \n \n                   ðµð¶\nðµð·= ðµð¹\nðµð¸= ð¶ð¹\nð·ð¸ \n \n                   4\nðµð·= 4,5\nðµð¸= 3\n7 \n \n                               4,5\nðµð¸= 3\n7 \nSoit : ðµð¸ =  4,5 Ã— 7 âˆ¶3 = 10,5   \n \nb) On a :  \n$\n\"% =\n$,'\n\"( =\n)\n*  \n               \n                               4\nðµð·= 3\n7 \nSoit : ðµð·= 4 Ã— 7 : 3 = \n+,\n)  (Valeur exacte) \n                                       Â» 9,3 (Valeur arrondie) \n \n \n \n \n \n \n \nE \nD \nC \nB \nF \n7 \n3 \n4,5 \n4 \nÃ— \n: \n\n3 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \nPartie 2 : Le thÃ©orÃ¨me de ThalÃ¨s Â« version papillon Â» \n \nAnimation : http://www.maths-et-tiques.fr/telech/Thales.ggb \n \n \n \nLE THÃ‰ORÃˆME DE THALÃˆS \n \nSoit deux triangles ð´ðµð¶ et ð´ðµâ€™ð¶â€™, tels que : \nð´, ðµ, ðµâ€™ et ð´, ð¶, ð¶â€™ sont alignÃ©s. \n \nSi (ðµâ€™ð¶â€™)//(ðµð¶) \n \nalors :  \n!\"!\n!\" = \n!#!\n!# =\n\"!#!\n\"#  \n \n \n \n \n \n \n \n \nMÃ©thode : Calculer une longueur Ã  lâ€™aide du thÃ©orÃ¨me de ThalÃ¨s  \n \n VidÃ©o https://youtu.be/cq3wBbXYB4A  \n \nLes triangles ðµð´ð¸ et ðµð·ð¶ sont tels que les droites  \n(ð´ð¸) et (ð¶ð·) sont parallÃ¨les. \nOn donne : ðµð¸= 2 ð‘ð‘š, ðµð·= 5 ð‘ð‘š, et ð¶ð·= 6 ð‘ð‘š. \nCalculer ð´ð¸.  \n \n \nCorrection \nLes triangles ðµð´ð¸ et ðµð·ð¶ sont en situation de ThalÃ¨s car (ð´ð¸) et (ð¶ð·) sont parallÃ¨les, donc : \n \n     ðµð´\nðµð¶= ðµð¸\nðµð·= ð´ð¸\nð¶ð· \n \n     ðµð´\nðµð¶= 2\n5 = ð´ð¸\n6  \n   \n          2\n5 = ð´ð¸\n6  \n \nEt donc ð´ð¸= 6 Ã— 2 : 5 = 2,4 ð‘ð‘š.  \n \nActivitÃ©s de groupe : Le paradoxe de Lewis Carroll \nhttp://www.maths-et-tiques.fr/telech/L_CARROLL.pdf \n \nCâ€™ \nBâ€™ \nA \nB \nC \nE \nD \nC \n \nB \nA \n\n4 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \n \n \nDes hauteurs inaccessibles \nhttp://www.maths-et-tiques.fr/telech/haut_inacc.pdf \nhttp://www.maths-et-tiques.fr/index.php/expositions-deleves/hauteurs-inaccessibles \n \n \n \nPartie 3 : La rÃ©ciproque du thÃ©orÃ¨me de ThalÃ¨s \n \nAnimation : http://www.maths-et-tiques.fr/telech/RThales.ggb \n \n \nLA RÃ‰CIPROQUE DU THÃ‰ORÃˆME DE THALÃˆS \n \n \n     Si les points ð´, ðµ, ðµâ€™ sont alignÃ©s dans  \n     le mÃªme ordre que les points ð´, ð¶, ð¶â€™  \n     et \n!\"!\n!\" =\n!#!\n!#  \n                      \n                        alors (ðµâ€™ð¶â€™)//(ðµð¶) \n         \n       ThalÃ¨s de Milet (-624 ; -546) \n \nVersion Â« triangles emboitÃ©s Â» \n \n \nVersion Â« papillon Â» \n \n \n \n \n \n \n \n \n \n \n \n \n \nMÃ©thode : DÃ©montrer que deux droites sont parallÃ¨les  \n VidÃ©o https://youtu.be/uaPicwUSQz0 \n \nSur la figure ci-contre, les points ð´, ð¶, ð¸ sont alignÃ©s et les  \npoints ðµ, ð¶, ð· sont Ã©galement alignÃ©s dans le mÃªme ordre. \nLes droites (ð´ðµ) et (ð·ð¸) sont-elles parallÃ¨les ?  \n \n \n \n \nA \nBâ€™ \nB \nCâ€™ \nC \nCâ€™ \nBâ€™ \nA \nB \nC \nB \nC \n \nD \nE \nA \n3 \n4,5 \n6 \n4 \n\n5 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \nCorrection \nâ— Dâ€™une part : \n#!\n#( =\n)\n$ = 0,75 \nâ— Dâ€™autre part : \n#\"\n#% =\n$,'\n- = 0,75 \n \nDonc : \n#!\n#( =\n#\"\n#% \nDe plus les points ð´, ð¶, ð¸ sont alignÃ©s dans le mÃªme ordre que les points ðµ, ð¶, ð·. \nDâ€™aprÃ¨s la rÃ©ciproque du thÃ©orÃ¨me de ThalÃ¨s, on peut conclure que les droites (ð´ðµ) et (ð·ð¸) \nsont parallÃ¨les. \n \nMÃ©thode : DÃ©montrer que deux droites ne sont pas parallÃ¨les \n \n VidÃ©o https://youtu.be/ovlhagzONlw \n \n                                         \n                                                            Les droites (ð‘ƒð‘…) et (ð·ð¸) sont-elles parallÃ¨les ? \n \n \n                                          \n \n \n \nCorrection \n \nâ€¢ Dâ€™une part : \n#.\n#% =\n$\n- â‰ˆ0,67 \nâ€¢ Dâ€™autre part : \n#/\n#( =\n+,'\n$ = 0,625 \nDonc :  \n#.\n#% â‰ \n#/\n#(\n \nOn ne peut pas utiliser la rÃ©ciproque du thÃ©orÃ¨me de ThalÃ¨s. \n(ð‘ƒð‘…) et (ð·ð¸) ne sont pas parallÃ¨les. \n \n \n \nLors dâ€™un voyage en Egypte, ThalÃ¨s de Milet (-624 ; -546) aurait mesurÃ© la hauteur de la pyramide de \nKheops par un rapport de proportionnalitÃ© avec son ombre. \nCitons : Â« Le rapport que jâ€™entretiens avec mon ombre est le mÃªme que celui que la pyramide entretient \navec la sienne. Â» \nPar une relation de proportionnalitÃ©, il obtient la hauteur de la pyramide grÃ¢ce Ã  la longueur de son \nombre. \nL'idÃ©e ingÃ©nieuse de ThalÃ¨s est la suivante : Â« A lâ€™instant oÃ¹ mon ombre sera Ã©gale Ã  ma taille, l'ombre de \nla pyramide sera Ã©gale Ã  sa hauteur. Â» \n \n \nHors du cadre de la classe, aucune reproduction, mÃªme partielle, autres que celles prÃ©vues Ã  l'article L 122-5 du code de \nla propriÃ©tÃ© intellectuelle, ne peut Ãªtre faite de ce site sans l'autorisation expresse de l'auteur. \nwww.maths-et-tiques.fr/index.php/mentions-legales \n \n",
    "h1-2-les-regimes-totalitaires.pdf": "H1-2 LES REGIMES TOTALITAIRES  \nA LA RECHERCHE Dâ€™UNE NOUVELLE GEOPOLITIQUE EUROPEENNE \n \nIntroduction : La PremiÃ¨re Guerre mondiale, par la brutalisation des sociÃ©tÃ©s, est un bouleversement profond pour \nles pays europÃ©ens. Si en Russie, les rÃ©volutions aboutissent Ã  la mise en place progressive dâ€™un rÃ©gime communiste, \nla PremiÃ¨re Guerre mondiale fragilise les dÃ©mocraties. Par ses traitÃ©s, elle fait naÃ®tre en Allemagne et en Italie des \nmouvements politiques antidÃ©mocratiques violents qui prospÃ¨rent sur les difficultÃ©s Ã©conomiques et la montÃ©e du \ncommunisme. Des rÃ©gimes que lâ€™on qualifiera plus tard de totalitaires se mettent en place en Russie, en Italie et en \nAllemagne. \nProblÃ©matique : Pourquoi qualifier les rÃ©gimes russe, allemand et italien de totalitaires et comment ont-ils fait \nbasculer lâ€™Europe et le monde dans la guerre ?  \nI. \nDes racines et des pratiques communes. \nA. \nDes rÃ©gimes issus de la brutalisation des sociÃ©tÃ©s. \nLâ€™impact de la Grande Guerre : Le rÃ©gime communiste qui sâ€™installe progressivement en Russie est nÃ© dans le contexte \nde la PremiÃ¨re Guerre mondiale, avec le soulÃ¨vement de la population et des mutineries qui aboutissent Ã  la chute du \nTsar Nicolas II, remplacÃ© par un gouvernement provisoire qui est Ã  son tour renversÃ© par un coup dâ€™Ã©tat construit par \nles Bolcheviks, mouvement minoritaire communiste dirigÃ© par LÃ©nine (octobre 1917). En Italie, Benito Mussolini prend \nles rÃªnes du PNF, parti national fasciste dans un pays qui se sent trahi par la Â« victoire mutilÃ©e Â» et rÃ©clame les terres \nirredente quâ€™elle nâ€™a pas obtenu Ã  lâ€™issue des traitÃ©s de paix. Alors que les grÃ¨ves se multiplient, Mussolini sâ€™appuie sur \nles Squadre ou chemises noires (groupe paramilitaire majoritairement composÃ© de soldats dÃ©mobilisÃ©s qui mÃ¨nent \ndes opÃ©rations violentes) pour crÃ©er un climat de violence favorable Ã  sa nomination Ã  la tÃªte de lâ€™Italie (marche sur \nRome en octobre 1922). En Allemagne, la RÃ©publique de Weimar, nÃ©e en 1918, est considÃ©rÃ©e par les partis politiques \ndâ€™extrÃªme droite comme responsable de la dÃ©faite et de lâ€™humiliation de Versailles. \nLe poids de la crise Ã©conomique : Lâ€™autre racine commune Ã  ces rÃ©gimes totalitaires est la misÃ¨re. En Russie, la \nPremiÃ¨re Guerre mondiale, puis la guerre civile qui oppose les communistes aux Â« Blancs Â» entre 1917 et 1922, crÃ©Ã© \nles conditions nÃ©cessaires Ã  lâ€™Ã©tablissement dâ€™un rÃ©gime violent dans un pays dÃ©jÃ  marquÃ© par une grande misÃ¨re. La \ncrise Ã©conomique trÃ¨s grave que connaÃ®t lâ€™Italie aprÃ¨s la PremiÃ¨re Guerre mondiale est aussi un facteur expliquant \nlâ€™arrivÃ©e au pouvoir de Mussolini. Mais câ€™est en Allemagne que le poids de la crise Ã©conomique est le plus fort dans la \nmise en place du rÃ©gime totalitaire. Si le NSDAP, le parti nazi est prÃ©sent dÃ¨s les annÃ©es 20 en Allemagne, il reste \nmarginal et ne connaÃ®t pas de succÃ¨s politiques majeurs avant lâ€™arrivÃ©e de la crise Ã©conomique de 1929 en Allemagne \nqui crÃ©Ã© les conditions favorables Ã  la nomination dâ€™Hitler comme chancelier en janvier 1933, dans un contexte de \nviolence extrÃªme organisÃ©e par les SA (sturmabteilung, organisation paramilitaire issue du parti nazi) et de lutte avec \nleurs principaux opposants, le parti communiste allemand. \nB. \nUne sociÃ©tÃ© contrÃ´lÃ©e. \nUn culte du chef organisÃ© par une propagande puissante : Dans les rÃ©gimes totalitaires, la place du chef est centrale \net un vÃ©ritable culte se met en place autour de ces guides, de ces leaders quâ€™il faut Ã©couter et suivre aveuglÃ©ment. De \nmaniÃ¨re diffÃ©rente, ils occupent chacun lâ€™espace mÃ©diatique grÃ¢ce Ã  une propagande savamment orchestrÃ©e par des \nhommes comme Joseph Goebbels, le ministre allemand de lâ€™information et de la Propagande. Le Duce, le Vojd ou le \nFÃ¼hrer sont omniprÃ©sents, ne se trompent jamais et se sacrifient pour leur nation, invitant les populations Ã  faire de \nmÃªme. De grandes manifestations sportives ou politiques sont organisÃ©es, mettant en scÃ¨ne cette adhÃ©sion populaire \ncomme Ã  Nuremberg ou bien encore lors des festivitÃ©s de la rÃ©volution dâ€™octobre sur la Place Rouge Ã  Moscou. \nDes sociÃ©tÃ©s encadrÃ©es : Dans les Ã©tats totalitaires, lâ€™individu doit sâ€™effacer au nom de la construction dâ€™une sociÃ©tÃ© \nnouvelle, dâ€™un homme nouveau. Chaque moment de la vie sociale est encadrÃ© par une organisation issue du parti \nunique : les organisations de jeunesse (Jeunesses hitlÃ©riennes, Balilla et Avant-gardistes, Pionniers et Komsomols), le \n\ntravail (Front du travail, soviet), la vie sociale et politique au sein du parti qui devient un ascenseur social (PCUS, parti \nfasciste, parti nazi). En Allemagne, lâ€™organisation Kraft durch Freude (la Force par la joie) organise les temps libres et \nles vacances des travailleurs.  Lâ€™ensemble de la vie politique est politisÃ© et les rÃ©sistances restent faibles, malgrÃ© \ncertaines tentatives au sein des milieux catholiques en Italie et en Allemagne et orthodoxes en URSS. \n \nC. \nUne Ã©conomie dirigÃ©e. \nLe dirigisme dâ€™Ã©tat allemand et italien : En Italie, Mussolini lance des programmes ambitieux pour faire de son pays \nune grande puissance industrielle et agricole autour de lâ€™assÃ¨chement des terres insalubres. Avec le contrecoup de la \ncrise Ã©conomique, le dirigisme italien sâ€™accentue. Lâ€™IRI (institut pour la reconstruction industrielle) est crÃ©Ã© en 1933 et \ncontrÃ´le une large part de lâ€™industrie italienne. En Allemagne, le mÃªme schÃ©ma de politiques de grands travaux se met \nen place, mise en lumiÃ¨re par une propagande intensive. Lâ€™autre chemin choisi par lâ€™Allemagne et lâ€™Italie est celui de \nlâ€™autarcie, câ€™est-Ã -dire dâ€™une Ã©conomie fermÃ©e. La bataille de lâ€™emploi nâ€™est gagnÃ©e en Allemagne que dans la cadre du \nrÃ©armement (en 1939, les 2/3e du revenu national sont consacrÃ©s au rÃ©armement) et au prix dâ€™une baisse importante \ndu pouvoir dâ€™achat et dâ€™un pillage industriel des pays annexÃ©s. Mais le recul du chÃ´mage explique en partie lâ€™adhÃ©sion \nde la population aux rÃ©gimes totalitaires. \nLa politique Ã©conomique de Staline : En 1929, Staline lance son pays dans une grande rÃ©forme Ã©conomique : il sâ€™agit \nde faire de lâ€™URSS une grande puissance industrielle. Pour y arriver, lâ€™Ã‰tat nationalise lâ€™ensemble de lâ€™Ã©conomie et fixe \ndes objectifs de production (planification) Ã  lâ€™industrie lourde. Dans un pays largement agricole, lâ€™agriculture est \nsacrifiÃ©e pour financer lâ€™industrialisation forcÃ©e. La propriÃ©tÃ© privÃ©e est supprimÃ©e et les terres sont regroupÃ©es dans \ndes fermes collectives dâ€™Ã‰tat (sovkhozes). La dÃ©sorganisation totale du monde agricole entraÃ®ne des famines, \nattribuÃ©es aux koulaks (paysans opposÃ©s Ã  la collectivisation) mais le rÃ©gime glorifie les nouveaux hÃ©ros comme \nStakhanov, mineur qui aurait produit 14 fois plus que les objectifs. \nII. \nDes idÃ©ologies diffÃ©rentes qui lÃ©gitiment la violence. \nA. \nLe socialisme soviÃ©tique. \nLe socialisme soviÃ©tique : Le parti bolchevik qui prend le pouvoir Ã  partir de 1917 sous la tutelle de LÃ©nine sâ€™appuie \nsur le communisme. Il sâ€™agit dâ€™une idÃ©ologie qui vise Ã  la crÃ©ation dâ€™une sociÃ©tÃ© Ã©galitaire sans classe. Pour y parvenir, \nles ouvriers (les prolÃ©taires) doivent faire la rÃ©volution et imposer des rÃ©formes : câ€™est la dictature du prolÃ©tariat. Pour \nles soviÃ©tiques, le communisme doit sâ€™Ã©tablir dans le monde entier et lâ€™URSS doit aider Ã  la mise en place dâ€™une \nrÃ©volution mondiale. Câ€™est le rÃ´le du Komintern, organisation internationale communiste dont le siÃ¨ge est Ã  Moscou.  \nStaline, qui sâ€™empare progressivement du pouvoir en 1927, pousse Ã  la naissance de lâ€™Homme nouveau. \nLa Terreur stalinienne au cÅ“ur de lâ€™idÃ©ologie : Tous ceux qui sâ€™opposent au pouvoir sont considÃ©rÃ©s comme des \nennemis de la classe ouvriÃ¨re. Le pouvoir soviÃ©tique met en place dÃ¨s 1918 des premiers camps dans lesquels il sâ€™agit \nde Â« rÃ©Ã©duquer Â» par le travail. Ce sont en fait des camps de travail forcÃ© administrÃ© par le Goulag. Staline, qui arrive \nau pouvoir en Ã©liminant ses opposants, sâ€™appuie sur la police politique, le NKVD, pour traquer ceux quâ€™il appelle Â« les \nennemis de lâ€™intÃ©rieur Â», en fait tous ceux qui sâ€™opposent, mÃªme et surtout au sein du Parti Communiste. Entre 1936 \net 1938, la Grande Terreur sâ€™abat sur lâ€™URSS. La violence dâ€™Ã©tat devient systÃ©matique, des objectifs sont fixÃ©s dans les \nprovinces. Entre 1,5 et 2 millions de personnes sont arrÃªtÃ©es, condamnÃ©es Ã  mort (750 000) ou envoyÃ©es dans des \ncamps. Les principaux opposants Ã  Staline sont jugÃ©s en public pendant les ProcÃ¨s de Moscou, et sont gÃ©nÃ©ralement \ncondamnÃ©s Ã  la peine de mort.  \nB. \nLe fascisme italien. \nDÃ©finir le fascisme : Avec les lois fascistissimes de 1925-1926, Mussolini met en place les outils nÃ©cessaires Ã  \nlâ€™encadrement de la sociÃ©tÃ©. Le fascisme se base sur une double rÃ©fÃ©rence au passÃ© glorieux de lâ€™Italie (Lâ€™Empire \nromain) et sur la volontÃ© de construire un Ã©tat moderne autour de son chef. Il sâ€™appuie donc sur un fort nationalisme \n\net un rejet de la dÃ©mocratie et du communisme. La culture de guerre, visible dans lâ€™encadrement militaire de la sociÃ©tÃ©, \ntout comme dans la propagande aprÃ¨s la guerre en Ã‰thiopie, est un Ã©lÃ©ment central du fascisme. Le fascisme intÃ¨gre \nde maniÃ¨re incomplÃ¨te un racisme dâ€™Ã©tat, affirmant la supÃ©rioritÃ© du peuple italien. En 1938, une sÃ©rie de lois \nantijuives complÃ¨tent la dÃ©finition du fascisme.  \nUne violence dâ€™Ã©tat antidÃ©mocratique : Si le nombre de victimes du rÃ©gime de Mussolini nâ€™est pas comparable avec \nlâ€™Allemagne et lâ€™URSS, lâ€™Italie est un Ã©tat policier dans lequel les opposants politiques sont systÃ©matiquement \npourchassÃ©s. AprÃ¨s lâ€™assassinat de lâ€™opposant Matteotti (1924) et les lois fascistissimes, Mussolini utilise la violence \npour asseoir son autoritÃ©. Lâ€™OVRA, police politique mise en place en 1927, traque les opposants politiques, notamment \nles communistes. ArrÃªtÃ©s, ils sont jugÃ©s et condamnÃ©s Ã  mort ou Ã  la dÃ©portation dans les Ã®les Lipari. \nC. \nLe nazisme. \nLe racisme comme base idÃ©ologique : DÃ©veloppÃ©e par Hitler dans son ouvrage Mein Kampf, rÃ©digÃ© en 1924-1925, le \nnazisme se dÃ©finit comme une rÃ©volution sociale qui doit permettre Ã  la race aryenne de conserver sa supÃ©rioritÃ©. Il \nentend donc lutter contre tout ce qui pourrait affaiblir la race aryenne (handicapÃ©s, homosexuelsâ€¦). Pour Hitler, les \nJuifs sont les principaux responsables de lâ€™affaiblissement et de la dÃ©faite de 1918 (Â« coup de poignard dans le dos Â»), \nassociÃ©s aux communistes. Lâ€™antisÃ©mitisme est donc un fondement central du nazisme et les Juifs sont la cible du \nnazisme : boycott des magasins juifs, Lois de Nuremberg (1935) les privant de la nationalitÃ© allemande et interdisant \nles mariages entre Juifs et citoyens allemands, ou bien encore les mesures de 1938 leur interdisant dâ€™exercer un \nnombre importants de mÃ©tiers. \nLa violence comme outil idÃ©ologique : Hitler confisque le pouvoir grÃ¢ce Ã  la violence (incendie du Reichstag en fÃ©vrier \n1933) et fait du NSDAP le seul parti autorisÃ©. Devenu ReichsfÃ¼hrer en 1934 Ã  la mort du PrÃ©sident Hindenburg, Hitler \nmet en place de nombreux outils pour orchestrer la violence nazie : la Gestapo, police politique, arrÃªte les opposants \ntandis que les SA puis les SS sont chargÃ©s de lâ€™application violente des mesures nazies. Ainsi, utilisant comme prÃ©texte \nun attentat Ã  Paris contre un reprÃ©sentant nazi, les dirigeants nazis appellent les Allemands Ã  se venger. Câ€™est le dÃ©but \ndu pogrom (mot russe signifiant la persÃ©cution des Juifs) orchestrÃ© par les SS. Dans la nuit du 9 au 10 novembre 1938, \nappelÃ©e la Nuit de Cristal, les magasins juifs sont dÃ©truits, les synagogues incendiÃ©es et les Juifs sont dÃ©portÃ©s dans les \ncamps de concentration dont certains ont ouvert dÃ¨s 1933. \nIII. \nLe rÃªve dâ€™un nouvel ordre europÃ©en. \nA. \nLe culte de la guerre. \nLa guerre au centre des idÃ©ologies : Dans les rÃ©gimes totalitaires, le fonctionnement de base de la sociÃ©tÃ© est celui de \nlâ€™embrigadement câ€™est-Ã -dire de la militarisation des organisations civiles : le port de lâ€™uniforme dans les groupes \nparamilitaires (SS, squadre) et les organisations de jeunesse en tÃ©moignent. Dans les rÃ©gimes fasciste et nazi, la guerre \nest un but en soi, un projet de sociÃ©tÃ© qui permettra de crÃ©er une nouvelle sociÃ©tÃ©. En URSS, la guerre est dâ€™abord \ncelle contre les ennemis intÃ©rieurs. Mais elle est aussi glorifiÃ©e par le sacrifice pour assurer la victoire du communisme, \ncomme lors de la guerre civile entre 1917 et 1922 durant laquelle les forces europÃ©ennes aidÃ¨rent les troupes \nblanches. La guerre est donc pour lâ€™URSS nÃ©cessaire comme moyen de survie. \nLe refus de lâ€™ordre international : Lâ€™un des enjeux centraux du rÃ©gime nazi est de venger lâ€™affront de Versailles en \nredonnant Ã  lâ€™Allemagne une place centrale en Europe. Il faut donc, par la guerre, se venger de la France et du Royaume \nUni. Mais il faut surtout pour Hitler donner Ã  lâ€™Allemagne Â« un espace vital Â» suffisant pour sa population par une \ncolonisation de lâ€™Europe de lâ€™Est et donc la disparition de la Pologne, de la Russieâ€¦Lâ€™Italie a une position plus ambigÃ¼e \nvis-Ã -vis de lâ€™ordre international. Si elle quitte la SDN, elle cherche tout de mÃªme Ã  rester proche de la France et du \nRoyaume Uni. Mais, devant les rÃ©ticences occidentales, lâ€™Italie opÃ¨re un rapprochement avec lâ€™Allemagne pour former \nlâ€™Axe Rome-Berlin (1936), le nationalisme italien devant nÃ©cessairement passer par des annexions et des guerres. \nLâ€™URSS prÃ´ne la rÃ©volution communiste mondiale et le renversement des rÃ©gimes bourgeois. Ses relations avec les \n\nautres puissances mondiales sont marquÃ©es par un forte dÃ©fiance mÃªme si elle se rapproche de la France (adhÃ©sion Ã  \nla SDN en 1934, accords avec la France). \nB. \nLâ€™Espagne, lieu dâ€™affrontement des totalitarismes \nUn pays dÃ©chirÃ© par une guerre civile : Les Ã©lections de fÃ©vrier 1936 voient la victoire en Espagne dâ€™un Front Populaire \nregroupant les forces de gauche, dont les communistes (comme en France). Une partie de lâ€™armÃ©e, basÃ©e au Maroc \nespagnol, se soulÃ¨ve sous la direction du GÃ©nÃ©ral Franco qui prend la tÃªte des nationalistes contre les RÃ©publicains. La \nguerre dure trois ans et se termine par la victoire des nationalistes et la fuite en France des RÃ©publicains espagnols \n(600 000 morts). \n Un lieu dâ€™affrontement idÃ©ologique : Si malgrÃ© la demande espagnole, la France de LÃ©on Blum refuse dâ€™intervenir \ndans le conflit et essaie dâ€™imposer un embargo sur les armes, les rÃ©gimes totalitaires sâ€™engagent dans le conflit. Lâ€™URSS, \npar le biais du Komintern, organise et arme les Brigades Internationales qui rÃ©unit les volontaires du monde entier qui \nsâ€™engagent aux cÃ´tÃ©s des RÃ©publicains espagnols. De leur cÃ´tÃ©, les rÃ©gimes allemand et italien envoient des troupes et \ndu matÃ©riel pour aider les nationalistes de Franco, crÃ©ant une alliance idÃ©ologique. Ils vont profiter de ce conflit pour \nexpÃ©rimenter leurs troupes et leur matÃ©riel comme lors du bombardement du village de Guernica par lâ€™aviation \nallemande (1937) ou celui de Barcelone par lâ€™aviation italienne (1938). \nC. \nLa marche vers la guerre. \nLa faiblesse des dÃ©mocraties europÃ©ennes : lâ€™exemple espagnol montre quâ€™en Angleterre et en France, les \ngouvernements et les opinions publiques, traumatisÃ©es par la PremiÃ¨re Guerre mondiale, sont trÃ¨s largement \npacifistes et en faveur de politiques dâ€™apaisement et de renoncement. DÃ¨s 1936, face Ã  la rÃ©occupation militaire de la \nRhÃ©nanie, interdite par le TraitÃ© de Versailles, lâ€™absence de rÃ©ponse ferme franco-britannique illustre cette faiblesse. \nLorsque Hitler revendique les SudÃ¨tes, une rÃ©gion de la TchÃ©coslovaquie en 1938, Chamberlain (Royaume Uni) et \nDaladier (France) prÃ©fÃ¨rent sacrifier une alliÃ© militaire pour Ã©viter de faire basculer lâ€™Europe dans la guerre. Câ€™est \nlâ€™esprit de Munich (nom de la ville oÃ¹ a eu lieu la confÃ©rence). MÃªme si la France a conscience que la guerre est proche, \nle Royaume Uni espÃ¨re encore une confÃ©rence sur la paix. \nLes coups de force nazis et italiens : Profitant de lâ€™absence de rÃ©action des grandes puissances mondiales, Hitler, aprÃ¨s \navoir rÃ©armÃ© son pays, se lance dans une politique dâ€™expansion. En mars 1938, il rÃ©alise lâ€™annexion de lâ€™Autriche \nlâ€™Anschluss au nom de la Â« Grande Allemagne Â» qui doit rÃ©unir tous les populations germanophones sous lâ€™autoritÃ© \nnazie. En Septembre 1938, les SudÃ¨tes, province majoritairement germanophone en TchÃ©coslovaquie sont annexÃ©es \npar lâ€™Allemagne avec lâ€™accord de la France et du Royaume Uni (confÃ©rence de Munich). En mars 1939, Hitler sâ€™empare \ndu reste de la TchÃ©coslovaquie et commence Ã  revendiquer une partie de la Pologne. La France et le Royaume Uni \ncomprennent que la guerre est inÃ©vitable. Lorsque lâ€™Allemagne, aprÃ¨s avoir signÃ© un pacte avec lâ€™URSS se partageant \nla Pologne, se lance dans la guerre le 1e septembre 1939, les puissances occidentales se lancent Ã  contre cÅ“ur dans la \nSeconde Guerre mondiale. \nConclusion : Lâ€™URSS, lâ€™Italie fasciste et lâ€™Allemagne nazie sont donc des rÃ©gimes nÃ©s de la brutalisation des sociÃ©tÃ©s Ã  \nlâ€™issue des deux Ã©vÃ¨nements du premier XXe siÃ¨cle : la PremiÃ¨re Guerre mondiale et la crise de 1929. Sâ€™appuyant sur \nun parti unique et rejetant les fondements dÃ©mocratiques, ces trois rÃ©gimes ont mis en place des mÃ©thodes de \ngouvernement communes pour atteindre des objectifs idÃ©ologiques diffÃ©rents : une dictature du prolÃ©tariat en URSS, \nun nationalisme guerrier en Italie et un nationalisme raciste et expansionniste en Allemagne. Ces trois pays ont eu en \ncommun de vouloir mettre en place une nouvelle gÃ©opolitique dans lâ€™Europe des annÃ©es 30, avec, comme moyen \nultime dâ€™y parvenir la guerre face Ã  des dÃ©mocraties traumatisÃ©es par les sacrifices de la Grande Guerre. \n",
    "Memoire_M1___Efrei.pdf": "Floware\nVision\nJourney in Edge AI Computer Vision Application for Traï¬€ic Analysis.\nNoÃ© Breton\nSupervised by Julian Garbiso, LÃ©onard Benedetti\nEFREI Paris PanthÃ©on-Assas UniversitÃ©\n-\nFloware\nAugust 2023\n------>\nJune 2024\n\nAbstract\nThis memoir explores the development and application of Edge AI computer vision technology for traï¬€ic analysis through\nFloware Vision.\nThe integration of computer vision, artificial intelligence, and the Internet of Things transformed various\nindustries, transportation for instance.\nThis project highlights Edge AIâ€™s real-time data processing capabilities to address\nmobility challenges and optimize traï¬€ic flow. Floware Vision, using Nvidiaâ€™s Edge AI solutions and YOLO models, processes\nvideo inputs from multiple cameras to detect and track objects, providing essential data for traï¬€ic management. This approach\nnot only enhances real-time decision-making but also maintains data privacy by performing computations locally. The memoir\ndescribes the architecture of Floware Vision, explaining the video processing pipeline, model integration, and the deployment\nstrategies used. It aims to enhance the importance of modularity, real-time eï¬€iciency, and compatibility in the systemâ€™s design.\nAdditionally, it justifies the implementation of functional programming and microservices architecture to meet these requirements.\nPerformance metrics and case studies comparing different YOLO models are presented to demonstrate the systemâ€™s effectiveness.\nThe memoir concludes with a review of the future work needed to further upgrade Floware Vision and its potential impact on\nurban mobility.\n\nCONFIDENTIALITY\nTHIS REPORT IS INTENDED SOLELY FOR THE EVALUATORS AND MAY NOT BE DISCLOSED TO OTHER INDIVID-\nUALS, WHETHER INTERNAL OR EXTERNAL TO THE INSTITUTION. THE DOCUMENT MUST NOT BE RETAINED\nBEYOND THE PERIOD STRICTLY NECESSARY FOR ITS EVALUATION.\n\nPlagiarism\nI, NoÃ© BRETON, hereby certify that I am the author of this Memoire, and I conducted the research myself. I confirm that this\nMemoire has not been previously submitted for any other degree. Any statement taken from the work of another person (with\nor without minor changes) and cited in this report is enclosed in quotation marks, and proper and precise references have been\nprovided for such citations. I am aware that plagiarism can result in the invalidation of this Memoire and, in severe cases, lead\nto expulsion from the University. I also aï¬€irm that, except for the duly acknowledged citations, this report represents my own\nwork.\n\nAcknowledgements\nI would like to thank Floware, Julian Garbiso, and Mathieu Lafarge for allowing me to build my experience in their company\non interesting projects. Additionally, I would like to extend my thanks to all my collaborators, especially Ali, Bond, and Enea,\nfor their support and sympathy.\n\nContents\n1\nIntroduction\n1\n2\nFloware: Edge AI for Traï¬€ic Analysis\n1\n2.1\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n2.2\nTeam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n2.3\nMissions Organisation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.4\nPartners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.4.1\nAcademical Partners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.4.2\nEconomic Partners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.4.3\nCommercial Partner\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.4.4\nComercial Partner Archetype\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.5\nLocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.6\nSome Competitors\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.7\nServices\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.7.1\nFloware Vision\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.7.2\nFloware Core (In development)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.7.3\nFloware Autopilot (In development)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.8\nProject Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nEdge AI: Definition and Context\n5\n3.1\nIntroduction to Edge Computing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nWhy using Edge AI ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.3\nApplications of Edge AI\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4\nReal Time Computer Vision and the YOLO approach\n6\n4.1\nIntroduction to computer vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.2\nHow YOLO Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3\nYOLOâ€™s Iterations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3.1\nYOLOv1-v4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3.2\nYOLOv5-v8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.4\nComparison with Competitor Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.5\nYOLO limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.6\nApplication of YOLO in traï¬€ic detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5\nNvidia Jetson: The Chosen Hardware\n10\n5.1\nIntroduction to Nvidia Jetson[1] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5.2\nTechnical Specifications\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5.3\nApplications in Floware Vision\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n6\nFocus On Nvidia Deepstream, the complexity worth the performance ?\n12\n6.1\nOverview of Nvidia Deepstream . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6.2\nFeatures and Capabilities\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6.3\nPerformance vs. Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n7\nFloware Vision: Review of the Existing\n12\n\n7.1\nExisting States\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n7.1.1\nInputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n7.1.2\nOutputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n7.1.3\nKey Features\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n7.1.4\nArchitectures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n7.1.5\nVision Processing Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n7.1.6\nModel Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.2\nExisting Deployment Process\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.3\nLimitations, Issues & Challenges\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.3.1\nStructure Issue\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.3.2\nSecurity Issues\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n7.3.3\nOptimization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n7.3.4\nDeployement issue\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n7.3.5\nBugs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n7.4\nFeedback and Improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n8\nFloware Vision: Development\n19\n8.1\nImplementation Choice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n8.2\nImprovement On The Existing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n8.3\nAI Model Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n8.4\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n8.4.1\nJustification for C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n8.4.2\nDesign & Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n9\nFloware Vision: Deployment\n24\n9.1\nDeployment Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n9.2\nIntroduction to Azure IoT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n9.3\nCI/CD: Actual Deployment Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n10 Results and Discussion\n26\n10.1 Floware Vision Performance Metrics\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n10.2 Case Studies : Yolo8s vs Yolo8n performance\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n10.3 Output Analysis: the end side of the pipeline\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n10.3.1 Visualization of Flows\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n10.3.2 Transition Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n10.3.3 Activity Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n10.3.4 Post-Alerting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n10.3.5 Queue Length Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n10.3.6 Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n11 Conclusion\n30\n\nFloware Vision: Journey in Edge AI Computer Vision Application\nDevelopment for Traï¬€ic Analysis\nNoÃ© Breton\n \n \n16 July 2024\n1\nIntroduction\nThe convergence of computer vision, artificial intelligence, and the Internet of Things has revolutionized various industries like\ntransportation, security, and home automation. Among these advancements, Edge AI has stepped up as a powerful and eï¬€icient\nsolution for providing real-time data processing and decision-making capabilities directly at the source of data recording. Traï¬€ic\nanalysis has benefited from these advancements. Indeed, the issue of mobility flows1 represents a major challenge for many\nentities, whether public or private. According to a study of the Institute for Transport & Economics Technische UniversitÂ¨at\nDresde, Traï¬€ic congestion typically leads to an increase in fuel consumption of about 80% while travel time can increase by up\nto a factor of 4[2].\nUnderstanding and optimizing movements within a territory is essential for improving transportation eï¬€iciency, reducing con-\ngestion, and contributing to the transition toward more sustainable modes of transportation. In this context, the use of Edge AI\nsensors is really powerful as it can bring real-time processing and alerting by local computing and complex analysis by embedded\nAI software without privacy-threatening data transfers to the cloud. That is the path Floware has chosen. Floware seeks to\nprovide useful information and innovative solutions to these entities over the long term, using Nvidia Edge AI solution and\nYolo.\nWhat role does Edge AI software play in traï¬€ic analysis ?\nAfter providing some context by presenting the company and defining key notions in the field, with a significant state-of-the-\nart section on our utilized technology, my memoir will illustrate my contributions to Flowareâ€™s activities, particularly on the\nembedded Edge AI software: FLWR-Vision. This approach will provide a comprehensive overview of Flowareâ€™s activities, from\nthe sensor to the data.\n2\nFloware: Edge AI for Traï¬€ic Analysis\n2.1\nOverview\nFloware is a startup founded in 2023 in Palaiseau, specializing in analyzing mobility flows. It aims to bring innovative technologies\nfor optimizing and de-carbonizing mobility over the long term. Relying on a network of proprietary sensors and a software chain\nstrengthened by artificial intelligence, Floware aims to provide precise, secure, and anonymized data by controlling the entire\ndata acquisition chain, from software to hardware, usable for a wide range of applications cases.\n2.2\nTeam\nFounded by Julian Garbiso (Telecom Paris Tech, IMT Atlantique), PhD in Computer Science and Networks, and Mathieu\nLafarge (Science Po, ESTP), specialized in Urban Planning and Civil Engineering, Floware currently employs one trainee\n(myself) with the title of Edge AI and Cloud Services Developer. To strengthen our workforce and refocus my activity on\nsensor work, a recruitment campaign has been launched for two positions in Data Analysis and the Internet of Things. We now\nhave a Senior Edge IA Engineer, Enea de Bolivier, (INP Grenoble, Kyoto University), an Industrial + UX/UI Designer, Daniel\nEsperben (Universidad Nacional de La Plata), and 3 interns. Ali Choucair (UniversitÃ© de Strasbourg), Mecatronics Engineer\nIntern, Bond Zhang (Polytechnique), Iot intern and Mathias (Epita), data scientist intern.\n1Mobility flows refer to the movements and displacements of people or vehicles within a given space.\n1\n\nFigure 1: Floware Organisational Chart\n2.3\nMissions Organisation\nâ€¢ Enea\nâ€“ Manage Interns and trainees in all fields before new Seniors are hired.\nâ€“ Lead the embedded software part of Floware, including FLWR-Vision.\nâ€¢ Ali & Bond\nâ€“ Works on operational device deployment and hardware setup.\nâ€“ Research and development tasks on hardware (Motorized Camera, GPS feature, log writings)\nâ€¢ David\nâ€“ Design new device case improving temperature, weight, and size.\nâ€“ Design Floware websites.\nâ€¢ Mathias\nâ€“ Work on data science client missions: transform, and create a visualization of the data created from the sensor.\n2.4\nPartners\n2\nThis section outlines Flowareâ€™s key partnerships, including academic, economic, and commercial collaborators. These partner-\nships play a crucial role in enhancing Flowareâ€™s capabilities and market reach.\n2.4.1\nAcademical Partners\nFloware collaborates with several academic institutions to develop their technological expertise.\n2Informations transmitten by Julian Garbiso\n2\n\n2.4.1.1\nInstitut Vedecom\n3\nFloware is an aï¬€iliate of Vedecom Institute and is therefore a natural partner for R&D in the mobile sector. This partnership\nbrings significant benefits in technical support, networking, and branding. Floware aims to create a mutually beneficial dynamic\nfor both organizations by jointly responding to project proposals and tenders and developing a common communication and\nmarketing strategy.\n2.4.1.2\nÃ‰cole Polytechnique (X-UP / X-TECH)\nFloware was founded as a company as part of the X-UP acceleration program at the Ecole Polytechnique. Through the schoolâ€™s\ninvolvement in the company, the relationship has grown into a long-term partnership. Flowareâ€™s oï¬€ices are within X-TECH,\nallowing the company to continue benefiting from this valuable ecosystemâ€™s technological developments. The Drahi X-Novation\nCenter also houses a FabLab, providing a conducive environment for Floware to develop and test its sensors on-site.\n2.4.1.3\nUniversitÃ© Gustave Eiffel\nWe are working with Mahdi Zargayouna, Deputy Director of the GRETTIA Laboratory, on Traï¬€ic Modeling and Simulation.\nThe Sci-Ty program is currently developing a Technology Maturation and Transfer project.\n2.4.2\nEconomic Partners\nFlowareâ€™s economic partners enhance the scalability of the business and support our business strategy, sales, and marketing.\n2.4.2.1\nHEC\nFloware joined the HEC incubator in September 2023 and getting support in business strategy, sales, and marketing.\n2.4.2.2\nMoove Lab [4]\nFloware is part of batch number 11 starting from September 2023. Its goal is to use this opportunity as a platform to access\nthe ecosystem related to the mobility sector, including industry players and venture capital investors in the mobility market,\nnotably ViaID, which runs the program with Mobilians.\n2.4.2.3\nLeonard (Groupe Vinci)[5]\nFloware is part of batch number 11 starting from September 2023. Its goal is to use this opportunity as a platform to access\nthe ecosystem related to the mobility sector, including industry players and venture capital investors in the mobility market,\nnotably ViaID, which runs the program with Mobilians.\n2.4.2.4\nNextmove [6]\nFloware adheres to the Nextmove competitiveness cluster to benefit from its rich ecosystem, its network of member cities and\nterritories, and its commitment to fostering a mutually beneficial partnership.\n2.4.3\nCommercial Partner\nFlowareâ€™s commercial partnerships focus on either implementing our sensor on their territory, providing useful data, or both.\n2.4.3.1\nEcomesure [7]\nEcomesure is developing an IoT solution to measure air quality. Floware and Ecomesure are working together on EcoFlow,\nselected by Paris&Co as an innovative metropolitan area. The tool will monitor and warn about vehicle emissions to support\nlow-emission zone policies in European cities.\n3French Institute for Energy Transition dedicated to road mobility [3]\n3\n\n2.4.3.2\nEPAPS (Etablissement dâ€™amÃ©nagement de Paris Saclay)\nFlowareâ€™s first major partner commissioned them to conduct a large-scale proof of concept. EPAPS helped Floware to contract\nwith EPAPS and DiRIF and collaborate with the Ecole Polytechnique to conduct decarbonization experiments.\n2.4.3.3\nLa Fabrique des MobilitÃ©s [8]\nFloware is working with FabMob to develop Tracemob, an app that tracks travel intentions. They plan to conduct a joint citizen\ntravel survey in Noisy-le-Grand, combining sensor data and citizen voluntary travel information.\n2.4.4\nComercial Partner Archetype\nâ€¢ Local collectivities (E.g, traï¬€ic data usage to optimize public transportation routes)\nâ€¢ A developer/promoter (E.g, real estate developers planning new residential areas based on traï¬€ic patterns)\nâ€¢ A mobility operator (E.g, bus companies adjusting schedules based on real-time traï¬€ic conditions)\nâ€¢ A construction industry actor (E.g, construction firms coordinating work schedules to minimize traï¬€ic disruptions)\nâ€¢ An engineering firm (E.g, firms designing new road networks with the help of advanced traï¬€ic modeling)\n2.5\nLocation\nâ€¢ Ã‰cole Polytechnique Palaiseau - Drahi-X Novation Center\nâ€¢ Station F - Open Space Moove Lab, Paris 13th\nâ€¢ Leonard:Paris - Coworking space, Paris 11th\n2.6\nSome Competitors\nâ€¢ Wintics[9]: Develops AI-based software for traï¬€ic management, parking optimization and public transport monitoring\nto improve urban mobility and reduce congestion.\nâ€¢ ALYCE[10]: Provides insights into traï¬€ic patterns, pedestrian flows, and public transport usage, as well as mobility\ndata and analytics to optimize infrastructure and services.\nâ€¢ UPcity[11]: Provides urban planning and smart city management tools, including traï¬€ic simulation and infrastructure\nplanning, to support eï¬€icient and sustainable urban development.\nâ€¢ Eurovia[12]: VINCI Group subsidiary focuses on transport infrastructure construction and urban development, integrat-\ning IoT and AI to achieve sustainable and resilient infrastructure.\nâ€¢ CDVIA[13]: Provides advanced traï¬€ic management systems with real-time data analytics and machine learning to\noptimize traï¬€ic flows, reduce congestion, and improve traï¬€ic safety.\n2.7\nServices\nAs mentioned earlier, Floware offers an innovative solution for analyzing mobility flows for private and public territorial operators.\nTheir philosophy is centered on privacy-by-design (privacy protection rooted in robust design) and focuses on eï¬€iciency and\nautonomy. Floware aims to control every aspect of the data pipeline, from the acquisition of data to its processing and analysis.\nFloware divides its services into three products :\n4\n\nFigure 2: Floware Solutions\n2.7.1\nFloware Vision\nThe embedded software that collects data and performs the first processing, powered by AI, is the subject of this memoir. You\ncan find more details in the FLWR-Vision focused section here.\n2.7.2\nFloware Core (In development)\nSaaS platform for data analysis and visualization on the cloud. Production of mobility models and simulations tailored to\nspecific use cases.\n2.7.3\nFloware Autopilot (In development)\nFlowareâ€™s AI co-pilot uses Larges language models to analyze data and manage cloud software. This tool provides clients with\nvisualizations, reports, simulations, and customized insights. It improves strategic decision-making in mobility. Floware aims\nto be the top provider of these services with a scalable business model.\n2.8\nProject Management\nFloware chose Agile management[14] for developing their solution, with long-term tasks and urgent, short-term tasks for clientsâ€™\nneeds.\n3\nEdge AI: Definition and Context\n3.1\nIntroduction to Edge Computing\nAfter the deployment of the World Wide Web in 1989 by Tim Berners-Lee, and the invention of web servers, web browsers, and\nHTML, data processing started to switch from the local machine to a server. Berners-Lee noticed some issues: in the future,\nwhen many devices are connected to the internet and if all the data is processed by a group of centralized servers, congestion\nproblems tend to appear, causing bugs and crashes for users. A decentralization of computation was needed. Akamai[15] was\none of the first companies to introduce this concept in 1989. By using multiple networks physically closer to users and devices,\nlatency can be reduced, costs minimized, and network connectivity improved. Imagine a festival with thousands of people inside.\nIf there were only one big food stand, toilet, or bar, it would be a matter of time before the whole stand becomes bloated and\nnearly unreachable. Splitting the big stand into smaller ones, located in diverse places, and closer to the festival-goers that can\nget their food quickly and eï¬€iciently.\nWith time, edge computing extended to the content-delivering usage from other applications, and now defines every case where\nrunning a computer program delivers a quick response to where the request is made. Edge Computing is not the same concept\n5\n\nas the Internet Of Things (IoT), because this network of physical objects is not obligated to process the data, and can send\ntheir data to the cloud for processing, whereas edge computing focuses on local processing.\nEdge Artificial intelligence (Edge AI)[16] is one of the derivate uses of edge computing. It refers to deploying models and running\nmachine learning tasks directly close to the device location instead of a distant cloud. The data is stored and processed at the\nsame place, before an optional sending to a cloud for retaining or deploying purposes.\n3.2\nWhy using Edge AI ?\nCloud AI and Distributed AI are also popular solutions, but for some use cases, Edge AI brings distinct advantages, particularly\nfor small structures operating in public spaces.\nOperating at the device level removes the need to send data across the network, reducing the risk of data leaks or breaches[17].\nDecentralization decreases the threat of major data breaches because data is not stored in a single location.\nAdditionally,\nprocessing sensitive private data locally allows for retaining only the analysis output or encrypted information, minimizing\nprivacy issues and improving security.\nAdditionally, on a latency aspect, by the edge computing properties mentioned above, the data is immediately processed\nwithout server traveling delay.\nThis reduced latency and local processing enables instantaneous task treatment and permits an alerting system. For instance,\na Computer Vision IoT device can recognize a person searched by the authorities and immediately alert them in real-time.\nFewer requests to a distant server also mean decreased bandwidth usage, with all the cost reductions it induces. reduced\nbandwidth and cloud treatment enhance the scalability of edge solutions. Indeed, increasing the number of devices doesnâ€™t\nsignificantly increase the cloud resources needs because each device processes its data independently.\n3.3\nApplications of Edge AI\nEdge AI can be applied to numerous domains.\nIndeed, edge AI devices like sensors, drones, and cameras can be used in\nagriculture to determine the health state of the soil, and infestation in real-time, allowing the farmer to act quickly, and\nreducing his work time[18].\nFloware Sensors are a good example of edge AI utilization in Traï¬€ic Flux Analysis. Processing data directly from a camera\nstream lightens the data sent to the server while reducing the amount of sensitive information traveling to the server. Real-time\nprocessing enables an alerting process if congestion is detected, for instance.\nIn another case, a device with many diagnostic sensors can provide an immediate diagnosis with confidentiality. More softly,\nhealth monitoring with noninvasive edge devices like Fitbit[19].\n4\nReal Time Computer Vision and the YOLO approach\n4.1\nIntroduction to computer vision\nâ€Computer vision is a field of computer science that focuses on enabling computers to identify and understand objects and\npeople in images and videos. Like other types of AI, computer vision seeks to perform and automate tasks by replicating human\ncapabilities. In this case, computer vision seeks to replicate both the way humans see, and the way humans make sense of what\nthey seeâ€[20]\nComputer vision regroups techniques and tools that enable computers to understand, interpret, and process information from\nimages and videos. To understand an image, a computer translates different aspects of it into an array of numbers. These\naspects can represent the contrast or color of each pixel, and the intensity of luminosity or the grey level, for example.\nComputer vision existed before the rise of Deep learning and is still used in its traditional form for simple tasks, like edge or\nshape detection, and integrated into more complex processes. Convolutional Neural Networks, for instance, still use filters and\nshape detection in their process.\nIt was significantly transformed by two major developments: the invention of convolutions neural networks in 1980[21] and\nthe rise of GPU Computing in the 2000s. But because of limited computer resources, it could not be used then. The latter\nwas required as originally computing power was not suï¬€icient to run CNN. In 2014, the CNN AlexNet trained on the dataset\nImageNet dominated the image classification field and proved the eï¬€iciency of CNN on image classification. A CNN comprises\nlayers that apply matrix operation at each step on the image vector.\nThis method significantly improved the accuracy of\nobject detection and enabled the handling of massive data volumes.\nUnlike traditional methods requiring manual feature\nengineering, Deep Learning algorithms can automatically learn relevant features from data, thus simplifying the development\nprocess significantly improving the accuracy of object detection, and enabling the handling of massive data volumes.\n6\n\nDespite the exponential growth of computer capacity, there were still performance issues, CNNs were requiring too many\nresources to run in real-time. But why it was still the case in the 2010s? Letâ€™s explain how real-time computer vision worked\nbefore 2016. A popular way to process vision object detection was through Faster RCNN (Fast Region-based Convolutional\nNeural Network). RCNN divides the image into smaller regions instead of grids, up to 2000 regions, and then passes these\nregions into a pre-trained AlexNet and a feature map, for each region before a regression.\nFigure 3: RCNN\nDetermining 2000 regions took too much time for real-time (47 seconds per frame), therefore fast RCNN was created, and this\nnetwork established the region from the feature maps, without the need to feed the network 2000 regions directly.\nFigure 4: Fast RCNN\nFaster RCNN improved the process by suppressing the selective search algorithm for regions and let a network predict them\ninstead.\nFigure 5: Faster RCNN\nDespite the optimization improvements of these region-based models, it still wasnâ€™t enough for real-time use. To address these\nlimitations, the YOLO[22] (You Only Look Once) approach was developed.\n7\n\n4.2\nHow YOLO Works\nWhile RCNNs do not look at the complete image and process regions, YOLO chooses a different approach by processing one\nframe at once, using only one network. The network predicts bounding boxes and class probabilities directly from full images\nin one evaluation.\nFigure 6: The Model\nYOLO divides an image into an SxS grid of elements, and for each grid cell, it detects if an object is present through a multi-layer\nneural network. In the case of detection, it determines the center of the object (x, y), its height, width, and a class confidence\nscore. The output is an object/tensor of dimension SxSx(B*5 + C).\nFigure 7: The YOLO Detection System.\n4.3\nYOLOâ€™s Iterations\nYOLO has seen several versions, each improving after each iteration in terms of accuracy, speed, and capabilities. Here is a\nbrief overview of the key iterations and advancements:\n4.3.1\nYOLOv1-v4\nYOLOv1, which was first presented by Redmon et al. in 2016, transformed object detection by approaching it as a single\nregression problem. It enabled real-time detection by processing images on a GPU at 45 frames per second. In contrast to\nRCNN models, which were accurate but slow, YOLOv1 provided a noticeable speed boost.\nYOLOv2, or YOLO9000, was established in 2017. Anchor boxes, batch normalization, and a brand-new network architecture\nknown as Darknet-19 were all included. With over 9000 object classes detected, YOLO9000 improved speed and accuracy.\nYOLOv3 in 2018, included detection at three different scales along with Darknet-53, an enriched architecture with residual\nconnections. This version improved precision and recall while handling both small and large objects.\nYOLOv4, released in 2020 combined new methods like PANet and SAM blocks with CSPDarknet53 as the backbone to optimize\nspeed and accuracy. It enhanced the mean Average Precision (mAP) while retaining real-time detection.\n4.3.2\nYOLOv5-v8\nYOLOv5, created by Ultralytics in 2020, was designed with deployment and usability in mind. Offering multiple versions suited\nto varying computational budgets, from mobile devices to server-grade GPUs, it was optimized for production environments.\nYOLOv6, which was first introduced by Li et al. in 2022, was designed for industrial use. Its Rep-PAN neck, anchor-free\ndetection method, and Eï¬€icientRep backbone optimized performance for particular industrial scenarios.\n8\n\nYOLOv7, published in 2022, combined novel model architectures with cutting-edge learning strategies.\nIt created â€bag-of-\nfreebiesâ€ methods to boost accuracy without raising inference costs, and it raised the bar for real-time object detection. YOLOv8,\nlaunched in 2023, kept improving performance and usability. It included mosaic data augmentation, anchor-free detection, and\na decoupled head.\nWith its effective real-time detection capabilities, YOLOv8 is especially well-suited for edge AI devices.\nBecause of their smaller size and higher framerate, YOLOv8 in its small (YOLOv8s) and nano (YOLOv8n) versions are used\non Floware embedded computer vision.\n4.4\nComparison with Competitor Models\nFigure 8: Error Analysis: Fast R-CNN vs. YOLO : Despite R-CNN having more correct detection, Yolo compensated\nby better performances\nFigure 9: YOLO Framerate compared to other state-of-the-art object detectors [23]\n4.5\nYOLO limitations\nWhile YOLO is a powerful object detection algorithm, it suffers from some limitations:\nYOLO has several limitations that affect its performance in certain applications. One major limitation is the detection of\nsmall objects within large images due to its grid-based approach, which can lead to inaccurate predictions. Additionally, YOLO\nmay struggle with objects that are too close to each other, causing overlapping bounding boxes and reduced detection accuracy.\nThe algorithm also tends to underperform in scenarios with significant variations in object scale, orientation, and occlusion.\nYOLO can also be less precise than real-time detectors such as Faster R-CNN, even though this issue is compensated by its\nreal-time processing capabilities. YOLO does not include tracking functionality natively. These limitations must be considered\nwhen applying YOLO to traï¬€ic detection tasks, where high accuracy and reliability are crucial.\n9\n\n4.6\nApplication of YOLO in traï¬€ic detection\nDespite its limitations and coupled with tracking algorithms, YOLO has been effectively applied in traï¬€ic detection systems\ndue to its real-time processing capabilities. Its speed allows for quick identification of vehicles, pedestrians, and other objects,\nmaking it suitable for dynamic traï¬€ic environments. YOLO can be integrated with edge AI hardware to analyze video streams\nfrom traï¬€ic cameras, processing data for traï¬€ic management and monitoring. By optimizing the model and fine-tuning it for\nspecific traï¬€ic scenarios, YOLO can achieve satisfactory performance levels, contributing to eï¬€icient traï¬€ic flow and incident\ndetection (see figure 12).\n5\nNvidia Jetson: The Chosen Hardware\n5.1\nIntroduction to Nvidia Jetson[1]\nThe Nvidia Jetson is a family of hardware designed for edge and embedded computing. These devices are engineered to eï¬€iciently\nrun AI and machine learning models, using GPU computing and Nvidia technology, such as the DeepStream SDK, enabling\ndevelopers to create AI systems at the edge. This makes them ideal for various applications, including autonomous machines,\nIoT devices, and edge computing solutions. Nvidia offers different versions of Jetson with varying prices and functionalities:\nJetson Nano, Jetson Orin, and Xavier listed in ascending order of capabilities and prices. Currently, Floware uses the Nano and\nOrin models due to their scalability and cost-effectiveness. However, Nvidia has discontinued the Jetson Nano due to its aging\nperformance capabilities, forcing Floware to transition to the Jetson Orin.\n5.2\nTechnical Specifications\nFigure 10: Specification comparison between Jetson Nano, Orin and a Raspberry Pi\n10\n\n5.3\nApplications in Floware Vision\nFigure 11: Floware Sensor\nA classic Floware sensor is composed of a Jetson Nano (1) linked to a camera (5), and an Ubertooth antenna [24] (4) to scrape\nBluetooth data. The Jetson Nano does not contain a Wi-Fi card, so we use a Wi-Fi dongle (3) with a Wi-Fi key (3) to connect\nto the network. The Nvidia Jetson Nano is powered by a 12-volt to 5-volt converter(2).\nFigure 12: Jetson Nano Desktop with Floware Vision Running\n11\n\n6\nFocus On Nvidia Deepstream, the complexity worth the performance\n?\nDeepstream is an Nvidia-developed project based on Gstreamer, an open-source pipeline-based framework [25] made for pro-\ncessing media streams. Gstreamer links many stream-processing into complex workflows.\n6.1\nOverview of Nvidia Deepstream\nNvidia DeepStream[26] reuses tasks in a cascade routine but is specialized for multi-sensor (audio, video, and image) AI\nprocessing. DeepStream incorporates deep neural networks and other complex processes such as encoding/decoding, rendering,\nand tracking. GStreamerâ€™s basic pipeline element is still usable, in addition to Nvidiaâ€™s created element plugin. DeepStream\ncan be used on multiple devices, personal computers with Ubuntu installed, or with Nvidia edge devices, such as Nvidia Jetson,\nused for Floware Sensor.\n6.2\nFeatures and Capabilities\nâ€¢ Multi-Stream Processing: Capable of managing multiple video streams at once, ensuring eï¬€icient use of resources.\nâ€¢ AI Models Integration: Allows modular integration with multiple pre-trained AI models for detection, classification,\nand segmentation.\nâ€¢ Hardware Acceleration: Permits full integration with Nvidia graphics card acceleration. GPU enables fast parallel\ncomputation, improving performance. Every element of the pipeline is GPU computed.\nâ€¢ Scalability: Adaptable for deployment on various platforms, from edge devices to cloud environments, supporting scalable\nsolutions.\nâ€¢ Customizability: Offers a flexible pipeline architecture that enables easy fine-tuning of complex processes to fit various\napplication needs.\nâ€¢ Developer Tools: Includes tools for performance monitoring, debugging, and optimization to assist developers in fine-\ntuning their applications.\nâ€¢ Compatibility: Deepstream is adaptable on many platforms, from edge devices to cloud environments.\n6.3\nPerformance vs. Complexity\nSetting up and optimizing DeepStream requires a deep knowledge of the Nvidia ecosystem, including CUDA and TensorRT.\nCustomizing and fitting a pipeline for our needs can require more development and testing effort. Why would we use DeepStream\nfor a computer vision task instead of a basic Python script using Ultralytics[27] and OpenCV on GPU? Besides the fact that\nthe Jetson Nano was built to use DeepStream, the customizability and scalability arguments, an OpenCV application is never\nentirely running on the GPU, such as the rendering of videos and bounding boxes. In contrast, a DeepStream pipeline runs\nentirely on the GPU, allowing for overall better performance.\n7\nFloware Vision: Review of the Existing\nFloware Vision, as discussed in the first part of the report, is the embedded software used in the Floware sensor. Floware Vision\naims to centralize all the functionalities and AI processes needed for the output data, using Deepstream and Ubertooth as their\nmain dependencies to perform computation and processing on the edge. Flowareâ€™s needs include object detection, tracking, and\nclassification (not in real-time but very regularly).\nAdditionally, Ubertooth is used to capture a fragment of the Bluetooth address (LAP) of the Bluetooth devices emitting packets\nin the detection range of the sensor, anonymizing it on the fly, helping in the continuity of detection between sensors. For some\nclients, relay control is required. As an example, local authorities may need to trigger lights when a person passes by during\nthe night but not for a vehicle. This requires a relay control in the object detection pipeline. If the sensor camera angle is wide,\ntriggering should be limited to a particular zone, such as a pedestrian crossing. This application needs a Region Of Interest\n(ROI) filtering to limit detection to a small zone, necessitating an ROI drawing tool and an encoding convention (either on the\nedge or in a preprocessing step).\nThis example highlights that the sensor has to be adapted to different use cases. Modularity is crucial to fine-tune each sensor\naccording to the clientâ€™s needs while responding to privacy needs.\nAs many sensors can be deployed simultaneously, they must be as failure-proof as possible. Floware needs to incorporate features\nthat ensure reliability, and if a failure occurs, it is essential to understand why and how it happened.\n12\n\n7.1\nExisting States\nFloware Vision is a composite application, using a corpus of Python scripts distributed across multiple files.\nIt manages\nBluetooth data, computer vision data, but also some intern information like power and disk usage data.\n7.1.1\nInputs\nâ€¢ Video Stream via USB camera.\nâ€¢ Bluetooth data via Ubertooth antenna.\nâ€¢ Disk usage.\nâ€¢ Date and time.\n7.1.2\nOutputs\nFloware Vison sends Itâ€™s output to servers (virtual machines) on the Scaleway or Azure clouds.\n7.1.2.1\nVision Detection Data\nSensor output data is in the form of a CSV or Parquet file, divided by variable periods, with the following columns: ['Frame_number',\n'ObjectID', 'ClassID', 'Confidencelevel', 'BoundingBoxTop', 'BoundingBoxLeft', 'BoundingBoxWidth', 'BoundingBoxHeight',\n'Datetime']\nâ€¢ ObjectID: corresponds to a unique identifier for a unique object.\nâ€¢ ClassID: corresponds to the type of object among these categories:\n('car', 'truck', 'bike', 'motorbike', 'person')\nâ€¢ BoundingBox_: corresponds to the characteristics of the detected objectâ€™s frame.\nâ€¢ Confidencelevel: corresponds to the YOLO class confidence.\nEach file is dated.\n7.1.2.2\nSnapshot Data\nSnapshot data correspond to frames of the camera stream captured at certain intervals in JPG format. There are two categories\nof snapshots: time and class snapshots. The snapshots taken by class are named according to the detected class in the picture.\nThe purpose of snapshots is to aid in debugging in case of incoherent results, provide an accurate representation of the sensor\nsituation (snapshot time), and build our internal dataset (snapshot class).\n(a) Example of Snapshot Class: truck-20240423165514\n(b) Example of Snapshot Time: snapshot-202405032204s\n13\n\n7.1.2.3\nBluetooth data\nBluetooth data, collected using the Ubertooth[24] device, is sent to either the log or parquet format for further processing. The\ncollected data includes several key fields, which are as follows: ['systime', 'ch', 'LAP', 'err', 'clkn', 'clk_offset',\n's', 'n', 'snr']. Each field represents specific information:\nâ€¢ systime: The system time when the data was captured.\nâ€¢ ch: The channel on which the data was captured.\nâ€¢ LAP: The Lower unique Address Part of the Bluetooth devices address. Itâ€™s the main information for FFlowareâ€™s Bluetooth\ndata analysis.\nâ€¢ err: Error information, if any, related to the data capture.\nâ€¢ clkn: The clock number of the Bluetooth device.\nâ€¢ clk_offset: The clock offset.\nâ€¢ s: The signal strength or quality.\nâ€¢ n: The noise level at the time of data capture.\nâ€¢ snr: The signal-to-noise ratio, is useful for knowing the clearness of the signal.\nThis structured format ensures that the data can be effectively analyzed and utilized for traï¬€ic analysis and other applications.\n7.1.2.4\nThe ippadrr case\nFigure 14: Server Side Folder Tree\nThe ipaddr folder is deprecated. It was initially created to regularly send the IP address information of the sensors (on the\ncellular network) to establish SSH connections. However, there were issues bypassing the NAT of the 4g dongle; thus, it was\nabandoned. Eventually, it was repurposed to transfer occasional files, such as syslog files. This functionality should be retained,\nbut it should not be called ipaddr.\n7.1.3\nKey Features\nComputer Vision (FLWR-Vision.py)\nâ€¢ Implementation of the Computer Vision module can be found in FLWR-Vision.py. This script configures and executes a\nGStreamer pipeline to process video streams. It offers primary and secondary inference, tracking, and analytics features\nusing DeepStream SDK.\nâ€¢ Additionally, the module contains a scheduler that periodically converts CSV files to Parquet format, enabling eï¬€icient\nstorage and analysis of the generated results.\nBluetooth Detection Scrapping (FLWR-UbertoothService.py)\nâ€¢ Responsible for capturing Bluetooth data using the Ubertooth device which executes the ubertooth-rx command and\nrecords the received data.\nâ€¢ Periodically (every 10 minutes), the script generates new log files to facilitate easier management and analysis of the data\nby segmenting it based on time intervals.\n14\n\nServer File Sender (FLWR-SyncFoldersDaemon.py)\nâ€¢ The FLWR-SyncFoldersDaemon.py script synchronizes local folders with remote server folders using the rsync command.\nROI Filtering (drawRoi.py)\nâ€¢ The drawRoi.py script facilitates the drawing of Regions of Interest (ROIs) for the analytics module.\nLogs to Parquet Converter (FLWR-UbertoothLogsToParquet.py)\nâ€¢ The FLWR-UbertoothLogsToParquet.py script scrap log files generated by the Ubertooth device, converting them to\nParquet format for optimized storage and analysis.\nâ€¢ The script parses each log file, extracts the relevant fields and saves the data into Parquet files. After successful conversion,\nthe original log files are deleted to save disk space.\nDisk Usage Control (FLWR-DiskUsageControlDaemon.py)\nâ€¢ The FLWR-DiskUsageControlDaemon.py script monitors disk usage in specified directories and deletes the oldest files when\nusage exceeds predefined limits.\nâ€¢ It ensures that the system remains operational by preventing disk space from being filled up, thus avoiding potential\ncrashes or data loss.\nPower Management (FLWR-PowerDaemon.py)\nâ€¢ The FLWR-PowerDaemon.py script manages the power state of the system, including suspending during night hours and\nrebooting in the morning.\nâ€¢ It uses a state machine approach to track the current power state and performs actions based on the time of day, ensuring\noptimal power usage.\n15\n\n7.1.4\nArchitectures\nFigure 15: Floware Vision Architecture Representation\nAll the real-time processing is managed by the Analytics class, which saves computer vision results and activates the relay\ndepending on the class detected.\n7.1.5\nVision Processing Pipeline\nAs mentioned, FLWR-Vision uses a Deepstream pipeline to process video flux for object detection, tracking, and analytics. The\npipeline is implemented in the FLWR-Vision.py script and uses the DeepStream SDK, enabling GPU processing throughout the\nentire pipeline.\nPipeline Components\nâ€¢ Source Element: The pipeline starts with a source element that captures video from available video devices.\nThe\nfunction list_video_devices() is used to list all video devices available on the system.\nâ€¢ Inference Engine: One of the main elements of the pipeline is the primary inference element. It uses pre-trained models\nlike Yolo to perform object detection and classification.\nâ€¢ Tracker: The tracking element tracks detected objects across frames. It attributes a unique id for each detected object.\nâ€¢ Analytics Module: An analytics module processes information output from the tracking and inference elements, per-\nforming analysis if needed.\nâ€¢ Sink Element: The sink element is placed at the end of the pipeline, which can either display the video on the screen\nor save it to a file, depending on the configuration.\n16\n\nFigure 16: Existing Deepstream Pipeline Representation\nPipeline Elements\nThe usb-cam-source element captures video input from the USB camera, serving as the starting point of the video stream\nin the pipeline. Next, the nvmm_caps element applies properties to the video stream to match the expected format for future\nprocessing, setting up properties like frame rate, resolution, and format. Following this, the convertor_src1 element converts\nthe video stream into a compatible format for the next pipeline elements, ensuring compatibility for hardware-accelerated GPU\nprocessing. Another video conversion is performed by convertor_ssing. After that, the Stream-muxer element concatenates\nmultiple input video streams into a single output stream, which is essential for scenarios where multiple camera feeds are\nprocessed simultaneously. Then, the primary-inference element performs primary inference using a pre-trained deep learning\nmodel, such as YOLO, and is responsible for object detection and classification within the video stream. Configurations for\nYOLO models (YOLOv8s, YOLOv8n, YOLOv5) can be loaded via the config file here. Additionally, the tracker element tracks\ndetected objects across frames, maintaining unique identifiers for each object, and ensuring continuity of object detection over\ntime for one sensor, which is essential for traï¬€ic detection. The onscreendisplay element then overlays information such as\nbounding boxes and labels on the video stream, useful for visualizing detection and tracking results in real-time, mostly for\ndebugging and maintenance. Following this, the nvvideorenderer element renders the processed video stream to a display,\nproviding real-time visualization of the pipeline output. A sink element, fakesink, does not display the stream visualization\nand is used when visualization is not needed; it is not currently used in production. Lastly, the analytics element performs\nadditional analytics on the inferred data, processing the detection and tracking results to generate insights and metrics. In\nFloware Vision, the analysis part is transmitted to the Analytics class.\n \n17\n\n7.1.5.1\nOverall Flow\nâ€¢ Video is captured from the USB camera.\nâ€¢ The video stream is processed through several conversion steps to ensure compatibility.\nâ€¢ Inference is performed to detect and classify objects.\nâ€¢ Detected objects are tracked across frames.\nâ€¢ The results are displayed on-screen or sent to analytics for personalized processing.\nâ€¢ Additional analytics are performed to generate insights and results are saved.\n7.1.6\nModel Integration\nAt the primary inference level, YOLOv8s is used by default and pretrained for 640x480 images. Labels used (see Vision Data\nPart is tuned for traï¬€ic purposes), for performance purposes YOLO8n can be used for improving framerate (see Floware Vision\nPerformance Metrics).\n7.2\nExisting Deployment Process\nSystemd services are used to maintain scripts to run even after failure.\n7.2.0.1\nJetson Nano\nOn the Jetson Nano, Floware uses an SD card already flashed that contains Floware Vision from a previous Floware Sensor,\nthen copies bit by bit data on another sd card to avoid most of the failure that can happen on a regular copy. The new sd\ncard is then inserted in a new jetson. Bit-by-bit copy is a long process and can be time-consuming, it also does not allow any\nautomatic Floware Vision Update.\n7.2.0.2\nJetson Orin\nPrevious deployment on Jetson Orin is slightly different because itâ€™s a â€recentlyâ€ acquired hardware. Floware chose an inde-\npendent developer to convert floware Vision to Orin. The independent developer created docker images that contain every\ndependency that Floware Vision requires. By pulling the docker images in a â€virginâ€ Orin with Floware Vision on it, we can\nlaunch the application inside the docker. I was far from being finished at this state.\n7.3\nLimitations, Issues & Challenges\nChecking the Floware Vision codebase revealed that a lot of angles of improvement were possible.\n7.3.1\nStructure Issue\nFloware Visionâ€™s codebase doesnâ€™t have code documentation, making it very diï¬€icult to understand the purpose of each func-\ntionality. Besides the lack of documentation, there are many unused files with no purpose in production, including various sh,\ntxt, and py files.\nAdditionally, the class used in the code has issues that make it hard to read. The main script, FLWR-Vision.py, is not structured\nwith methods or classes, which is a major issue for modularity. If we wanted to add elements to the pipeline, such as a second\nclassifier, or fix any bugs, every feature could be impacted by small modifications.\nEven if a configuration file rigs this script, it still lacks modularity, and many features are hardcoded. For instance, we cannot\nchoose the video device or the path of the configuration file for the tracker and the primary inference; only the model name is\nin the configuration. The script uses an if-else structure with hard coded paths inside the code:\npgie = Gst . ElementFactory . make( â€ nvinfer â€ , â€primaryâˆ’inference â€ )\ni f not pgie :\nsys . stderr . write ( â€â£Unableâ£toâ£create â£pgie â£\\nâ€ )\ni f\n(c_app [ â€™app â€™ ] [ â€™MODELâ€™ ] == â€™YOLOV8Nâ€™ ) :\npgie . set_property ( â€™ configâˆ’f i l e âˆ’path â€™ , â€ cfg /YOLOV8N. txt â€ )\ne l i f\n(c_app [ â€™app â€™ ] [ â€™MODELâ€™ ] == â€™YOLOV8Sâ€™ ) :\npgie . set_property ( â€™ configâˆ’f i l e âˆ’path â€™ , â€ cfg /YOLOV8S. txt â€ )\n18\n\ne l i f\n(c_app [ â€™app â€™ ] [ â€™MODELâ€™ ] == â€™YOLOV5Nâ€™ ) :\npgie . set_property ( â€™ configâˆ’f i l e âˆ’path â€™ , â€ cfg /YOLOV5N. txt â€ )\ne l i f\n(c_app [ â€™app â€™ ] [ â€™MODELâ€™ ] == â€™YOLOV5Sâ€™ ) :\npgie . set_property ( â€™ configâˆ’f i l e âˆ’path â€™ , â€ cfg /YOLOV5S. txt â€ )\nElse :\nprint ( â€™Modelâ£Notâ£found â€™ )\n7.3.2\nSecurity Issues\nThe previously reported structural issue also brings major security concerns. The script responsible for synchronization with the\nserver has the server IP hardcoded. Additionally, there were two scripts with the same code but with different IPs for Scaleway\nand Azure, indicating a lack of modularity. The authentication keys are stored in the main Floware Vision repository without\nany protection. If an intruder accesses one of the Floware Jetson devices, they will also gain access to our servers. The Docker\nimage of the application was publicly pulled from the personal DockerHub of a freelance developer.\n7.3.3\nOptimization\nFloware Vision on Jetson Nano runs at 8 fps, while on Jetson Orin it runs at 30 fps. The low resolution and frame rate can\nreduce the precision of the measurements. There is also a size optimization issue; the Docker image for the Jetson Orin version\nof the application is 16 gigabytes, which is unusually large as it runs directly on the disk, indicating that the application is not\nproperly containerized.\n7.3.4\nDeployement issue\nThe deployment method on the Jetson Nano does not permit eï¬€icient software updating on the deployed sensor. For example,\nwe used the TeamViewer remote file transfer system to update a dozen sensors. An operation like this usually takes one and a\nhalf days, and many issues can occur during the process, such as ownership problems with the executable file or file conversion\nissues if the source computer is running Windows. Additionally, there was no online repository for the application, such as\nGitHub or GitLab, nor any version control.\n7.3.5\nBugs\nSome bugs were still in production and slowed the maintenance and the post-process offset:\nâ€¢ Permission issue: The server key was not always accessible for the systemâ€™s service due to an ownership issue.\nâ€¢ Files sending issue: On the server side, files were sent as soon they were written inside, so a lot of data was missing, they\nwere also sent before the conversion to parquet, sometimes with only an error output inside.\nâ€¢ As the video device name was hardcoded in the source pipeline element (/dev/video0), the video device was not always\nat this location, and a manual reboot was required.\n7.4\nFeedback and Improvements\nAll these issues highlight the need for structuring and organization of a work pipeline so that each fix and addition can be reviewed\nand homogenized. Besides bug fixes and performance improvements, an effort will be made in modularity and compatibility\nto ensure eï¬€icient debugging and adaptability for various missions, particularly in AI detection. Indeed, some missions require\nfine-tuning in detection, such as work vehicle detection, license plate detection, or even speed detection. The future development\nand deployment process of Floware Vision will be considered in this way.\n8\nFloware Vision: Development\nBefore any refactoring, correcting the failure-causing bugs was the priority. First, we created a bash script that deployed the\nsystemâ€™s services as the IoT user, eliminating permission issues. In the file-sending system services, we chose to ignore the last\nfile created and exclude any logs and CSV files. The video device selection was no longer hardcoded; we retrieved a list of all\ndevices and selected the first one from the sorted list.\n19\n\n(a) Bluetooth Data before fixes\n(b) Bluetooth Data after fixes\nThe Floware Vision development and refactorization process is driven by three constraints: real-time eï¬€iciency, modularity,\nand compatibility. The first decision we had to make was to choose a programming paradigm that combined these three\ncharacteristics.\nObject-oriented programming (OOP) has been used since the 1960s in software development and was the default option\nin the 1990s, the principal paradigm to learn if you wanted to be a good software developer. However, OOP has slightly lost\npopularity over time, even if in some cases, it is still the obvious choice. The purpose of OOP is to divide services into objects\nwith internal properties, allowing for easy addition or modification of features without impacting the rest of the code.\nWhen applied strictly, developers realize that OOP suffers from numerous problems, particularly in real-time and IoT devel-\nopment. Indeed, it can lead to hard-to-read code and increase the debugging and maintenance time, with a deep hierarchy\ndue to inheritance and abstraction properties. Multiple abstraction layers can lead to overhead in limited resource environ-\nments, due to the need to manage multiple objects. Mutable state objects is a significant problem in real-time processing,\ncomplicating concurrency; in multithreading, for example, it can lead to data leaks and data access issues.\nIn Floware Vision (particularly the computer vision part), we didnâ€™t find any use cases that would require a mutable internal\nstate. These are the main reasons for our choice not to use OOP in the main part of Floware Vision. Additionally, how do you\ndefine objects objectively? What would happen if an object fits into multiple categories? (See The â€œPlatypusâ€ Effect[28]).These\nquestions are not necessary and would make Floware lose time and money.\nAnother programming paradigm that has gained popularity in the last decade is functional programming[29]. While OOP\nfocuses on interacting or communicating with objects, in functional programming the emphasis is on transforming them[28].\nThe focus in functional programming is to pass an object, data, or variables into a function that returns a new object, data, or\nvariables representing the transformed input, without altering some internal state in any way. This is achieved using immutable\nobjects and â€pureâ€ functions as described previously.\nThis lack of a mutable internal state is recognized as a great way to achieve parallel computation because it avoids concurrency\nand accessibility issues. It makes the code more readable and debugging easier. Pure functions also significantly improve\nreliability in data streams because they ensure consistent output for the same input. Furthermore, the code is less bloated with\ncomplex hierarchies and inheritance, resulting in shorter, less complex code that enhances maintainability. This simplicity and\nthe reduced failure that induced is are researched in IoT programming [30]. Our sensors needed a paradigm that fitted best our\nconstraint. And functional programming is this paradigm.\n8.1\nImplementation Choice\nWe noticed quickly that Floware Vision is an aggregate of functionalities that didnâ€™t communicate; the computer vision script\ndoes not interact with the Bluetooth script, and the sender system gets the data from the folder directly. It is the same for\nthe power management and disk usage services. The application was already composite because each â€serviceâ€ was launched\nindependently, but it wasnâ€™t implicit.\nBesides the functional programming choice as a programming paradigm, a microservices4 architecture was chosen.\nBy its\ncomposite structure, it facilitates the debugging and updating of each service, but also the deployment. Indeed, teamwork is\nimproved as each member of the team can access a service and upgrade it independently. Services are faster to build, test, and\n4Microservices - also known as the microservice architecture - is an architectural style that structures an application as a collection of\nservices[31]\n20\n\ndeploy5.\nEven though functional programming is the main choice, we donâ€™t exclude the use of OOP or procedural programming when it\nis the most appropriate choice. For instance, some services may need different implementations (conversion purposes or sending\nto different servers in different formats using different libraries). In this case, using a Strategy Pattern[32] in OOP can be useful,\nor we can also functionally translate this pattern, using custom-typed functions as parameters in another function.\n8.2\nImprovement On The Existing\nAfter the bug fixes, we started by refactoring every Python script of Floware Vision to translate procedural computation into\na corpus of functions and integrate a configuration JSON file for each script, replacing hardcoded information and mutable\nobject attributes in certain scripts like Analytics. We also removed all the unused files. And organized the file hierarchy by type\n(models, configuration files, scripts, and services).\nHere are some of the improvements in the Python version of the application:\nâ€¢ Added Python documentation to all functions and a general README.\nâ€¢ Removed unused files.\nâ€¢ Refactored the Sync-folder service to use a JSON config file, allowing for the selection of the desired server.\nâ€¢ Automatically select the video device in the Vision service.\nâ€¢ Bugs Fixes\nâ€¢ Fusion of the jetson nano orin, and dGpu application, it can now run on every device.\nFloware is also required to modify the Deepstream pipeline of the computer vision to add more classifiers depending on the\nspecificities of each mission, which is easier to do since the refactoring of the script.\n5More details in the deployment section\n21\n\n8.3\nAI Model Integration\nFigure 18: Deepstream Pipeline\nCompared to the previous existing pipeline, we can notice the addition of an optional secondary and third classifier.\nIt is\ncurrently used for license plate recognition. Using custom Licence plate Detection and text Recognition models provided by\nNvidia[33]\n22\n\nFigure 19: Testing Of Licence Plate Recognition feature\n8.4\nFloware Vision Bedrock6: The C++ Version\nWe made limited changes to the Python app to ensure reduced deployment and maintenance time, but we knew Python was\nnot the best option for IoT programming where we needed to be closer to the metal7 in an optimized way. C++ remains one\nof the most used and fastest languages.\n8.4.1\nJustification for C++\nNvidia Deepstream and Gstreamer are C++ libraries. By using the Deepstream Python bindings, we are adding an intermediate\nlayer that is not necessary. Using C++ natively improves performance and also simplifies the deployment process and the edge\ndevice. Not having to install Python bindings[34] also improves compatibility between devices by avoiding any issues caused by\nPython or pip, but also reduces the bload of IoT. The fewer elements we have to care about, the fewer issues will happen.\nC++ is natively typed and supports both OOP and functional programming[35].\nIt contains every library needed for our\nservices (even if our architecture permits to use of different languages depending on the service) like Azure Iot, Deepstream,\nand Ubertooth. We choose the classic C++ naming convention.\n6A well chosen name.\n7Closer To The Hardware\n23\n\n8.4.2\nDesign & Architecture\nFigure 20: C++ Floware Vision Bedrock Architecture\nThe architecture graph shows how the data is processed from left to right, providing a clearer view of the microservices\narchitecture. Each service is contained with its libraries or configuration files. Notably, the convert-to-parquet service, which was\npreviously (see 15) implemented in two different scriptsâ€”FLWR-Bluetooth for Bluetooth data and hardcoded in the FLWR-Vision\nScript for computer vision dataâ€”is now an independent service allowing multiple conversion methods. Similarly, the sync folder\nservice has been improved. Floware decided to abandon Virtual Machine storage in favor of using Azure Blob Storage[36] for\ncost eï¬€iciency, management, and scalability. By using the Azure library inside the application, we need to ensure the possibility\nof multiple implementations to facilitate future migrations to other services. Floware seeks European partners. We could have\nthe need, in the future, to use a European cloud instead of Microsoft Azure.\n9\nFloware Vision: Deployment\nFloware possesses 46 sensors, and deploying and updating them eï¬€iciently is crucial for maintaining our reputation with clients.\nAs mentioned above, the initial deployment and debugging method used TeamViewer[37] file transfer system. This method was\nnot scalable because it was not designed for this purpose. Deploying 10 sensors required two people to spend one and a half\ndays. Even before refactoring the legacy code, our main challenge was to find scalable and eï¬€icient ways to deploy and update\nFloware Vision.\n9.1\nDeployment Strategies\nSetting up a Jetson from scratch can be time-consuming due to the number of dependencies to install, such as Gstreamer,\nDeepstream, and Python bindings. Noticing the significant time loss during this initial setup, we decided to use a container\nsystem. This approach allows us to include all dependencies inside a container and then run it to execute any application stored\ninside a pre-setup environment. We chose to use Docker containers[38] for this purpose.\nWhere to store the created container? Many solutions exist, like Azure Container Registry, GitHub Container Registry, or\n24\n\nDockerHub.\nWe chose Azure Container Registry for its capacity (GitHub Container Registry is restricted to 500MB per\ncontainer) and for practicality, as we already have a startup Azure Subscription.\nBy modifying the Dockerfile8 and refactoring the application, we reduced the container image size from 16 gigabytes to 3.4\ngigabytes.\nWith this container, we have an object we can easily pull and run. The next challenge was scalability: how to automate the\ndeployment process?\n9.2\nIntroduction to Azure IoT\nAccording to Microsoft:\nâ€IoT Hub is a managed cloud-hosted service that acts as a central message hub for bi-\ndirectional communication between your IoT solution and the devices it manages.â€ [39]. Azureâ€™s key feature is Device\nManagement; it simplifies the process of adding new devices to the Azure IoT Hub and also allows the monitoring of their\nstatus or performance while updating device firmware over the air. Our main issue with TeamViewer was scalability; Azure\nIoT can manage hundreds of devices simultaneously. Additionally, the security of our devices is improved by having unique\ncredentials per device and managing permissions for each device group with custom user creation.\nEach device can then be recorded as an Azure Edge module and deployed with our Docker images directly from our Azure\nPortal or Visual Studio Code.\n9.3\nCI/CD: Actual Deployment Pipeline\nIf we use Git, a good deployment pipeline also means a good versioning pipeline. We chose to create a Floware GitHub repository\nto store every project the company has. We defined strict conventions to ensure the most secure deployment process. First, we\ndefined the production branchâ€”the one that contains the version of the code we deploy. We protect this branch to avoid any\narbitrary other branch merging or committing without a review. If we want to add a modification to the code, we must create\na new branch with a name reflecting its purpose (e.g., fix/ubertooth, feature/parquetconversion, update/syncfolder).\nWhen the fix or feature update is implemented, the user should create a pull request9 into a development branch. The senior\ndevelopers then review the modifications and decide whether to merge them. When a new version of the code is finished in the\ndevelopment branch, the content is transmitted to a stash branch for testing and debugging purposes, so developers can still\nadd new modifications to the development branch.\nWhen every feature is tested, we merge the code into production. We can now create a GitHub Release from this production\nbranch with a version tag, representing the version of the application (for instance, flwr vision v1.2.3).\n8The file that contains the build instruction of the docker image\n9A request to add these modifications into another branch\n25\n\nFigure 21: Deployment Pipeline representation\nAfter this release, we create a GitHub Action pipeline triggered at every new release that takes place in a Linux arm64\nenvironment (same as a Jetson) that contains a list of automated instructions. Here, the GitHub Action connects to Azure\nContainer Registry with secretly stored login credentials10. The Docker image is then built and pushed to ACR with the version\nas a container tag. We can then deploy the images to all or a fraction of Azure IoT Edge devices and receive metrics and status\nupdates to ensure the deployment goes right.\n10\nResults and Discussion\n10.1\nFloware Vision Performance Metrics\nFloware Vision can use two performance metrics sender, deepstream perf for the framerate of floware vision, and jtop, a software\nthat extracts every property of a jetson in real-time, it can be useful for the creation of failure prediction models, or create\nperformance benchmarks on the different Floware vision version.\n10.2\nCase Studies : Yolo8s vs Yolo8n performance\nYOLOv8s[41] and YOLOv8n are designed for different computational capacities and use cases.\nYOLOv8s is optimized for\na balance between speed and accuracy, making it suitable for applications where moderate hardware resources are available.\nYOLOv8n, on the other hand, is designed for scenarios where minimal computational resources are required, prioritizing speed\nand eï¬€iciency. Having a latency issue on Floware Vision, itâ€™s important to know which model has the best accuracy for resources\ntaken.\n10See GitHub Secrets[40]\n26\n\nâ€¢ Launching Floware Vision\nâ€“ YOLOv8s FPS: 8.8 - 9.2\nâ€“ YOLOv8n FPS: 17.2 - 17.9\nâ€¢ One person detection\nâ€“ YOLOv8s FPS: 7.6 - 8.4\nâ€“ YOLOv8n FPS: 13.0 - 15.6\n27\n\nâ€¢ End detection\nâ€“ YOLOv8s FPS: 7.6 - 8.4\nâ€“ YOLOv8n FPS: 13.0 - 15.6\nâ€¢ Person & license plate detection\nâ€“ YOLOv8s FPS: 5.7 - 8.8\nâ€“ YOLOv8n FPS: 11.0 - 15.3\nDespite greater GPU usage, we observe better framerate on YOLOv8n and reduced CPU usage, indicated by a flatter courb\ncompared to YOLOv8s. We still need post-process data analysis to judge if the model size loss justifies the better performance,\nas framerate can affect tracking accuracy.\n10.3\nOutput Analysis: the end side of the pipeline\n10.3.1\nVisualization of Flows\nThe first task is visualization. We aggregate sensor data for one or two days using the pandaâ€™s library. We determine the central\npoint of the bounding box, then for each object, we draw a line between each point. The challenge lies in reducing the noise\npoints. The linear interpolation method was then used to obtain clean and viewable trajectories.\nFigure 22: Flux Visualization of sensor FLWR-007\n10.3.2\nTransition Matrix\nThis task was carried out in two parts. First, we need to determine regions of interest (ROIs) for each sensor (there are 15\nsensors, so some task automation is required). Then, for a chosen time step, we determine the number of movements from one\nROI to another for each vehicle class. The required output is a matrix in the following form:\ninterval\ntransition\nnb_transitions\nmode\ncapter_name\nverification\n16/09/23 07:00-07:15\nT1-2\n0\ncar\nFLWR-004\nOK\n16/09/23 07:00-07:15\nT1-3\n5\ncar\nFLWR-004\nOK\n16/09/23 07:15-07:30\nT..-..\n...\n...\nFLWR-00...\nOK\n28\n\n10.3.3\nActivity Diagram\nThe required output is a diagram in the form of a CSV document usable for PowerBi, representing the activity time of a sensor\nover a day, week, or month. The sensor is considered inactive if there is no data for an extended period.\n10.3.4\nPost-Alerting\nWe check the files received on the server side, and if any data is missing, we send a Slack alert to address the issue as quickly as\npossible in the following format: [Mission][Importance Level][Sensor Name][Data Type (Bluetooth or Computer Vision]\nNo DATATPE file received since N days, TIME\nExample of an alert output:\nâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’\n[ECOFLOW] [RED] [FLWRâˆ’017][BT] No BT f i l e\nreceived\nsince 14 days ,\n19:36:31.251712\n[ECOFLOW] [RED] [FLWRâˆ’017][CV] No CV f i l e\nreceived\nsince 14 days ,\n19:38:30.996416\n[ECOFLOW] [RED] [FLWRâˆ’021][BT] No BT f i l e\nreceived\nsince\n100 days ,\n13:46:31.340826\n[ECOFLOW] [RED] [FLWRâˆ’021][CV] No CV f i l e\nreceived\nsince\n100 days ,\n13:02:31.331053\n[ECOFLOW] [RED] [FLWRâˆ’022][BT] No BT f i l e\nreceived\nsince 21 days ,\n2:07:31.396548\n[ECOFLOW] [RED] [FLWRâˆ’022][CV] No CV f i l e\nreceived\nsince 21 days ,\n2:05:31.379133\nâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’\n10.3.5\nQueue Length Measurement\nFor each ROI, we aim to determine the average distance thresholds traveled by each vehicle to classify them as moving, slowing\ndown, or stopping. Then, we choose a distance for each ROI that corresponds to the queue length and their states over time\nbased on the average mobility states of all vehicles. By cumulating the queue lengths for several ROIs according to the direction\nof traï¬€ic, we obtain a queue length for each congestion period, presented in a table format with columns for time and queue\nlength.\n10.3.6\nProjection\nThe problem with data analysis on our sensors is that distances are relative to the orientation of the sensor or the road, meaning\nthe distances between points do not correspond to real distances. We project the points from the sensor image onto the satellite\nimage using various research methods, including the RANSAC regression algorithm[42].\nFigure 23: Scatter Plot of Detections on Sensor Images\n29\n\nFigure 24: Scatter Plot Projected onto a Satellite Image of the Sensor Location, Still In Testing Stage\n11\nConclusion\nWriting this memoir made me realize the importance and advantages of Flowareâ€™s choice to use Edge AI for traï¬€ic analysis, due\nto its scalability, data privacy in public spaces, and real-time processing capabilities. Additionally, it highlighted the need for\na robust and maintainable architecture and deployment process to ensure trust and reliability for our clients. The future work\nwill focus on completing our deployment plan in parallel with finishing the development of Floware Vision Bedrock. After that,\nthe next few months will be dedicated to training and testing new computer vision models fine-tuned for specific mission tasks.\nFor instance, as part of our partnership with Vinci, we need to fine-tune a YOLO model for classifying various types of trucks.\nIn the long term, Edge AI will not be the only AI paradigm used. Work on flux simulation model and large language model\nfor interrogating our database. I am confident that the continued development and refinement of our Edge AI solutions will\ncontribute significantly to advancements in traï¬€ic management and urban mobility. We are confident that our ongoing efforts\nto develop and refine our Edge AI solutions will play a significant role in advancing traï¬€ic management and urban mobility.\n30\n\nReferences\n[1] â€œModules et kits de dÃ©veloppement pour systÃ¨mes Embedded | NVIDIA Jetson.â€ https://www.nvidia.com/fr-fr/autonomous-\nmachines/embedded-systems/.\n[2] M. Treiber, A. Kesting, and C. Thiemann, â€œHow Much does Traï¬€ic Congestion Increase Fuel Consumption and Emissions?\nApplying a Fuel Consumption Model to the NGSIM Trajectory Data,â€\n[3] â€œInstitut vedecom website.â€ available at https://www.vedecom.fr/accueil-2/ite/quisommesnous.\n[4] â€œMoove Lab â€“ Lâ€™accÃ©lÃ©rateur des startups de la mobilitÃ© de Station F.â€ available at https://moove-lab.com/en/.\n[5] â€œLeonard, la plate-forme de prospective et dâ€™innovation de VINCI.â€ https://leonard.vinci.com/.\n[6] â€œNextmove : PÃ´le de compÃ©titivitÃ© EuropÃ©en de la mobilitÃ©.â€ https://nextmove.fr/.\n[7] â€œEcomesure.â€ https://ecomesure.com/en.\n[8] â€œFabMob France.â€ https://lafabriquedesmobilites.fr.\n[9] â€œWintics â€¢ Cityvision, video analysis software for urban stakeholders.â€ https://wintics.com/en/.\n[10] â€œCollecte & valorisation des donnÃ©es de mobilitÃ©.â€ https://www.alyce.fr/.\n[11] â€œAccueil, UPcity.â€ https://www.up-city.be/.\n[12] â€œEurovia.â€ https://www.eurovia.fr/.\n[13] â€œAccueil | CDVIA.â€ https://www.cdvia.fr/.\n[14] S. Denning, â€œWhat Is Agile?.â€ https://www.forbes.com/sites/stevedenning/2016/08/13/what-is-agile/.\n[15] â€œCloud Computing, sÃ©curitÃ© et rÃ©seau de diffusion de contenu (CDN).â€ https://www.akamai.com/fr.\n[16] â€œWhat Is Edge AI? | IBM.â€ https://www.ibm.com/topics/edge-ai, Aug. 2023.\n[17] K.\nChowdhary,\nâ€œMicrosoft\nAzure\nHit\nWith\nThe\nLargest\nData\nBreach\nIn\nIts\nHistory;\nHundreds\nOf\nExecu-\ntive Accounts Compromised.â€ https://techreport.com/news/microsoft-azure-hit-with-the-largest-data-breach-in-its-history-\nhundreds-of-executive-accounts-compromised/, Feb. 2024.\n[18] â€œProspera.ag.â€ https://prospera.ag.\n[19] â€œSite\noï¬€iciel\nFitbit\n:\ncoachs\nÃ©lectroniques\npour\nla\nforme\net\nle\nsport,\net\nbien\nplus\nencore.â€\nhttps://www.fitbit.com/global/fr/home.\n[20] â€œWhat\nIs\nComputer\nVision?\n|\nMicrosoft\nAzure.â€\nhttps://azure.microsoft.com/en-us/resources/cloud-computing-\ndictionary/what-is-computer-vision.\n[21] K. Fukushima, â€œNeocognitron,â€ Scholarpedia, vol. 2, p. 1717, Jan. 2007.\n[22] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, â€œYou Only Look Once: Unified, Real-Time Object Detection,â€ May\n2016.\n[23] S. SÃ¡nchez HernÃ¡ndez, H. Romero, and A. Morales, â€œA review: Comparison of performance metrics of pretrained models\nfor object detection using the tensorflow framework,â€ IOP Conference Series: Materials Science and Engineering, vol. 844,\np. 012024, 06 2020.\n[24] â€œUbertooth One - Great Scott Gadgets.â€ https://greatscottgadgets.com/ubertoothone/.\n[25] â€œGStreamer/gstreamer.â€ GStreamer GitHub mirrors, July 2024.\n[26] â€œDeepStream SDK.â€ https://developer.nvidia.com/deepstream-sdk.\n[27] â€œUltralytics: Ultralytics YOLOv8 for SOTA object detection, multi-object tracking, instance segmentation, pose estimation\nand image classification..â€\n[28] Talin, â€œThe Rise and Fall of Object Oriented Programming,â€ Nov. 2018.\n[29] S. Akhmechet, â€œFunctional Programming For The Rest of Us.â€ https://www.defmacro.org/2006/06/19/fp.html, June 2006.\n[30] T. HÃ¤nisch, â€œA Case Study on Using Functional Programming for Internet of Things Applications,â€ Athens Journal of\nTechnology & Engineering, vol. 3, pp. 29â€“38, Feb. 2016.\n[31] â€œWhat are microservices?.â€ http://microservices.io/index.html.\n[32] V. Thennakoon and B. Hettige, A STUDY ON OBJECT-ORIENTED DESIGN PRINCIPLES AND PATTERNS. July\n2022.\n[33] â€œNVIDIA-AI-IOT/deepstream_lpr_app.â€ NVIDIA AI IOT, June 2024.\n[34] â€œNVIDIA-AI-IOT/deepstream_python_apps.â€ NVIDIA AI IOT, July 2024.\n[35] kexugit,\nâ€œC++\n-\nFunctional-Style\nProgramming\nin\nC++.â€\nhttps://learn.microsoft.com/en-us/archive/msdn-\nmagazine/2012/august/c-functional-style-programming-in-c, Jan. 2016.\n31\n\n[36] â€œAzure Blob Storage | Microsoft Azure.â€ https://azure.microsoft.com/en-us/products/storage/blobs.\n[37] â€œTeamViewer - Le logiciel de connectivitÃ© Ã  distance.â€ https://www.teamviewer.com/fr/.\n[38] â€œOverview of the Docker workshop.â€ https://docs.docker.com/guides/workshop/, 11:50:25 -0700 -0700.\n[39] leestott,\nâ€œIntroduction\n-\nTraining.â€\nhttps://learn.microsoft.com/fr-fr/training/modules/introduction-to-iot-hub/1-\nintroduction.\n[40] â€œUsing\nsecrets\nin\nGitHub\nActions.â€\nhttps://docs.github.com/_next/data/cjISr1MlIQhzdaVAuo8xo/en/free-\npro-team@latest/actions/security-guides/using-secrets-in-github-actions.json?versionId=free-pro-\nteam%40latest&productId=actions&restPage=security-guides&restPage=using-secrets-in-github-actions.\n[41] Ultralytics, â€œYOLOv8.â€ https://docs.ultralytics.com/models/yolov8.\n[42] M. Rezaei, M. Azarmi, and F. M. P. Mir, â€œ3d-net: Monocular 3d object recognition for traï¬€ic monitoring,â€ Expert Systems\nwith Applications, vol. 227, p. 120253, 2023.\n32\n"
}