{
    "1 - Introduction to RL and MDPs.pdf": "Reinforcement Learning\nAn Introduction\n\nInstructor\nHussam ATOUI\nâ—\nSoftware Engineer at Valeo \nCrÃ©teil (France) since Nov 2022\nâ—\nPhD-Cifre RENAULT & \nGrenoble-Alpes University \n(2019-2022)\nâ—\nSpecialities: Automated Driving, \nReinforcement Learning, \nAutomatic Control, Optimization\n\nInstructor\nVictor MORAND\nmorand@isir.upmc.fr\nâ—\nPhD Student at ISIR (Sorbonne - \nCNRS)\nâ—‹\nExplaining how LLMs manipulate \nKnowledge\nâ—‹\ntowards AIs that know what they \nknow\nâ—\nAlso out of a School of Engineering !\nFeel free to reach out at the end of our \nsessions ! \n\nCourse Content\n1.\nIntroduction to Reinforcement Learning\n2.\nMarkov Decision Processes (MDPs)\n3.\nPolicy and Value Functions\n4.\nDynamic Programming (DP) for RL\n5.\nModel-Free methods\n6.\nValue Function Approximation\n7.\nPolicy-Gradient and Actor-Critic Methods\n8.\nDeep RL\n9.\nTP Project\n\nIntroduction to Reinforcement \nLearning (RL)\n\nSupervised Learning\nReinforcement Learning\nUnsupervised Learning\nTrain with labeled data\nTrain with unlabeled data\nClustering\nTrain with environment \nexperience\nRegression\nClassiï¬cation\n6\nTypes of Learning\n\nSupervised Learning \nModel\nInputs:\nFeatures / States\nPredicted Outputs:\nValue/ Class\nTraining target:\nTarget Output\nâ—\nError: Target Output - Predicted Output\nâ—\nObjective: Minimize the error between the target and the predicted output\n7\nSupervised Learning\n\nSupervised Learning\n\nReinforcement Learning\nReinforcement \nLearning Model\nInputs:\nFeatures / States\nPredicted Outputs:\nActions\nEvaluation:\nRewards / Penalties\nâ—\nError: Awards - Penalties \nâ—\nObjective: Maximize the awards and decrease penalties as much as possible\n9\n\nReinforcement Learning\n\nExamples of Rewards [1]\nâ—\nFly stunt manoeuvres in a helicopter\nâ—‹\n+ve reward for following desired trajectory\nâ—‹\nâˆ’ve reward for crashing\nâ—\nManage an investment portfolio\nâ—‹\n+ve reward for each $ in bank\nâ—\nControl a power station\nâ—‹\n+ve reward for producing power\nâ—‹\nâˆ’ve reward for exceeding safety thresholds\nâ—\nMake a humanoid robot walk\nâ—‹\n+ve reward for forward motion\nâ—‹\nâˆ’ve reward for falling over\nâ—\nPlay many different Atari games better than humans\nâ—‹\n+/âˆ’ve reward for increasing/decreasing score\n11\n\nAgent and Environment\nActions   A(t)\nObservations   O(t) \nRewards   R(t) \nAgent\nEnvironment\nAt step t: \nThe Agent:\nâ—\nReceives O(t)\nâ—\nReceives R(t)\nâ—\nExecutes A(t)\nThe Environment:\nâ—\nReceives A(t)\nâ—\nEmits O(t+1)\nâ—\nEmits R(t+1)\nt++\n12\n\nFully Observable Environment\nObservations   O(t) \nAgent\nEnvironment\nâ—\nEnvironment observations = Agent \nstate\nâ—\nThis is assumed in Markov Decision \nProcess (MDP)\n13\n\nPartially Observable Environment\nObservations   O(t) \nAgent\nEnvironment\nâ—\nEnvironment observations â‰  Agent state\nâ—‹\nA drone navigating a forest only sees \nnearby obstacles.\nâ—‹\nA healthcare agent observes patient \nsymptoms but not the underlying \ndisease.\nâ—‹\nA self-driving car detects nearby \nvehicles but not hidden pedestrians.\nâ—‹\nA weather forecasting model \nobserves recent conditions but not \nfuture patterns\nâ—\nThis is called Partially Observable Markov \nDecision Process (POMDP)\n(Missing info)\n14\n\nReinforcement Learning\n15\nAgent: The system that takes \nactions to be trained.\nEnvironment: The external \nsystem with which the agent \ninteracts.\nState: The information \nrequired by the agent to take \nan action. This info is observed \nfrom the environment.\nAction: The decision or \nmove that the agent makes \nat a particular state\nReward: Feedback received \nby the agent to evaluate the \ntaken action under a certain \nstate.\nGeneral Architecture\n\n16\nReinforcement Learning\n\nPolicy\nRL Agent\nA policy deï¬nes the agentâ€™s behavior in the environment. It \nrepresents a mapping from states to actions, for example:\nâ—\nDeterministic policy:                   , \nwhere the action a is chosen directly based on state s.\nâ—\nStochastic policy:                                                   ,\nwhere the policy gives the probability of taking action \na given state s.\n17\n\nValue Function\nRL Agent\nA value function \nâ—\nestimates the expected future reward \nâ—\nassesses the quality of states, helping to determine the best actions to \ntake. \nFor example, the state value under policy ğœ‹ is given by:\nThis equation expresses the expected sum of discounted rewards starting from \nstate ğ‘ .\n18\n\nValue Function\nRL Agent\nA value function \nâ—\nestimates the expected future reward \nâ—\nassesses the quality of states, helping to determine the best actions to \ntake. \nFor example, the state value under policy ğœ‹ is given by:\nThis equation expresses the expected sum of discounted rewards starting from \nstate ğ‘ .\n19\n\nValue Function\nRL Agent\n20\nÎ³âˆˆ[0,1]:\nâ—\nIf Î³=0, the agent focuses solely on immediate rewards.\nâ—\nIf Î³=1, future rewards are valued equally to immediate rewards.\n\nModel\nRL Agent\nA model forecasts the environment's next state and expected reward:\nâ—\nğ‘ƒ represents the probability of the next state given the current state and \naction:\nâ—\nğ‘… represents the expected immediate reward given the current state and \naction:\n21\n\nStates, Actions, Rewards\nExample: Maze [1]\nâ—\nStates: Agentâ€™s location\nâ—\nActions: Right, Left, Up, Down\nâ—\nRewards: -1 per time-step\n22\n\nPolicy\nExample: Maze [1]\nArrows represent policy Ï€(s) for \neach state s\n23\n\nExample: Maze [1]\nNumbers represent value           of \neach state s\n24\n\nDifferent Types\nRL Agents\nâ”\nValue-based:\nâ—†\nNo Policy\nâ—†\nValue Function\nâ”\nPolicy-based:\nâ—†\nPolicy\nâ—†\nNo Value Function\nâ”\nActor-Critic:\nâ—†\nPolicy\nâ—†\nValue Function\nâ”\nModel-free:\nâ—†\nPolicy and/or Value Function\nâ—†\nNo Model\nâ”\nModel-based:\nâ—†\nPolicy and/or Value Function\nâ—†\nModel\n25\n\nMarkov Decision Processes (MDPs)\n\nMarkov Process\nâ—\nA Markov Process is a memoryless process where the future state depends only \non the current state and not on any past states.\nâ—\nFormally, a Markov Process is a tuple: M=(S,P)\nWhere:\nâ—\nS: A ï¬nite set of states.\nâ—\nP: Transition probabilities between states, deï¬ned as:\n27\n\nThe Markov Property\nâ—\nMarkov property: Future depends only on the present, not past states\nâ—\nSimpliï¬es state transition modeling\n28\n\nâ—\nA Markov Reward Process is a Markov Process with added rewards.\nâ—\nIt is represented as a tuple: MR=(S,P,R,Î³)\nWhere:\nâ—\nR(s): Reward function providing the expected reward at each state s, \nâ—\nÎ³: Discount factor, controlling the importance of future rewards.\n29\nMarkov Reward Process\n\nCumulative Reward - Gain\nMarkov Reward Process\nâ—\nCumulative Reward G(t) : Expected cumulative reward from state s\n30\nâ—\n(State-)Value Function v(s) : Expected state-value of state s\n\nState-Value Function\nBellman Equation\nâ—\nThe state-value function can be presented as an immediate reward and future reward \nas follows:\nPROOF?\n31\n\nProof\nBellman Equation\nStochastic Eq.\n?\n32\n\nExample\n\nExample: Student MRP (P, S, R) [1] \n34\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n35\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n36\n\nExercise\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n38\n?\n\nExample of Bellmanâ€™s equation\nExample: Student MRP (P, S, R) [1]  \n39\n\nMRP â†’ MDP\n(P, S, R) â†’ (P, S, A, R)\n\nMarkov Decision Process (MDP)\n A Markov decision process is a 4-tuple (S, A, P, R):\nNote: A ï¬nite MDP is an MDP with ï¬nite state, action, and reward \nsets. Much of the current theory of reinforcement learning is \nrestricted to ï¬nite MDPs.\nâ—\nStates (S): Describe environment situations\nâ—\nActions (A): Choices available to the agent\nâ—\nRewards (R): Immediate feedback for actions\nâ—\nTransition Probabilities (P): Likelihood of reaching a \nnew state\n41\n\nState Transitions - Policy\nMarkov Decision Process\nâ—\nTransition probability: P(sâ€²âˆ£s,a)\nâ—\nModels probability of moving to sâ€² from s after action a\n42\n\nReward Function and Policy\nMarkov Decision Process\nâ—\nReward function R(s,a): Immediate feedback\nâ—\nPositive rewards encourage actions; negative prevent actions\nâ—\nDeterministic policy:                   , \nwhere the action a is chosen directly based on state s.\nâ—\nStochastic policy:                                                   ,\nwhere the policy gives the probability of taking action a given \nstate s.\n43\n\nPolicies\nMarkov Decision Process\nâ—\nDeterministic policy: \n                  \nwhere the action a is chosen directly based on state s.\nâ—\nStochastic policy:         \n                                          \nwhere the policy gives the probability of taking action a given state s.\n44\n\nExample: Student MDP (P, S, A, R) [1]  \n45\n\nValue Functions\nMarkov Decision Process\nâ—\nState-value function : Expected cumulative reward from state s under policy Ï€\n46\nâ—\nAction-value function: Expected reward of taking action a in state s under policy Ï€\n\nState-Value Function\nMarkov Decision Process\n47\n\nBellman Expectation Equation\n48\nâ—\nState-value function : Expected cumulative reward from state s under policy Ï€\nâ—\nAction-value function: Expected reward of taking action a in state s under policy Ï€\n\nBellman Expectation Equation [1]\n49\n\nBellman Expectation Equation [1]\n50\n\nBellman Expectation Equation [1]\n51\n\nBellman Expectation Equation [1]\n52\n\nExercise\n\nExample: Student MDP\n54\n?\n\nExample: Student MDP\n55\n\nState-Value and Action-Value Functions\nBellman Optimality\nâ—\nThe optimal state-value function\nâ—\nOptimal action-value function\n56\n\nExercise: Optimal State-Value Function [1]\n57\n\nExercise: Optimal Action-Value Function [1]\n58\n\nFind an Optimal Policy\nâ—\nAn optimal policy Ï€âˆ— can be determined by selecting actions that \nmaximize the optimal action-value function qâˆ—(s,a). The optimal policy Ï€âˆ—\n(aâˆ£s) is deï¬ned as:\nâ—\nFor any MDP, there is always a deterministic optimal policy. If qâˆ—(s,a) is \nknown, we can directly derive the optimal policy from it.\n59\n\nExercises\n\nExercise 1: Understanding Policies\nQuestion:\nLet S={s1,s2} be a set of two states and A={a1,a2} be a set of two actions. Suppose a \nstochastic policy Ï€ is deï¬ned as follows:\n1.\nWhat is the probability of taking action a2  in state s1  under this policy?\n2.\nIf the agent is in state s2 , what is the probability of taking action a1  under this \npolicy?\n61\n\nExercise 1: Understanding Policies\nSolution:\n1.\nThe probability of taking action a2  in state s1  is given directly by Ï€(a2 |s1 )=0.3\n2.\nThe probability of taking action a1  in state s2  is given by Ï€(a1 âˆ£s2 )=0.4\n62\n\nExercise 2: State-Value Function\nQuestion:\nConsider a simple MDP with two states s1  and s2  and a single action a with the \nfollowing reward structure:\nâ—\nStarting from s1  and taking action a, the agent moves to s2  with a reward of 5.\nâ—\nStarting from s2  and taking action a, the agent stays in s2  and receives a \nreward of 3.\nAssuming a discount factor Î³=0.9 and a deterministic policy where action a is \nalways taken, compute the value of each state v(s1 ) and v(s2 ).\n63\n\nExercise 2: State-Value Function\nSolution:\nThe Bellman equation for the value of each state s is:\n1.\nFor s2 :\nSolving for v(s2 ) â†’ v(s2)=30\n2.\nFor s1 :\nThus, v(s1)=32 and v(s2)=30.\n64\n\nExercise 3: Action-Value Function\nQuestion:\nUsing the same MDP setup as in Exercise 2, calculate the action-value q(s1 ,a) and \nq(s2 ,a) for each state-action pair.\n65\n\nExercise 3: Action-Value Function\nSolution:\nThe Bellman equation for the action-value function is:\nUsing the state values calculated in Exercise 2:\n66\n\nExercise 4: Bellman Optimality Equation\nQuestion:\nSuppose we have an MDP with three states S={s1,s2,s3} and two actions A={a1,a2}. \nThe reward function and transitions are given below:\nâ—\nFrom s1  taking a1  leads to s2  with reward 4.\nâ—\nFrom s1  taking a2  leads to s3  with reward 2.\nâ—\nFrom s2  taking a1  leads to s3  with reward 5.\nâ—\nFrom s3  taking a1  or a2  leads back to s3  with reward 3.\nAssuming a discount factor Î³=0.9, write the Bellman optimality equation for vâˆ—(s1 ).\n67\n\nExercise 4: Bellman Optimality Equation\n68\nSolution:\nThe Bellman optimality equation for the state-value function is:\nSubstituting the rewards:\nTo solve this, we would need the values of v*(s2) and v*(s3), which can be calculated \nrecursively by applying the Bellman optimality equation to each state.\n\nExercise 4: Bellman Optimality Equation\n69\nSolution:\nThe optimal values for each state are: \nâ—\nv*(s1) = 32.8\nâ—\nv*(s2) = 32\nâ—\nv*(s3) = 30\n\nExercise 5: Optimal Policy Derivation\nQuestion:\nIf the optimal action-value function qâˆ—(s,a) for some state s is given by:\nâ—\nqâˆ—(s,a1 )=12\nâ—\nqâˆ—(s,a2 )=10\nWhat is the optimal policy Ï€âˆ—(aâˆ£s)?\n70\n\nExercise 5: Optimal Policy Derivation\nSolution:\nThe optimal policy Ï€âˆ—(aâˆ£s) chooses the action that maximizes qâˆ—(s,a).\nSo:\n \nThus, the optimal policy is to always choose action a1  in state s, since qâˆ—(s,a1 )>qâˆ—\n(s,a2 ).\n71\n\nExploration & Exploitation\n\nExploration vs. Exploitation\nâ—\nIn RL, the agent faces a dilemma between:\nâ—‹\nExploration: Trying new actions to discover valuable \noutcomes. (can be harmfulâ€¦)\nâ—‹\nExploitation: Choosing actions that have yielded high \nrewards in the past.\n73\nâ—\nGoal: Balance exploration and exploitation to maximize rewards over time.\nâ—\nChallenge: Too much exploration can delay achieving rewards, while too \nmuch exploitation can lead to suboptimal long-term results.\n\nExploration: Discovering New Opportunities\nâ—\nExample 1 - A robot navigating a maze:\nâ—‹\nThe robot tries unfamiliar paths to locate shorter routes or more valuable \nrewards.\nâ—\nExample 2 - A recommendation system:\nâ—‹\nOccasionally recommends new, lesser-known products to a user to \nlearn their interests.\nâ—\nBeneï¬t: Exploration can uncover higher rewards that arenâ€™t immediately \nobvious.\n74\n\nExploitation: Leveraging Known Information\nâ—\nExample 1 - A trading agent:\nâ—‹\nSelects stocks it has previously identiï¬ed as proï¬table, prioritizing \nconsistency over discovering new options.\nâ—\nBeneï¬t: Exploitation capitalizes on known successes, ensuring steady rewards.\nâ—\nExample 2 - A game-playing AI:\nâ—‹\nRepeats a high-reward move (e.g., a chess opening) that has led to victories \nin past games.\n75\n\nAny Questions ? \nDonâ€™t hesitate to contact me\nmorand@isir.pmc.fr\n\nAny Questions ? \nContact us !\nhussam.atoui@valeo.com\n\nReferences\n78\n[1] David Silver, Lectures on Reinforcement Learning, 2015\n[2] Reinforcement Learning and Advanced Deep Learning (Sorbonne) - Olivier \nSigaud\n[3] Sutton, R. S. and Barto, A. G. (2018), Reinforcement Learning: An Introduction \n(Second edition). MIT Press\n \nOlivier Sigaud Youtube Channel\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nState (St)\n\nAction\nReward (Rt) (At)\n\nRien)\nSie) Environment\n\n\n\n{\\mathfrak{v}}^{\\chi^{\\prime}{\\overset{\\underset{\\sim}}\n\n\n\n[image]\n\n\n\nTonya oge care\n\nTHE CORRECT\nANSWER WAS:\nyest!\n\n\n\nFreely inspired from PHDComics.com|\n\nDD You GET YOU GET -10.45...\nYOUR PROJECT\nTO WORK?\n\n\n\nACTION\n\nâ€”â€”â€”â€”*\n\n\n\nACTION\n\nâ€”â€”â€”â€”*\n\n\n\nACTION\n\nâ€”â€”â€”â€”*\n\n\n\nACTION\n\nâ€”â€”â€”â€”*\n\n\n\n[image]\n\n\n\n$$ \\begin{array}{c}{{\\mathrm{Deterripiriztic~Policy\\\\ {{\\mathrm{\\\\ {{\\dot{\\ \\\\ {{\\dot{\\cal A}=\\overline\\cal T(S)}}\\end{array} $$\n\n\n\n$$ \\begin{array}{c}{{\\operatorname{Stochastic}\\operatorname{Policy\\\\ {{m(\\partial|S)=P[A_{t}={\\mathcal{B}}_{t}={\\mathcal{S}}]}}\\end{array} $$\n\n\n\n{\\mathfrak{s o}}_{\\mathfrak{h}}\\mathfrak{g}\\mathfrak{i}\n\n\n\nag= 715%\na4 = 25%\n\n\n\nv_{\\pi}(s)=\\mathbb{R}_{\\pi}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\cdot\\cdot\\cdot\\right]S_{t}=s\\right]\n\n\n\nv_{\\pi}(s)=\\mathbb{R}_{\\pi}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\cdot\\cdot\\cdot\\right]S_{t}=s\\right]\n\n\n\nv_{\\pi}(s)=\\mathbb{R}_{\\pi}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\cdot\\cdot\\cdot\\right]S_{t}=s\\right]\n\n\n\n+ a5 + + +\ny â„¢\n\ny~\\ rN\nV(So) T>|  IHre\nont. ft. me\n\n\n\nPe = PlSir1 = 8 | Sp = 8, Ap =\n\n\n\nR_{s}^{a}=\\mathbb{E}\\left[R_{t+1}\\mid S_{t}=s,A_{t}=a\\right]\n\n\n\n$$ \\overline{{\\operatorname{t}{\\mathbf{Z}}\\lambda}} $$\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n$$ \\scriptstyle{\\frac{\\Gamma^{*}\\Gamma_{1}^{\\prime}}{\\Delta\\Lambda_{1}^{2 $$\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n$$ \\frac{e\\mathbf{c}\\tau^{2}}{\\Delta\\mathbf{k}_{1}^{2}\\mathbf{k}_{1}^{2}} $$\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nU_{\\pi}\\!\\left(S\\right)\n\n\n\n[image]\n\n\n\nD\\Bigl({\\bf S}_{t+1}\\vert{\\bf S}_{t}\\Bigr)\\underline\\bf-\\ D\\Bigl({\\bf S}_{t+1}\\vert{\\bf S}_{1},\\,{\\bf S}_{2},\\,\\cdot\\,\\cdot\\,,\\,{\\bf\\ldots}\\dots\\,,\\,{\\bf S}_{t}\\Bigr)\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nSleep\n\n\n\nOL\n\nv(s) for y =0\n\n\n\n$$ \\begin{array}{r l}{\\left(\\sum_{a\\cdot(x,a)}\\right)}\\\\ {\\langle\\sum_{a^{\\prime}=a}^{\\prime}\\left(\\sum_{a^{\\prime}=a}^{a}{\\frac{\\displaystyle(a^{\\prime}-b^{\\prime})}{\\displaystyle(\\sum_{a^{\\prime}}^{a}+b}\\right)}}\\\\ {\\langle\\sum_{\\lambda^{\\prime}=a}^{a}{\\frac{\\displaystyle(a^{\\prime}-b^{\\prime})}{\\displaystyle(\\sum_{a^{\\prime}}^{a}+b}\\right)}^{\\mathrm{c}^{\\prime}\\frac{\\displaystyle(a^{\\prime})}{\\displaystyle\\sum_{i^{\\prime}=i}^{a}\\displaystyle\\sum_{i^{\\prime}=i}^{\\prime}\\Big)\\end{array} $$\n\n\n\nv(s) for y =1\n\n\n\n4.3 =-2 + 0.6*10 + 0.4*0.8\n\n\n\n[image]\n\n\n\nP_{s,s^{\\prime}}^{a}\\longrightarrow P{\\big(}S_{t+1}\\underline1\\underline1\\underline\\Lambda\\underline-\\underlines,\\underlined_{t}\\underline-\\underlines\\underline-\\underlined\\big)\n\n\n\n[image]\n\n\n\n$$ \\left.\\frac{\\langle\\mid}{\\langle\\mid}{\\pi}\\right{array}\\mid\\longrightarrow\\left.\\right|\\frac{\\mid}{\\mid}\\right.\\leq\\left.\\frac{1}{\\mid}\\right.\\leq\\mid\\left.\\frac{\\mid}{\\mid}\\right.\\leq $$\n\n\n\n$$ \\begin{array}{c}{{\\mathrm{Deterripiriztic~Policy\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\end{array} $$\n\n\n\n$$ \\begin{array}{c}{{\\operatorname{Stochastic}\\operatorname{Policy\\\\ {{m(\\partial|S)=P[A_{t}={\\mathcal{B}}_{t}={\\mathcal{S}}]}}\\end{array} $$\n\n\n\n[image]\n\n\n\n$$ \\begin{array}{c}{{\\mathrm{Deterripiriztic~Policy\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\end{array} $$\n\n\n\n$$ \\begin{array}{c}{{\\mathrm{Stochastic}\\;\\mathrm{Policy\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\end{array} $$\n\n\n\nFacebook\nR=-l\n\nFacebook\n\nQuit\nR= R=-l\n\n=0\nStudy\nR=+10\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nq_{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a\\right]\n\n\n\nFacebook va(s) for 2(a|s)=0.5, y =1\n\nR=-l\n(23) *\n\nQuit Facebook\n\nR=0 R=-l\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nv_{\\pi}(s)\\stackrel{v_{\\pi}(s)\\longleftrightarrow s}{\\sqrt{\\sum}}\\left.\\begin{array}{c}{{}}\\\\ {{}}\\\\ {{}}\\\\ {{}}\\\\ {{v_{\\pi}(s,a)\\notin+a}}\\end{array}\\right.\n\n\n\n$$ \\begin{array}{c}{{q_{\\pi}(s,a)\\stackrel{\\leftarrow}{\\leftarrow}s,a_{\\sigma},a_{\\gamma}\\displaystyle\\langle\\left.\\nabla_{\\mu}\\right.^{\\infty\\\\ {{\\qquad\\left.r\\right.}}\\\\ {{\\qquad\\qquad\\left.q_{\\pi}(s^{\\prime})\\sqrt{-1}\\left.\\nabla\\right.}}\\\\ {{\\qquad\\qquad\\left.q_{\\pi}(s^{\\prime})\\right.\\left.\\left.v_{\\pi}(s^{\\prime})\\right.}}\\end{array} $$\n\n\n\nvr(s) = )_ (als) [R: +7 3) PS a)\n\nacA s/ES\n\n\n\n$$ \\begin{array}{c}{{\\left.\\phantom{\\frac{1}{\\mu(s)+s(s)+s(s)}}\\right.\\left.\\phantom{\\frac{1}{\\mu(s)+s(s)}}\\right.\\phantom{\\frac{\\lambda(s)}{\\lambda}}\\right.}}\\\\ {{\\left.\\phantom{\\frac{\\prime}{\\mu(s,s)+s(s)}}\\prod_{s=s}^{\\nu}\\overbrace{\\left.\\gamma(s)}\\sum_{\\mathcal{C(s)}}\\sum_{\\mathcal{B}_{s,s}}\\sum_{\\mathcal{B},\\left[s\\right]\\left.\\phantom{\\frac{1}{\\lambda(s,s)}}\\right.\\left|\\phantom{\\frac{1}{\\lambda_{s\\right.\\right.\\end{array} $$\n\n\n\nFacebook 7.4 =0.5 * (1 + 0.2* -1.3 + 0.4 * 2.7 + 0.4 * 7.4)\nR=-l +05 \"10\n\nFacebook\nR=-l\n\nQuit\nR=0\n\n\n\nFacebook 7.4 =0.5 * (1 + 0.2* -1.3 + 0.4 * 2.7 + 0.4 * 7.4)\nR=-l +05 \"10\n\nFacebook\nR=-l\n\nQuit\nR=0\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nFacebook vx(s) for y =1\nR=+]\n\n\n\nFacebook q+(s,a) for y =1\n\nR=-1\nqx =5\n0\nQuit Facebook\nR=0 R=-l\nqx =6 qu =5\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n$$ \\ _{\\frac{\\beta}{\\Lambda_{\\mathrm{B} $$\n\n\n\n[image]\n",
    "19Thales.pdf": "1 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \nğ´ğµâ€²\nğ´ğµ    =    ğ´ğ¶â€²\nğ´ğ¶    =    ğµâ€²ğ¶â€²\nğµğ¶ \nTHÃ‰ORÃˆME DE THALÃˆS \n Tout le cours en vidÃ©o : https://youtu.be/puuHhlf0jAQ \n \n \nThalÃ¨s serait nÃ© autour de 625 avant J.C. Ã  Milet en Asie Mineure (actuelle Turquie). ConsidÃ©rÃ© comme \nl'un des sept sages de l'AntiquitÃ©, il est Ã  la fois mathÃ©maticien, ingÃ©nieur, philosophe et homme d'Etat \nmais son domaine de prÃ©dilection est l'astronomie.  \nIl aurait prÃ©dit avec une grande prÃ©cision l'Ã©clipse du soleil du 28 mai de l'an - 585. Ce n'est peut-Ãªtre \nqu'une lÃ©gende, ThalÃ¨s en explique cependant le phÃ©nomÃ¨ne. \nCurieusement, le fameux thÃ©orÃ¨me de ThalÃ¨s n'a pas Ã©tÃ© dÃ©couvert par ThalÃ¨s. Il Ã©tait dÃ©jÃ  connu \navant lui des babyloniens et ne fut dÃ©montrÃ© qu'aprÃ¨s lui par Euclide d'Alexandrie. \n \n \n \n \nPartie 1 : Le thÃ©orÃ¨me de ThalÃ¨s Â« version triangles emboÃ®tÃ©s Â» (Rappel) \n \nAnimation : http://www.maths-et-tiques.fr/telech/Thales4.ggb \n \nLE THÃ‰ORÃˆME DE THALÃˆS \n \nSoit deux triangles ğ´ğµğ¶ et ğ´ğµâ€™ğ¶â€™, tels que : \nğ´, ğµ, ğµâ€™ et ğ´, ğ¶, ğ¶â€™ sont alignÃ©s. \n \nSi (ğµâ€™ğ¶â€™)//(ğµğ¶) \n \nalors :  \n!\"!\n!\" = \n!#!\n!# =\n\"!#!\n\"#  \n \n  \n \nComment retenir le thÃ©orÃ¨me de ThalÃ¨s ? \n \nğ´ğµğ¶ et ğ´ğµâ€™ğ¶â€™ sont deux triangles en situation de ThalÃ¨s : ils ont un sommet commun ğ´, et deux cÃ´tÃ©s \nparallÃ¨les (ğµâ€™ğ¶â€™) et (ğµğ¶). \nUn triangle est un Â« agrandissement Â» de lâ€™autre. Ils ont donc des cÃ´tÃ©s deux Ã  deux proportionnels.  \nOn obtient la formule de ThalÃ¨s : \n \n        Le petit triangle ğ´ğµâ€™ğ¶â€™ \n \n        Le grand triangle ğ´ğµğ¶ \n \n \n \n \n       \n           1ers cÃ´tÃ©s             2Ã¨mes cÃ´tÃ©s              3Ã¨mes cÃ´tÃ©s \n \n \nSavoir utiliser : http://www.maths-et-tiques.fr/telech/thales_ecrire.pdf \n \n \n \n\n2 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \nMÃ©thode : Calculer une longueur Ã  lâ€™aide du thÃ©orÃ¨me de ThalÃ¨s  \n \n VidÃ©o https://youtu.be/zP16D2Zrv1A \n \nSur la figure ci-dessous, les triangles ğµğ¶ğ¹ et ğµğ·ğ¸ sont tels que (ğ¶ğ¹) et (ğ·ğ¸) sont parallÃ¨les.  \nCalculer : a) ğµğ¸    b) ğµğ· \nDonner la valeur exacte et Ã©ventuellement lâ€™arrondi au dixiÃ¨me. \n \n \n \n \n \n \n \n \n \n \n \n \nCorrection \n \na) Les triangles ğµğ¶ğ¹ et ğµğ·ğ¸ sont en situation de ThalÃ¨s car (ğ¶ğ¹) // (ğ·ğ¸), donc : \n \n                   ğµğ¶\nğµğ·= ğµğ¹\nğµğ¸= ğ¶ğ¹\nğ·ğ¸ \n \n                   4\nğµğ·= 4,5\nğµğ¸= 3\n7 \n \n                               4,5\nğµğ¸= 3\n7 \nSoit : ğµğ¸ =  4,5 Ã— 7 âˆ¶3 = 10,5   \n \nb) On a :  \n$\n\"% =\n$,'\n\"( =\n)\n*  \n               \n                               4\nğµğ·= 3\n7 \nSoit : ğµğ·= 4 Ã— 7 : 3 = \n+,\n)  (Valeur exacte) \n                                       Â» 9,3 (Valeur arrondie) \n \n \n \n \n \n \n \nE \nD \nC \nB \nF \n7 \n3 \n4,5 \n4 \nÃ— \n: \n\n3 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \nPartie 2 : Le thÃ©orÃ¨me de ThalÃ¨s Â« version papillon Â» \n \nAnimation : http://www.maths-et-tiques.fr/telech/Thales.ggb \n \n \n \nLE THÃ‰ORÃˆME DE THALÃˆS \n \nSoit deux triangles ğ´ğµğ¶ et ğ´ğµâ€™ğ¶â€™, tels que : \nğ´, ğµ, ğµâ€™ et ğ´, ğ¶, ğ¶â€™ sont alignÃ©s. \n \nSi (ğµâ€™ğ¶â€™)//(ğµğ¶) \n \nalors :  \n!\"!\n!\" = \n!#!\n!# =\n\"!#!\n\"#  \n \n \n \n \n \n \n \n \nMÃ©thode : Calculer une longueur Ã  lâ€™aide du thÃ©orÃ¨me de ThalÃ¨s  \n \n VidÃ©o https://youtu.be/cq3wBbXYB4A  \n \nLes triangles ğµğ´ğ¸ et ğµğ·ğ¶ sont tels que les droites  \n(ğ´ğ¸) et (ğ¶ğ·) sont parallÃ¨les. \nOn donne : ğµğ¸= 2 ğ‘ğ‘š, ğµğ·= 5 ğ‘ğ‘š, et ğ¶ğ·= 6 ğ‘ğ‘š. \nCalculer ğ´ğ¸.  \n \n \nCorrection \nLes triangles ğµğ´ğ¸ et ğµğ·ğ¶ sont en situation de ThalÃ¨s car (ğ´ğ¸) et (ğ¶ğ·) sont parallÃ¨les, donc : \n \n     ğµğ´\nğµğ¶= ğµğ¸\nğµğ·= ğ´ğ¸\nğ¶ğ· \n \n     ğµğ´\nğµğ¶= 2\n5 = ğ´ğ¸\n6  \n   \n          2\n5 = ğ´ğ¸\n6  \n \nEt donc ğ´ğ¸= 6 Ã— 2 : 5 = 2,4 ğ‘ğ‘š.  \n \nActivitÃ©s de groupe : Le paradoxe de Lewis Carroll \nhttp://www.maths-et-tiques.fr/telech/L_CARROLL.pdf \n \nCâ€™ \nBâ€™ \nA \nB \nC \nE \nD \nC \n \nB \nA \n\n4 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \n \n \nDes hauteurs inaccessibles \nhttp://www.maths-et-tiques.fr/telech/haut_inacc.pdf \nhttp://www.maths-et-tiques.fr/index.php/expositions-deleves/hauteurs-inaccessibles \n \n \n \nPartie 3 : La rÃ©ciproque du thÃ©orÃ¨me de ThalÃ¨s \n \nAnimation : http://www.maths-et-tiques.fr/telech/RThales.ggb \n \n \nLA RÃ‰CIPROQUE DU THÃ‰ORÃˆME DE THALÃˆS \n \n \n     Si les points ğ´, ğµ, ğµâ€™ sont alignÃ©s dans  \n     le mÃªme ordre que les points ğ´, ğ¶, ğ¶â€™  \n     et \n!\"!\n!\" =\n!#!\n!#  \n                      \n                        alors (ğµâ€™ğ¶â€™)//(ğµğ¶) \n         \n       ThalÃ¨s de Milet (-624 ; -546) \n \nVersion Â« triangles emboitÃ©s Â» \n \n \nVersion Â« papillon Â» \n \n \n \n \n \n \n \n \n \n \n \n \n \nMÃ©thode : DÃ©montrer que deux droites sont parallÃ¨les  \n VidÃ©o https://youtu.be/uaPicwUSQz0 \n \nSur la figure ci-contre, les points ğ´, ğ¶, ğ¸ sont alignÃ©s et les  \npoints ğµ, ğ¶, ğ· sont Ã©galement alignÃ©s dans le mÃªme ordre. \nLes droites (ğ´ğµ) et (ğ·ğ¸) sont-elles parallÃ¨les ?  \n \n \n \n \nA \nBâ€™ \nB \nCâ€™ \nC \nCâ€™ \nBâ€™ \nA \nB \nC \nB \nC \n \nD \nE \nA \n3 \n4,5 \n6 \n4 \n\n5 \nYvan Monka â€“ AcadÃ©mie de Strasbourg â€“ www.maths-et-tiques.fr \nCorrection \nâ— Dâ€™une part : \n#!\n#( =\n)\n$ = 0,75 \nâ— Dâ€™autre part : \n#\"\n#% =\n$,'\n- = 0,75 \n \nDonc : \n#!\n#( =\n#\"\n#% \nDe plus les points ğ´, ğ¶, ğ¸ sont alignÃ©s dans le mÃªme ordre que les points ğµ, ğ¶, ğ·. \nDâ€™aprÃ¨s la rÃ©ciproque du thÃ©orÃ¨me de ThalÃ¨s, on peut conclure que les droites (ğ´ğµ) et (ğ·ğ¸) \nsont parallÃ¨les. \n \nMÃ©thode : DÃ©montrer que deux droites ne sont pas parallÃ¨les \n \n VidÃ©o https://youtu.be/ovlhagzONlw \n \n                                         \n                                                            Les droites (ğ‘ƒğ‘…) et (ğ·ğ¸) sont-elles parallÃ¨les ? \n \n \n                                          \n \n \n \nCorrection \n \nâ€¢ Dâ€™une part : \n#.\n#% =\n$\n- â‰ˆ0,67 \nâ€¢ Dâ€™autre part : \n#/\n#( =\n+,'\n$ = 0,625 \nDonc :  \n#.\n#% â‰ \n#/\n#(\n \nOn ne peut pas utiliser la rÃ©ciproque du thÃ©orÃ¨me de ThalÃ¨s. \n(ğ‘ƒğ‘…) et (ğ·ğ¸) ne sont pas parallÃ¨les. \n \n \n \nLors dâ€™un voyage en Egypte, ThalÃ¨s de Milet (-624 ; -546) aurait mesurÃ© la hauteur de la pyramide de \nKheops par un rapport de proportionnalitÃ© avec son ombre. \nCitons : Â« Le rapport que jâ€™entretiens avec mon ombre est le mÃªme que celui que la pyramide entretient \navec la sienne. Â» \nPar une relation de proportionnalitÃ©, il obtient la hauteur de la pyramide grÃ¢ce Ã  la longueur de son \nombre. \nL'idÃ©e ingÃ©nieuse de ThalÃ¨s est la suivante : Â« A lâ€™instant oÃ¹ mon ombre sera Ã©gale Ã  ma taille, l'ombre de \nla pyramide sera Ã©gale Ã  sa hauteur. Â» \n \n \nHors du cadre de la classe, aucune reproduction, mÃªme partielle, autres que celles prÃ©vues Ã  l'article L 122-5 du code de \nla propriÃ©tÃ© intellectuelle, ne peut Ãªtre faite de ce site sans l'autorisation expresse de l'auteur. \nwww.maths-et-tiques.fr/index.php/mentions-legales \n \n\n\n\nâ€œA\n\n\n\n^{\\mathrm{s}}\n\n\n\n$$ \\frac{\\partial\\Phi^{3}}{\\partial t} $$\n\n\n\n^{\\mathrm{s}}\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n^{\\mathrm{s}}\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n^{\\mathrm{s}}\n\n\n\n$$ \\frac{\\partial\\Phi^{3}}{\\partial t} $$\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n^{\\mathrm{s}}\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nPOUR NE PAS CONFONDRE :\n\nReciproque du\ntheoreme de Thales\n\n4 4\n\nDemontrer que\nCalculer des longueurs des droites\nsont paralleles\n\nTheoreme de Thales\n",
    "h1-2-les-regimes-totalitaires.pdf": "H1-2 LES REGIMES TOTALITAIRES  \nA LA RECHERCHE Dâ€™UNE NOUVELLE GEOPOLITIQUE EUROPEENNE \n \nIntroduction : La PremiÃ¨re Guerre mondiale, par la brutalisation des sociÃ©tÃ©s, est un bouleversement profond pour \nles pays europÃ©ens. Si en Russie, les rÃ©volutions aboutissent Ã  la mise en place progressive dâ€™un rÃ©gime communiste, \nla PremiÃ¨re Guerre mondiale fragilise les dÃ©mocraties. Par ses traitÃ©s, elle fait naÃ®tre en Allemagne et en Italie des \nmouvements politiques antidÃ©mocratiques violents qui prospÃ¨rent sur les difficultÃ©s Ã©conomiques et la montÃ©e du \ncommunisme. Des rÃ©gimes que lâ€™on qualifiera plus tard de totalitaires se mettent en place en Russie, en Italie et en \nAllemagne. \nProblÃ©matique : Pourquoi qualifier les rÃ©gimes russe, allemand et italien de totalitaires et comment ont-ils fait \nbasculer lâ€™Europe et le monde dans la guerre ?  \nI. \nDes racines et des pratiques communes. \nA. \nDes rÃ©gimes issus de la brutalisation des sociÃ©tÃ©s. \nLâ€™impact de la Grande Guerre : Le rÃ©gime communiste qui sâ€™installe progressivement en Russie est nÃ© dans le contexte \nde la PremiÃ¨re Guerre mondiale, avec le soulÃ¨vement de la population et des mutineries qui aboutissent Ã  la chute du \nTsar Nicolas II, remplacÃ© par un gouvernement provisoire qui est Ã  son tour renversÃ© par un coup dâ€™Ã©tat construit par \nles Bolcheviks, mouvement minoritaire communiste dirigÃ© par LÃ©nine (octobre 1917). En Italie, Benito Mussolini prend \nles rÃªnes du PNF, parti national fasciste dans un pays qui se sent trahi par la Â« victoire mutilÃ©e Â» et rÃ©clame les terres \nirredente quâ€™elle nâ€™a pas obtenu Ã  lâ€™issue des traitÃ©s de paix. Alors que les grÃ¨ves se multiplient, Mussolini sâ€™appuie sur \nles Squadre ou chemises noires (groupe paramilitaire majoritairement composÃ© de soldats dÃ©mobilisÃ©s qui mÃ¨nent \ndes opÃ©rations violentes) pour crÃ©er un climat de violence favorable Ã  sa nomination Ã  la tÃªte de lâ€™Italie (marche sur \nRome en octobre 1922). En Allemagne, la RÃ©publique de Weimar, nÃ©e en 1918, est considÃ©rÃ©e par les partis politiques \ndâ€™extrÃªme droite comme responsable de la dÃ©faite et de lâ€™humiliation de Versailles. \nLe poids de la crise Ã©conomique : Lâ€™autre racine commune Ã  ces rÃ©gimes totalitaires est la misÃ¨re. En Russie, la \nPremiÃ¨re Guerre mondiale, puis la guerre civile qui oppose les communistes aux Â« Blancs Â» entre 1917 et 1922, crÃ©Ã© \nles conditions nÃ©cessaires Ã  lâ€™Ã©tablissement dâ€™un rÃ©gime violent dans un pays dÃ©jÃ  marquÃ© par une grande misÃ¨re. La \ncrise Ã©conomique trÃ¨s grave que connaÃ®t lâ€™Italie aprÃ¨s la PremiÃ¨re Guerre mondiale est aussi un facteur expliquant \nlâ€™arrivÃ©e au pouvoir de Mussolini. Mais câ€™est en Allemagne que le poids de la crise Ã©conomique est le plus fort dans la \nmise en place du rÃ©gime totalitaire. Si le NSDAP, le parti nazi est prÃ©sent dÃ¨s les annÃ©es 20 en Allemagne, il reste \nmarginal et ne connaÃ®t pas de succÃ¨s politiques majeurs avant lâ€™arrivÃ©e de la crise Ã©conomique de 1929 en Allemagne \nqui crÃ©Ã© les conditions favorables Ã  la nomination dâ€™Hitler comme chancelier en janvier 1933, dans un contexte de \nviolence extrÃªme organisÃ©e par les SA (sturmabteilung, organisation paramilitaire issue du parti nazi) et de lutte avec \nleurs principaux opposants, le parti communiste allemand. \nB. \nUne sociÃ©tÃ© contrÃ´lÃ©e. \nUn culte du chef organisÃ© par une propagande puissante : Dans les rÃ©gimes totalitaires, la place du chef est centrale \net un vÃ©ritable culte se met en place autour de ces guides, de ces leaders quâ€™il faut Ã©couter et suivre aveuglÃ©ment. De \nmaniÃ¨re diffÃ©rente, ils occupent chacun lâ€™espace mÃ©diatique grÃ¢ce Ã  une propagande savamment orchestrÃ©e par des \nhommes comme Joseph Goebbels, le ministre allemand de lâ€™information et de la Propagande. Le Duce, le Vojd ou le \nFÃ¼hrer sont omniprÃ©sents, ne se trompent jamais et se sacrifient pour leur nation, invitant les populations Ã  faire de \nmÃªme. De grandes manifestations sportives ou politiques sont organisÃ©es, mettant en scÃ¨ne cette adhÃ©sion populaire \ncomme Ã  Nuremberg ou bien encore lors des festivitÃ©s de la rÃ©volution dâ€™octobre sur la Place Rouge Ã  Moscou. \nDes sociÃ©tÃ©s encadrÃ©es : Dans les Ã©tats totalitaires, lâ€™individu doit sâ€™effacer au nom de la construction dâ€™une sociÃ©tÃ© \nnouvelle, dâ€™un homme nouveau. Chaque moment de la vie sociale est encadrÃ© par une organisation issue du parti \nunique : les organisations de jeunesse (Jeunesses hitlÃ©riennes, Balilla et Avant-gardistes, Pionniers et Komsomols), le \n\ntravail (Front du travail, soviet), la vie sociale et politique au sein du parti qui devient un ascenseur social (PCUS, parti \nfasciste, parti nazi). En Allemagne, lâ€™organisation Kraft durch Freude (la Force par la joie) organise les temps libres et \nles vacances des travailleurs.  Lâ€™ensemble de la vie politique est politisÃ© et les rÃ©sistances restent faibles, malgrÃ© \ncertaines tentatives au sein des milieux catholiques en Italie et en Allemagne et orthodoxes en URSS. \n \nC. \nUne Ã©conomie dirigÃ©e. \nLe dirigisme dâ€™Ã©tat allemand et italien : En Italie, Mussolini lance des programmes ambitieux pour faire de son pays \nune grande puissance industrielle et agricole autour de lâ€™assÃ¨chement des terres insalubres. Avec le contrecoup de la \ncrise Ã©conomique, le dirigisme italien sâ€™accentue. Lâ€™IRI (institut pour la reconstruction industrielle) est crÃ©Ã© en 1933 et \ncontrÃ´le une large part de lâ€™industrie italienne. En Allemagne, le mÃªme schÃ©ma de politiques de grands travaux se met \nen place, mise en lumiÃ¨re par une propagande intensive. Lâ€™autre chemin choisi par lâ€™Allemagne et lâ€™Italie est celui de \nlâ€™autarcie, câ€™est-Ã -dire dâ€™une Ã©conomie fermÃ©e. La bataille de lâ€™emploi nâ€™est gagnÃ©e en Allemagne que dans la cadre du \nrÃ©armement (en 1939, les 2/3e du revenu national sont consacrÃ©s au rÃ©armement) et au prix dâ€™une baisse importante \ndu pouvoir dâ€™achat et dâ€™un pillage industriel des pays annexÃ©s. Mais le recul du chÃ´mage explique en partie lâ€™adhÃ©sion \nde la population aux rÃ©gimes totalitaires. \nLa politique Ã©conomique de Staline : En 1929, Staline lance son pays dans une grande rÃ©forme Ã©conomique : il sâ€™agit \nde faire de lâ€™URSS une grande puissance industrielle. Pour y arriver, lâ€™Ã‰tat nationalise lâ€™ensemble de lâ€™Ã©conomie et fixe \ndes objectifs de production (planification) Ã  lâ€™industrie lourde. Dans un pays largement agricole, lâ€™agriculture est \nsacrifiÃ©e pour financer lâ€™industrialisation forcÃ©e. La propriÃ©tÃ© privÃ©e est supprimÃ©e et les terres sont regroupÃ©es dans \ndes fermes collectives dâ€™Ã‰tat (sovkhozes). La dÃ©sorganisation totale du monde agricole entraÃ®ne des famines, \nattribuÃ©es aux koulaks (paysans opposÃ©s Ã  la collectivisation) mais le rÃ©gime glorifie les nouveaux hÃ©ros comme \nStakhanov, mineur qui aurait produit 14 fois plus que les objectifs. \nII. \nDes idÃ©ologies diffÃ©rentes qui lÃ©gitiment la violence. \nA. \nLe socialisme soviÃ©tique. \nLe socialisme soviÃ©tique : Le parti bolchevik qui prend le pouvoir Ã  partir de 1917 sous la tutelle de LÃ©nine sâ€™appuie \nsur le communisme. Il sâ€™agit dâ€™une idÃ©ologie qui vise Ã  la crÃ©ation dâ€™une sociÃ©tÃ© Ã©galitaire sans classe. Pour y parvenir, \nles ouvriers (les prolÃ©taires) doivent faire la rÃ©volution et imposer des rÃ©formes : câ€™est la dictature du prolÃ©tariat. Pour \nles soviÃ©tiques, le communisme doit sâ€™Ã©tablir dans le monde entier et lâ€™URSS doit aider Ã  la mise en place dâ€™une \nrÃ©volution mondiale. Câ€™est le rÃ´le du Komintern, organisation internationale communiste dont le siÃ¨ge est Ã  Moscou.  \nStaline, qui sâ€™empare progressivement du pouvoir en 1927, pousse Ã  la naissance de lâ€™Homme nouveau. \nLa Terreur stalinienne au cÅ“ur de lâ€™idÃ©ologie : Tous ceux qui sâ€™opposent au pouvoir sont considÃ©rÃ©s comme des \nennemis de la classe ouvriÃ¨re. Le pouvoir soviÃ©tique met en place dÃ¨s 1918 des premiers camps dans lesquels il sâ€™agit \nde Â« rÃ©Ã©duquer Â» par le travail. Ce sont en fait des camps de travail forcÃ© administrÃ© par le Goulag. Staline, qui arrive \nau pouvoir en Ã©liminant ses opposants, sâ€™appuie sur la police politique, le NKVD, pour traquer ceux quâ€™il appelle Â« les \nennemis de lâ€™intÃ©rieur Â», en fait tous ceux qui sâ€™opposent, mÃªme et surtout au sein du Parti Communiste. Entre 1936 \net 1938, la Grande Terreur sâ€™abat sur lâ€™URSS. La violence dâ€™Ã©tat devient systÃ©matique, des objectifs sont fixÃ©s dans les \nprovinces. Entre 1,5 et 2 millions de personnes sont arrÃªtÃ©es, condamnÃ©es Ã  mort (750 000) ou envoyÃ©es dans des \ncamps. Les principaux opposants Ã  Staline sont jugÃ©s en public pendant les ProcÃ¨s de Moscou, et sont gÃ©nÃ©ralement \ncondamnÃ©s Ã  la peine de mort.  \nB. \nLe fascisme italien. \nDÃ©finir le fascisme : Avec les lois fascistissimes de 1925-1926, Mussolini met en place les outils nÃ©cessaires Ã  \nlâ€™encadrement de la sociÃ©tÃ©. Le fascisme se base sur une double rÃ©fÃ©rence au passÃ© glorieux de lâ€™Italie (Lâ€™Empire \nromain) et sur la volontÃ© de construire un Ã©tat moderne autour de son chef. Il sâ€™appuie donc sur un fort nationalisme \n\net un rejet de la dÃ©mocratie et du communisme. La culture de guerre, visible dans lâ€™encadrement militaire de la sociÃ©tÃ©, \ntout comme dans la propagande aprÃ¨s la guerre en Ã‰thiopie, est un Ã©lÃ©ment central du fascisme. Le fascisme intÃ¨gre \nde maniÃ¨re incomplÃ¨te un racisme dâ€™Ã©tat, affirmant la supÃ©rioritÃ© du peuple italien. En 1938, une sÃ©rie de lois \nantijuives complÃ¨tent la dÃ©finition du fascisme.  \nUne violence dâ€™Ã©tat antidÃ©mocratique : Si le nombre de victimes du rÃ©gime de Mussolini nâ€™est pas comparable avec \nlâ€™Allemagne et lâ€™URSS, lâ€™Italie est un Ã©tat policier dans lequel les opposants politiques sont systÃ©matiquement \npourchassÃ©s. AprÃ¨s lâ€™assassinat de lâ€™opposant Matteotti (1924) et les lois fascistissimes, Mussolini utilise la violence \npour asseoir son autoritÃ©. Lâ€™OVRA, police politique mise en place en 1927, traque les opposants politiques, notamment \nles communistes. ArrÃªtÃ©s, ils sont jugÃ©s et condamnÃ©s Ã  mort ou Ã  la dÃ©portation dans les Ã®les Lipari. \nC. \nLe nazisme. \nLe racisme comme base idÃ©ologique : DÃ©veloppÃ©e par Hitler dans son ouvrage Mein Kampf, rÃ©digÃ© en 1924-1925, le \nnazisme se dÃ©finit comme une rÃ©volution sociale qui doit permettre Ã  la race aryenne de conserver sa supÃ©rioritÃ©. Il \nentend donc lutter contre tout ce qui pourrait affaiblir la race aryenne (handicapÃ©s, homosexuelsâ€¦). Pour Hitler, les \nJuifs sont les principaux responsables de lâ€™affaiblissement et de la dÃ©faite de 1918 (Â« coup de poignard dans le dos Â»), \nassociÃ©s aux communistes. Lâ€™antisÃ©mitisme est donc un fondement central du nazisme et les Juifs sont la cible du \nnazisme : boycott des magasins juifs, Lois de Nuremberg (1935) les privant de la nationalitÃ© allemande et interdisant \nles mariages entre Juifs et citoyens allemands, ou bien encore les mesures de 1938 leur interdisant dâ€™exercer un \nnombre importants de mÃ©tiers. \nLa violence comme outil idÃ©ologique : Hitler confisque le pouvoir grÃ¢ce Ã  la violence (incendie du Reichstag en fÃ©vrier \n1933) et fait du NSDAP le seul parti autorisÃ©. Devenu ReichsfÃ¼hrer en 1934 Ã  la mort du PrÃ©sident Hindenburg, Hitler \nmet en place de nombreux outils pour orchestrer la violence nazie : la Gestapo, police politique, arrÃªte les opposants \ntandis que les SA puis les SS sont chargÃ©s de lâ€™application violente des mesures nazies. Ainsi, utilisant comme prÃ©texte \nun attentat Ã  Paris contre un reprÃ©sentant nazi, les dirigeants nazis appellent les Allemands Ã  se venger. Câ€™est le dÃ©but \ndu pogrom (mot russe signifiant la persÃ©cution des Juifs) orchestrÃ© par les SS. Dans la nuit du 9 au 10 novembre 1938, \nappelÃ©e la Nuit de Cristal, les magasins juifs sont dÃ©truits, les synagogues incendiÃ©es et les Juifs sont dÃ©portÃ©s dans les \ncamps de concentration dont certains ont ouvert dÃ¨s 1933. \nIII. \nLe rÃªve dâ€™un nouvel ordre europÃ©en. \nA. \nLe culte de la guerre. \nLa guerre au centre des idÃ©ologies : Dans les rÃ©gimes totalitaires, le fonctionnement de base de la sociÃ©tÃ© est celui de \nlâ€™embrigadement câ€™est-Ã -dire de la militarisation des organisations civiles : le port de lâ€™uniforme dans les groupes \nparamilitaires (SS, squadre) et les organisations de jeunesse en tÃ©moignent. Dans les rÃ©gimes fasciste et nazi, la guerre \nest un but en soi, un projet de sociÃ©tÃ© qui permettra de crÃ©er une nouvelle sociÃ©tÃ©. En URSS, la guerre est dâ€™abord \ncelle contre les ennemis intÃ©rieurs. Mais elle est aussi glorifiÃ©e par le sacrifice pour assurer la victoire du communisme, \ncomme lors de la guerre civile entre 1917 et 1922 durant laquelle les forces europÃ©ennes aidÃ¨rent les troupes \nblanches. La guerre est donc pour lâ€™URSS nÃ©cessaire comme moyen de survie. \nLe refus de lâ€™ordre international : Lâ€™un des enjeux centraux du rÃ©gime nazi est de venger lâ€™affront de Versailles en \nredonnant Ã  lâ€™Allemagne une place centrale en Europe. Il faut donc, par la guerre, se venger de la France et du Royaume \nUni. Mais il faut surtout pour Hitler donner Ã  lâ€™Allemagne Â« un espace vital Â» suffisant pour sa population par une \ncolonisation de lâ€™Europe de lâ€™Est et donc la disparition de la Pologne, de la Russieâ€¦Lâ€™Italie a une position plus ambigÃ¼e \nvis-Ã -vis de lâ€™ordre international. Si elle quitte la SDN, elle cherche tout de mÃªme Ã  rester proche de la France et du \nRoyaume Uni. Mais, devant les rÃ©ticences occidentales, lâ€™Italie opÃ¨re un rapprochement avec lâ€™Allemagne pour former \nlâ€™Axe Rome-Berlin (1936), le nationalisme italien devant nÃ©cessairement passer par des annexions et des guerres. \nLâ€™URSS prÃ´ne la rÃ©volution communiste mondiale et le renversement des rÃ©gimes bourgeois. Ses relations avec les \n\nautres puissances mondiales sont marquÃ©es par un forte dÃ©fiance mÃªme si elle se rapproche de la France (adhÃ©sion Ã  \nla SDN en 1934, accords avec la France). \nB. \nLâ€™Espagne, lieu dâ€™affrontement des totalitarismes \nUn pays dÃ©chirÃ© par une guerre civile : Les Ã©lections de fÃ©vrier 1936 voient la victoire en Espagne dâ€™un Front Populaire \nregroupant les forces de gauche, dont les communistes (comme en France). Une partie de lâ€™armÃ©e, basÃ©e au Maroc \nespagnol, se soulÃ¨ve sous la direction du GÃ©nÃ©ral Franco qui prend la tÃªte des nationalistes contre les RÃ©publicains. La \nguerre dure trois ans et se termine par la victoire des nationalistes et la fuite en France des RÃ©publicains espagnols \n(600 000 morts). \n Un lieu dâ€™affrontement idÃ©ologique : Si malgrÃ© la demande espagnole, la France de LÃ©on Blum refuse dâ€™intervenir \ndans le conflit et essaie dâ€™imposer un embargo sur les armes, les rÃ©gimes totalitaires sâ€™engagent dans le conflit. Lâ€™URSS, \npar le biais du Komintern, organise et arme les Brigades Internationales qui rÃ©unit les volontaires du monde entier qui \nsâ€™engagent aux cÃ´tÃ©s des RÃ©publicains espagnols. De leur cÃ´tÃ©, les rÃ©gimes allemand et italien envoient des troupes et \ndu matÃ©riel pour aider les nationalistes de Franco, crÃ©ant une alliance idÃ©ologique. Ils vont profiter de ce conflit pour \nexpÃ©rimenter leurs troupes et leur matÃ©riel comme lors du bombardement du village de Guernica par lâ€™aviation \nallemande (1937) ou celui de Barcelone par lâ€™aviation italienne (1938). \nC. \nLa marche vers la guerre. \nLa faiblesse des dÃ©mocraties europÃ©ennes : lâ€™exemple espagnol montre quâ€™en Angleterre et en France, les \ngouvernements et les opinions publiques, traumatisÃ©es par la PremiÃ¨re Guerre mondiale, sont trÃ¨s largement \npacifistes et en faveur de politiques dâ€™apaisement et de renoncement. DÃ¨s 1936, face Ã  la rÃ©occupation militaire de la \nRhÃ©nanie, interdite par le TraitÃ© de Versailles, lâ€™absence de rÃ©ponse ferme franco-britannique illustre cette faiblesse. \nLorsque Hitler revendique les SudÃ¨tes, une rÃ©gion de la TchÃ©coslovaquie en 1938, Chamberlain (Royaume Uni) et \nDaladier (France) prÃ©fÃ¨rent sacrifier une alliÃ© militaire pour Ã©viter de faire basculer lâ€™Europe dans la guerre. Câ€™est \nlâ€™esprit de Munich (nom de la ville oÃ¹ a eu lieu la confÃ©rence). MÃªme si la France a conscience que la guerre est proche, \nle Royaume Uni espÃ¨re encore une confÃ©rence sur la paix. \nLes coups de force nazis et italiens : Profitant de lâ€™absence de rÃ©action des grandes puissances mondiales, Hitler, aprÃ¨s \navoir rÃ©armÃ© son pays, se lance dans une politique dâ€™expansion. En mars 1938, il rÃ©alise lâ€™annexion de lâ€™Autriche \nlâ€™Anschluss au nom de la Â« Grande Allemagne Â» qui doit rÃ©unir tous les populations germanophones sous lâ€™autoritÃ© \nnazie. En Septembre 1938, les SudÃ¨tes, province majoritairement germanophone en TchÃ©coslovaquie sont annexÃ©es \npar lâ€™Allemagne avec lâ€™accord de la France et du Royaume Uni (confÃ©rence de Munich). En mars 1939, Hitler sâ€™empare \ndu reste de la TchÃ©coslovaquie et commence Ã  revendiquer une partie de la Pologne. La France et le Royaume Uni \ncomprennent que la guerre est inÃ©vitable. Lorsque lâ€™Allemagne, aprÃ¨s avoir signÃ© un pacte avec lâ€™URSS se partageant \nla Pologne, se lance dans la guerre le 1e septembre 1939, les puissances occidentales se lancent Ã  contre cÅ“ur dans la \nSeconde Guerre mondiale. \nConclusion : Lâ€™URSS, lâ€™Italie fasciste et lâ€™Allemagne nazie sont donc des rÃ©gimes nÃ©s de la brutalisation des sociÃ©tÃ©s Ã  \nlâ€™issue des deux Ã©vÃ¨nements du premier XXe siÃ¨cle : la PremiÃ¨re Guerre mondiale et la crise de 1929. Sâ€™appuyant sur \nun parti unique et rejetant les fondements dÃ©mocratiques, ces trois rÃ©gimes ont mis en place des mÃ©thodes de \ngouvernement communes pour atteindre des objectifs idÃ©ologiques diffÃ©rents : une dictature du prolÃ©tariat en URSS, \nun nationalisme guerrier en Italie et un nationalisme raciste et expansionniste en Allemagne. Ces trois pays ont eu en \ncommun de vouloir mettre en place une nouvelle gÃ©opolitique dans lâ€™Europe des annÃ©es 30, avec, comme moyen \nultime dâ€™y parvenir la guerre face Ã  des dÃ©mocraties traumatisÃ©es par les sacrifices de la Grande Guerre. \n"
}