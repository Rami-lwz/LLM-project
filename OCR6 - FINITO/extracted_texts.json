{
    "1 - Introduction to RL and MDPs.pdf": "Reinforcement Learning\nAn Introduction\n\nInstructor\nHussam ATOUI\n●\nSoftware Engineer at Valeo \nCréteil (France) since Nov 2022\n●\nPhD-Cifre RENAULT & \nGrenoble-Alpes University \n(2019-2022)\n●\nSpecialities: Automated Driving, \nReinforcement Learning, \nAutomatic Control, Optimization\n\nInstructor\nVictor MORAND\nmorand@isir.upmc.fr\n●\nPhD Student at ISIR (Sorbonne - \nCNRS)\n○\nExplaining how LLMs manipulate \nKnowledge\n○\ntowards AIs that know what they \nknow\n●\nAlso out of a School of Engineering !\nFeel free to reach out at the end of our \nsessions ! \n\nCourse Content\n1.\nIntroduction to Reinforcement Learning\n2.\nMarkov Decision Processes (MDPs)\n3.\nPolicy and Value Functions\n4.\nDynamic Programming (DP) for RL\n5.\nModel-Free methods\n6.\nValue Function Approximation\n7.\nPolicy-Gradient and Actor-Critic Methods\n8.\nDeep RL\n9.\nTP Project\n\nIntroduction to Reinforcement \nLearning (RL)\n\nSupervised Learning\nReinforcement Learning\nUnsupervised Learning\nTrain with labeled data\nTrain with unlabeled data\nClustering\nTrain with environment \nexperience\nRegression\nClassiﬁcation\n6\nTypes of Learning\n\nSupervised Learning \nModel\nInputs:\nFeatures / States\nPredicted Outputs:\nValue/ Class\nTraining target:\nTarget Output\n●\nError: Target Output - Predicted Output\n●\nObjective: Minimize the error between the target and the predicted output\n7\nSupervised Learning\n\nSupervised Learning\n\nReinforcement Learning\nReinforcement \nLearning Model\nInputs:\nFeatures / States\nPredicted Outputs:\nActions\nEvaluation:\nRewards / Penalties\n●\nError: Awards - Penalties \n●\nObjective: Maximize the awards and decrease penalties as much as possible\n9\n\nReinforcement Learning\n\nExamples of Rewards [1]\n●\nFly stunt manoeuvres in a helicopter\n○\n+ve reward for following desired trajectory\n○\n−ve reward for crashing\n●\nManage an investment portfolio\n○\n+ve reward for each $ in bank\n●\nControl a power station\n○\n+ve reward for producing power\n○\n−ve reward for exceeding safety thresholds\n●\nMake a humanoid robot walk\n○\n+ve reward for forward motion\n○\n−ve reward for falling over\n●\nPlay many different Atari games better than humans\n○\n+/−ve reward for increasing/decreasing score\n11\n\nAgent and Environment\nActions   A(t)\nObservations   O(t) \nRewards   R(t) \nAgent\nEnvironment\nAt step t: \nThe Agent:\n●\nReceives O(t)\n●\nReceives R(t)\n●\nExecutes A(t)\nThe Environment:\n●\nReceives A(t)\n●\nEmits O(t+1)\n●\nEmits R(t+1)\nt++\n12\n\nFully Observable Environment\nObservations   O(t) \nAgent\nEnvironment\n●\nEnvironment observations = Agent \nstate\n●\nThis is assumed in Markov Decision \nProcess (MDP)\n13\n\nPartially Observable Environment\nObservations   O(t) \nAgent\nEnvironment\n●\nEnvironment observations ≠ Agent state\n○\nA drone navigating a forest only sees \nnearby obstacles.\n○\nA healthcare agent observes patient \nsymptoms but not the underlying \ndisease.\n○\nA self-driving car detects nearby \nvehicles but not hidden pedestrians.\n○\nA weather forecasting model \nobserves recent conditions but not \nfuture patterns\n●\nThis is called Partially Observable Markov \nDecision Process (POMDP)\n(Missing info)\n14\n\nReinforcement Learning\n15\nAgent: The system that takes \nactions to be trained.\nEnvironment: The external \nsystem with which the agent \ninteracts.\nState: The information \nrequired by the agent to take \nan action. This info is observed \nfrom the environment.\nAction: The decision or \nmove that the agent makes \nat a particular state\nReward: Feedback received \nby the agent to evaluate the \ntaken action under a certain \nstate.\nGeneral Architecture\n\n16\nReinforcement Learning\n\nPolicy\nRL Agent\nA policy deﬁnes the agent’s behavior in the environment. It \nrepresents a mapping from states to actions, for example:\n●\nDeterministic policy:                   , \nwhere the action a is chosen directly based on state s.\n●\nStochastic policy:                                                   ,\nwhere the policy gives the probability of taking action \na given state s.\n17\n\nValue Function\nRL Agent\nA value function \n●\nestimates the expected future reward \n●\nassesses the quality of states, helping to determine the best actions to \ntake. \nFor example, the state value under policy 𝜋 is given by:\nThis equation expresses the expected sum of discounted rewards starting from \nstate 𝑠.\n18\n\nValue Function\nRL Agent\nA value function \n●\nestimates the expected future reward \n●\nassesses the quality of states, helping to determine the best actions to \ntake. \nFor example, the state value under policy 𝜋 is given by:\nThis equation expresses the expected sum of discounted rewards starting from \nstate 𝑠.\n19\n\nValue Function\nRL Agent\n20\nγ∈[0,1]:\n●\nIf γ=0, the agent focuses solely on immediate rewards.\n●\nIf γ=1, future rewards are valued equally to immediate rewards.\n\nModel\nRL Agent\nA model forecasts the environment's next state and expected reward:\n●\n𝑃 represents the probability of the next state given the current state and \naction:\n●\n𝑅 represents the expected immediate reward given the current state and \naction:\n21\n\nStates, Actions, Rewards\nExample: Maze [1]\n●\nStates: Agent’s location\n●\nActions: Right, Left, Up, Down\n●\nRewards: -1 per time-step\n22\n\nPolicy\nExample: Maze [1]\nArrows represent policy π(s) for \neach state s\n23\n\nExample: Maze [1]\nNumbers represent value           of \neach state s\n24\n\nDifferent Types\nRL Agents\n➔\nValue-based:\n◆\nNo Policy\n◆\nValue Function\n➔\nPolicy-based:\n◆\nPolicy\n◆\nNo Value Function\n➔\nActor-Critic:\n◆\nPolicy\n◆\nValue Function\n➔\nModel-free:\n◆\nPolicy and/or Value Function\n◆\nNo Model\n➔\nModel-based:\n◆\nPolicy and/or Value Function\n◆\nModel\n25\n\nMarkov Decision Processes (MDPs)\n\nMarkov Process\n●\nA Markov Process is a memoryless process where the future state depends only \non the current state and not on any past states.\n●\nFormally, a Markov Process is a tuple: M=(S,P)\nWhere:\n●\nS: A ﬁnite set of states.\n●\nP: Transition probabilities between states, deﬁned as:\n27\n\nThe Markov Property\n●\nMarkov property: Future depends only on the present, not past states\n●\nSimpliﬁes state transition modeling\n28\n\n●\nA Markov Reward Process is a Markov Process with added rewards.\n●\nIt is represented as a tuple: MR=(S,P,R,γ)\nWhere:\n●\nR(s): Reward function providing the expected reward at each state s, \n●\nγ: Discount factor, controlling the importance of future rewards.\n29\nMarkov Reward Process\n\nCumulative Reward - Gain\nMarkov Reward Process\n●\nCumulative Reward G(t) : Expected cumulative reward from state s\n30\n●\n(State-)Value Function v(s) : Expected state-value of state s\n\nState-Value Function\nBellman Equation\n●\nThe state-value function can be presented as an immediate reward and future reward \nas follows:\nPROOF?\n31\n\nProof\nBellman Equation\nStochastic Eq.\n?\n32\n\nExample\n\nExample: Student MRP (P, S, R) [1] \n34\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n35\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n36\n\nExercise\n\nDiscount factor effect\nExample: Student MRP (P, S, R) [1]  \n38\n?\n\nExample of Bellman’s equation\nExample: Student MRP (P, S, R) [1]  \n39\n\nMRP → MDP\n(P, S, R) → (P, S, A, R)\n\nMarkov Decision Process (MDP)\n A Markov decision process is a 4-tuple (S, A, P, R):\nNote: A ﬁnite MDP is an MDP with ﬁnite state, action, and reward \nsets. Much of the current theory of reinforcement learning is \nrestricted to ﬁnite MDPs.\n●\nStates (S): Describe environment situations\n●\nActions (A): Choices available to the agent\n●\nRewards (R): Immediate feedback for actions\n●\nTransition Probabilities (P): Likelihood of reaching a \nnew state\n41\n\nState Transitions - Policy\nMarkov Decision Process\n●\nTransition probability: P(s′∣s,a)\n●\nModels probability of moving to s′ from s after action a\n42\n\nReward Function and Policy\nMarkov Decision Process\n●\nReward function R(s,a): Immediate feedback\n●\nPositive rewards encourage actions; negative prevent actions\n●\nDeterministic policy:                   , \nwhere the action a is chosen directly based on state s.\n●\nStochastic policy:                                                   ,\nwhere the policy gives the probability of taking action a given \nstate s.\n43\n\nPolicies\nMarkov Decision Process\n●\nDeterministic policy: \n                  \nwhere the action a is chosen directly based on state s.\n●\nStochastic policy:         \n                                          \nwhere the policy gives the probability of taking action a given state s.\n44\n\nExample: Student MDP (P, S, A, R) [1]  \n45\n\nValue Functions\nMarkov Decision Process\n●\nState-value function : Expected cumulative reward from state s under policy π\n46\n●\nAction-value function: Expected reward of taking action a in state s under policy π\n\nState-Value Function\nMarkov Decision Process\n47\n\nBellman Expectation Equation\n48\n●\nState-value function : Expected cumulative reward from state s under policy π\n●\nAction-value function: Expected reward of taking action a in state s under policy π\n\nBellman Expectation Equation [1]\n49\n\nBellman Expectation Equation [1]\n50\n\nBellman Expectation Equation [1]\n51\n\nBellman Expectation Equation [1]\n52\n\nExercise\n\nExample: Student MDP\n54\n?\n\nExample: Student MDP\n55\n\nState-Value and Action-Value Functions\nBellman Optimality\n●\nThe optimal state-value function\n●\nOptimal action-value function\n56\n\nExercise: Optimal State-Value Function [1]\n57\n\nExercise: Optimal Action-Value Function [1]\n58\n\nFind an Optimal Policy\n●\nAn optimal policy π∗ can be determined by selecting actions that \nmaximize the optimal action-value function q∗(s,a). The optimal policy π∗\n(a∣s) is deﬁned as:\n●\nFor any MDP, there is always a deterministic optimal policy. If q∗(s,a) is \nknown, we can directly derive the optimal policy from it.\n59\n\nExercises\n\nExercise 1: Understanding Policies\nQuestion:\nLet S={s1,s2} be a set of two states and A={a1,a2} be a set of two actions. Suppose a \nstochastic policy π is deﬁned as follows:\n1.\nWhat is the probability of taking action a2  in state s1  under this policy?\n2.\nIf the agent is in state s2 , what is the probability of taking action a1  under this \npolicy?\n61\n\nExercise 1: Understanding Policies\nSolution:\n1.\nThe probability of taking action a2  in state s1  is given directly by π(a2 |s1 )=0.3\n2.\nThe probability of taking action a1  in state s2  is given by π(a1 ∣s2 )=0.4\n62\n\nExercise 2: State-Value Function\nQuestion:\nConsider a simple MDP with two states s1  and s2  and a single action a with the \nfollowing reward structure:\n●\nStarting from s1  and taking action a, the agent moves to s2  with a reward of 5.\n●\nStarting from s2  and taking action a, the agent stays in s2  and receives a \nreward of 3.\nAssuming a discount factor γ=0.9 and a deterministic policy where action a is \nalways taken, compute the value of each state v(s1 ) and v(s2 ).\n63\n\nExercise 2: State-Value Function\nSolution:\nThe Bellman equation for the value of each state s is:\n1.\nFor s2 :\nSolving for v(s2 ) → v(s2)=30\n2.\nFor s1 :\nThus, v(s1)=32 and v(s2)=30.\n64\n\nExercise 3: Action-Value Function\nQuestion:\nUsing the same MDP setup as in Exercise 2, calculate the action-value q(s1 ,a) and \nq(s2 ,a) for each state-action pair.\n65\n\nExercise 3: Action-Value Function\nSolution:\nThe Bellman equation for the action-value function is:\nUsing the state values calculated in Exercise 2:\n66\n\nExercise 4: Bellman Optimality Equation\nQuestion:\nSuppose we have an MDP with three states S={s1,s2,s3} and two actions A={a1,a2}. \nThe reward function and transitions are given below:\n●\nFrom s1  taking a1  leads to s2  with reward 4.\n●\nFrom s1  taking a2  leads to s3  with reward 2.\n●\nFrom s2  taking a1  leads to s3  with reward 5.\n●\nFrom s3  taking a1  or a2  leads back to s3  with reward 3.\nAssuming a discount factor γ=0.9, write the Bellman optimality equation for v∗(s1 ).\n67\n\nExercise 4: Bellman Optimality Equation\n68\nSolution:\nThe Bellman optimality equation for the state-value function is:\nSubstituting the rewards:\nTo solve this, we would need the values of v*(s2) and v*(s3), which can be calculated \nrecursively by applying the Bellman optimality equation to each state.\n\nExercise 4: Bellman Optimality Equation\n69\nSolution:\nThe optimal values for each state are: \n●\nv*(s1) = 32.8\n●\nv*(s2) = 32\n●\nv*(s3) = 30\n\nExercise 5: Optimal Policy Derivation\nQuestion:\nIf the optimal action-value function q∗(s,a) for some state s is given by:\n●\nq∗(s,a1 )=12\n●\nq∗(s,a2 )=10\nWhat is the optimal policy π∗(a∣s)?\n70\n\nExercise 5: Optimal Policy Derivation\nSolution:\nThe optimal policy π∗(a∣s) chooses the action that maximizes q∗(s,a).\nSo:\n \nThus, the optimal policy is to always choose action a1  in state s, since q∗(s,a1 )>q∗\n(s,a2 ).\n71\n\nExploration & Exploitation\n\nExploration vs. Exploitation\n●\nIn RL, the agent faces a dilemma between:\n○\nExploration: Trying new actions to discover valuable \noutcomes. (can be harmful…)\n○\nExploitation: Choosing actions that have yielded high \nrewards in the past.\n73\n●\nGoal: Balance exploration and exploitation to maximize rewards over time.\n●\nChallenge: Too much exploration can delay achieving rewards, while too \nmuch exploitation can lead to suboptimal long-term results.\n\nExploration: Discovering New Opportunities\n●\nExample 1 - A robot navigating a maze:\n○\nThe robot tries unfamiliar paths to locate shorter routes or more valuable \nrewards.\n●\nExample 2 - A recommendation system:\n○\nOccasionally recommends new, lesser-known products to a user to \nlearn their interests.\n●\nBeneﬁt: Exploration can uncover higher rewards that aren’t immediately \nobvious.\n74\n\nExploitation: Leveraging Known Information\n●\nExample 1 - A trading agent:\n○\nSelects stocks it has previously identiﬁed as proﬁtable, prioritizing \nconsistency over discovering new options.\n●\nBeneﬁt: Exploitation capitalizes on known successes, ensuring steady rewards.\n●\nExample 2 - A game-playing AI:\n○\nRepeats a high-reward move (e.g., a chess opening) that has led to victories \nin past games.\n75\n\nAny Questions ? \nDon’t hesitate to contact me\nmorand@isir.pmc.fr\n\nAny Questions ? \nContact us !\nhussam.atoui@valeo.com\n\nReferences\n78\n[1] David Silver, Lectures on Reinforcement Learning, 2015\n[2] Reinforcement Learning and Advanced Deep Learning (Sorbonne) - Olivier \nSigaud\n[3] Sutton, R. S. and Barto, A. G. (2018), Reinforcement Learning: An Introduction \n(Second edition). MIT Press\n \nOlivier Sigaud Youtube Channel\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nState (St)\n\nAction\nReward (Rt) (At)\n\nRien)\nSie) Environment\n\n\n\n{\\mathfrak{v}}^{\\chi^{\\prime}{\\overset{\\underset{\\sim}}\n\n\n\n[image]\n\n\n\nTonya oge care\n\nTHE CORRECT\nANSWER WAS:\nyest!\n\n\n\nFreely inspired from PHDComics.com|\n\nDD You GET YOU GET -10.45...\nYOUR PROJECT\nTO WORK?\n\n\n\nACTION\n\n————*\n\n\n\nACTION\n\n————*\n\n\n\nACTION\n\n————*\n\n\n\nACTION\n\n————*\n\n\n\n[image]\n\n\n\n$$ \\begin{array}{c}{{\\mathrm{Deterripiriztic~Policy\\\\ {{\\mathrm{\\\\ {{\\dot{\\ \\\\ {{\\dot{\\cal A}=\\overline\\cal T(S)}}\\end{array} $$\n\n\n\n$$ \\begin{array}{c}{{\\operatorname{Stochastic}\\operatorname{Policy\\\\ {{m(\\partial|S)=P[A_{t}={\\mathcal{B}}_{t}={\\mathcal{S}}]}}\\end{array} $$\n\n\n\n{\\mathfrak{s o}}_{\\mathfrak{h}}\\mathfrak{g}\\mathfrak{i}\n\n\n\nag= 715%\na4 = 25%\n\n\n\nv_{\\pi}(s)=\\mathbb{R}_{\\pi}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\cdot\\cdot\\cdot\\right]S_{t}=s\\right]\n\n\n\nv_{\\pi}(s)=\\mathbb{R}_{\\pi}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\cdot\\cdot\\cdot\\right]S_{t}=s\\right]\n\n\n\nv_{\\pi}(s)=\\mathbb{R}_{\\pi}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\cdot\\cdot\\cdot\\right]S_{t}=s\\right]\n\n\n\n+ a5 + + +\ny ™\n\ny~\\ rN\nV(So) T>|  IHre\nont. ft. me\n\n\n\nPe = PlSir1 = 8 | Sp = 8, Ap =\n\n\n\nR_{s}^{a}=\\mathbb{E}\\left[R_{t+1}\\mid S_{t}=s,A_{t}=a\\right]\n\n\n\n$$ \\overline{{\\operatorname{t}{\\mathbf{Z}}\\lambda}} $$\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n$$ \\scriptstyle{\\frac{\\Gamma^{*}\\Gamma_{1}^{\\prime}}{\\Delta\\Lambda_{1}^{2 $$\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n$$ \\frac{e\\mathbf{c}\\tau^{2}}{\\Delta\\mathbf{k}_{1}^{2}\\mathbf{k}_{1}^{2}} $$\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nU_{\\pi}\\!\\left(S\\right)\n\n\n\n[image]\n\n\n\nD\\Bigl({\\bf S}_{t+1}\\vert{\\bf S}_{t}\\Bigr)\\underline\\bf-\\ D\\Bigl({\\bf S}_{t+1}\\vert{\\bf S}_{1},\\,{\\bf S}_{2},\\,\\cdot\\,\\cdot\\,,\\,{\\bf\\ldots}\\dots\\,,\\,{\\bf S}_{t}\\Bigr)\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nSleep\n\n\n\nOL\n\nv(s) for y =0\n\n\n\n$$ \\begin{array}{r l}{\\left(\\sum_{a\\cdot(x,a)}\\right)}\\\\ {\\langle\\sum_{a^{\\prime}=a}^{\\prime}\\left(\\sum_{a^{\\prime}=a}^{a}{\\frac{\\displaystyle(a^{\\prime}-b^{\\prime})}{\\displaystyle(\\sum_{a^{\\prime}}^{a}+b}\\right)}}\\\\ {\\langle\\sum_{\\lambda^{\\prime}=a}^{a}{\\frac{\\displaystyle(a^{\\prime}-b^{\\prime})}{\\displaystyle(\\sum_{a^{\\prime}}^{a}+b}\\right)}^{\\mathrm{c}^{\\prime}\\frac{\\displaystyle(a^{\\prime})}{\\displaystyle\\sum_{i^{\\prime}=i}^{a}\\displaystyle\\sum_{i^{\\prime}=i}^{\\prime}\\Big)\\end{array} $$\n\n\n\nv(s) for y =1\n\n\n\n4.3 =-2 + 0.6*10 + 0.4*0.8\n\n\n\n[image]\n\n\n\nP_{s,s^{\\prime}}^{a}\\longrightarrow P{\\big(}S_{t+1}\\underline1\\underline1\\underline\\Lambda\\underline-\\underlines,\\underlined_{t}\\underline-\\underlines\\underline-\\underlined\\big)\n\n\n\n[image]\n\n\n\n$$ \\left.\\frac{\\langle\\mid}{\\langle\\mid}{\\pi}\\right{array}\\mid\\longrightarrow\\left.\\right|\\frac{\\mid}{\\mid}\\right.\\leq\\left.\\frac{1}{\\mid}\\right.\\leq\\mid\\left.\\frac{\\mid}{\\mid}\\right.\\leq $$\n\n\n\n$$ \\begin{array}{c}{{\\mathrm{Deterripiriztic~Policy\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\end{array} $$\n\n\n\n$$ \\begin{array}{c}{{\\operatorname{Stochastic}\\operatorname{Policy\\\\ {{m(\\partial|S)=P[A_{t}={\\mathcal{B}}_{t}={\\mathcal{S}}]}}\\end{array} $$\n\n\n\n[image]\n\n\n\n$$ \\begin{array}{c}{{\\mathrm{Deterripiriztic~Policy\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\\\ {{\\mathrm{~\\end{array} $$\n\n\n\n$$ \\begin{array}{c}{{\\mathrm{Stochastic}\\;\\mathrm{Policy\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\\\ {{\\mathrm{\\end{array} $$\n\n\n\nFacebook\nR=-l\n\nFacebook\n\nQuit\nR= R=-l\n\n=0\nStudy\nR=+10\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nq_{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a\\right]\n\n\n\nFacebook va(s) for 2(a|s)=0.5, y =1\n\nR=-l\n(23) *\n\nQuit Facebook\n\nR=0 R=-l\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nv_{\\pi}(s)\\stackrel{v_{\\pi}(s)\\longleftrightarrow s}{\\sqrt{\\sum}}\\left.\\begin{array}{c}{{}}\\\\ {{}}\\\\ {{}}\\\\ {{}}\\\\ {{v_{\\pi}(s,a)\\notin+a}}\\end{array}\\right.\n\n\n\n$$ \\begin{array}{c}{{q_{\\pi}(s,a)\\stackrel{\\leftarrow}{\\leftarrow}s,a_{\\sigma},a_{\\gamma}\\displaystyle\\langle\\left.\\nabla_{\\mu}\\right.^{\\infty\\\\ {{\\qquad\\left.r\\right.}}\\\\ {{\\qquad\\qquad\\left.q_{\\pi}(s^{\\prime})\\sqrt{-1}\\left.\\nabla\\right.}}\\\\ {{\\qquad\\qquad\\left.q_{\\pi}(s^{\\prime})\\right.\\left.\\left.v_{\\pi}(s^{\\prime})\\right.}}\\end{array} $$\n\n\n\nvr(s) = )_ (als) [R: +7 3) PS a)\n\nacA s/ES\n\n\n\n$$ \\begin{array}{c}{{\\left.\\phantom{\\frac{1}{\\mu(s)+s(s)+s(s)}}\\right.\\left.\\phantom{\\frac{1}{\\mu(s)+s(s)}}\\right.\\phantom{\\frac{\\lambda(s)}{\\lambda}}\\right.}}\\\\ {{\\left.\\phantom{\\frac{\\prime}{\\mu(s,s)+s(s)}}\\prod_{s=s}^{\\nu}\\overbrace{\\left.\\gamma(s)}\\sum_{\\mathcal{C(s)}}\\sum_{\\mathcal{B}_{s,s}}\\sum_{\\mathcal{B},\\left[s\\right]\\left.\\phantom{\\frac{1}{\\lambda(s,s)}}\\right.\\left|\\phantom{\\frac{1}{\\lambda_{s\\right.\\right.\\end{array} $$\n\n\n\nFacebook 7.4 =0.5 * (1 + 0.2* -1.3 + 0.4 * 2.7 + 0.4 * 7.4)\nR=-l +05 \"10\n\nFacebook\nR=-l\n\nQuit\nR=0\n\n\n\nFacebook 7.4 =0.5 * (1 + 0.2* -1.3 + 0.4 * 2.7 + 0.4 * 7.4)\nR=-l +05 \"10\n\nFacebook\nR=-l\n\nQuit\nR=0\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nFacebook vx(s) for y =1\nR=+]\n\n\n\nFacebook q+(s,a) for y =1\n\nR=-1\nqx =5\n0\nQuit Facebook\nR=0 R=-l\nqx =6 qu =5\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n$$ \\ _{\\frac{\\beta}{\\Lambda_{\\mathrm{B} $$\n\n\n\n[image]\n",
    "19Thales.pdf": "1 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \n𝐴𝐵′\n𝐴𝐵    =    𝐴𝐶′\n𝐴𝐶    =    𝐵′𝐶′\n𝐵𝐶 \nTHÉORÈME DE THALÈS \n Tout le cours en vidéo : https://youtu.be/puuHhlf0jAQ \n \n \nThalès serait né autour de 625 avant J.C. à Milet en Asie Mineure (actuelle Turquie). Considéré comme \nl'un des sept sages de l'Antiquité, il est à la fois mathématicien, ingénieur, philosophe et homme d'Etat \nmais son domaine de prédilection est l'astronomie.  \nIl aurait prédit avec une grande précision l'éclipse du soleil du 28 mai de l'an - 585. Ce n'est peut-être \nqu'une légende, Thalès en explique cependant le phénomène. \nCurieusement, le fameux théorème de Thalès n'a pas été découvert par Thalès. Il était déjà connu \navant lui des babyloniens et ne fut démontré qu'après lui par Euclide d'Alexandrie. \n \n \n \n \nPartie 1 : Le théorème de Thalès « version triangles emboîtés » (Rappel) \n \nAnimation : http://www.maths-et-tiques.fr/telech/Thales4.ggb \n \nLE THÉORÈME DE THALÈS \n \nSoit deux triangles 𝐴𝐵𝐶 et 𝐴𝐵’𝐶’, tels que : \n𝐴, 𝐵, 𝐵’ et 𝐴, 𝐶, 𝐶’ sont alignés. \n \nSi (𝐵’𝐶’)//(𝐵𝐶) \n \nalors :  \n!\"!\n!\" = \n!#!\n!# =\n\"!#!\n\"#  \n \n  \n \nComment retenir le théorème de Thalès ? \n \n𝐴𝐵𝐶 et 𝐴𝐵’𝐶’ sont deux triangles en situation de Thalès : ils ont un sommet commun 𝐴, et deux côtés \nparallèles (𝐵’𝐶’) et (𝐵𝐶). \nUn triangle est un « agrandissement » de l’autre. Ils ont donc des côtés deux à deux proportionnels.  \nOn obtient la formule de Thalès : \n \n        Le petit triangle 𝐴𝐵’𝐶’ \n \n        Le grand triangle 𝐴𝐵𝐶 \n \n \n \n \n       \n           1ers côtés             2èmes côtés              3èmes côtés \n \n \nSavoir utiliser : http://www.maths-et-tiques.fr/telech/thales_ecrire.pdf \n \n \n \n\n2 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \nMéthode : Calculer une longueur à l’aide du théorème de Thalès  \n \n Vidéo https://youtu.be/zP16D2Zrv1A \n \nSur la figure ci-dessous, les triangles 𝐵𝐶𝐹 et 𝐵𝐷𝐸 sont tels que (𝐶𝐹) et (𝐷𝐸) sont parallèles.  \nCalculer : a) 𝐵𝐸    b) 𝐵𝐷 \nDonner la valeur exacte et éventuellement l’arrondi au dixième. \n \n \n \n \n \n \n \n \n \n \n \n \nCorrection \n \na) Les triangles 𝐵𝐶𝐹 et 𝐵𝐷𝐸 sont en situation de Thalès car (𝐶𝐹) // (𝐷𝐸), donc : \n \n                   𝐵𝐶\n𝐵𝐷= 𝐵𝐹\n𝐵𝐸= 𝐶𝐹\n𝐷𝐸 \n \n                   4\n𝐵𝐷= 4,5\n𝐵𝐸= 3\n7 \n \n                               4,5\n𝐵𝐸= 3\n7 \nSoit : 𝐵𝐸 =  4,5 × 7 ∶3 = 10,5   \n \nb) On a :  \n$\n\"% =\n$,'\n\"( =\n)\n*  \n               \n                               4\n𝐵𝐷= 3\n7 \nSoit : 𝐵𝐷= 4 × 7 : 3 = \n+,\n)  (Valeur exacte) \n                                       » 9,3 (Valeur arrondie) \n \n \n \n \n \n \n \nE \nD \nC \nB \nF \n7 \n3 \n4,5 \n4 \n× \n: \n\n3 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \nPartie 2 : Le théorème de Thalès « version papillon » \n \nAnimation : http://www.maths-et-tiques.fr/telech/Thales.ggb \n \n \n \nLE THÉORÈME DE THALÈS \n \nSoit deux triangles 𝐴𝐵𝐶 et 𝐴𝐵’𝐶’, tels que : \n𝐴, 𝐵, 𝐵’ et 𝐴, 𝐶, 𝐶’ sont alignés. \n \nSi (𝐵’𝐶’)//(𝐵𝐶) \n \nalors :  \n!\"!\n!\" = \n!#!\n!# =\n\"!#!\n\"#  \n \n \n \n \n \n \n \n \nMéthode : Calculer une longueur à l’aide du théorème de Thalès  \n \n Vidéo https://youtu.be/cq3wBbXYB4A  \n \nLes triangles 𝐵𝐴𝐸 et 𝐵𝐷𝐶 sont tels que les droites  \n(𝐴𝐸) et (𝐶𝐷) sont parallèles. \nOn donne : 𝐵𝐸= 2 𝑐𝑚, 𝐵𝐷= 5 𝑐𝑚, et 𝐶𝐷= 6 𝑐𝑚. \nCalculer 𝐴𝐸.  \n \n \nCorrection \nLes triangles 𝐵𝐴𝐸 et 𝐵𝐷𝐶 sont en situation de Thalès car (𝐴𝐸) et (𝐶𝐷) sont parallèles, donc : \n \n     𝐵𝐴\n𝐵𝐶= 𝐵𝐸\n𝐵𝐷= 𝐴𝐸\n𝐶𝐷 \n \n     𝐵𝐴\n𝐵𝐶= 2\n5 = 𝐴𝐸\n6  \n   \n          2\n5 = 𝐴𝐸\n6  \n \nEt donc 𝐴𝐸= 6 × 2 : 5 = 2,4 𝑐𝑚.  \n \nActivités de groupe : Le paradoxe de Lewis Carroll \nhttp://www.maths-et-tiques.fr/telech/L_CARROLL.pdf \n \nC’ \nB’ \nA \nB \nC \nE \nD \nC \n \nB \nA \n\n4 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \n \n \nDes hauteurs inaccessibles \nhttp://www.maths-et-tiques.fr/telech/haut_inacc.pdf \nhttp://www.maths-et-tiques.fr/index.php/expositions-deleves/hauteurs-inaccessibles \n \n \n \nPartie 3 : La réciproque du théorème de Thalès \n \nAnimation : http://www.maths-et-tiques.fr/telech/RThales.ggb \n \n \nLA RÉCIPROQUE DU THÉORÈME DE THALÈS \n \n \n     Si les points 𝐴, 𝐵, 𝐵’ sont alignés dans  \n     le même ordre que les points 𝐴, 𝐶, 𝐶’  \n     et \n!\"!\n!\" =\n!#!\n!#  \n                      \n                        alors (𝐵’𝐶’)//(𝐵𝐶) \n         \n       Thalès de Milet (-624 ; -546) \n \nVersion « triangles emboités » \n \n \nVersion « papillon » \n \n \n \n \n \n \n \n \n \n \n \n \n \nMéthode : Démontrer que deux droites sont parallèles  \n Vidéo https://youtu.be/uaPicwUSQz0 \n \nSur la figure ci-contre, les points 𝐴, 𝐶, 𝐸 sont alignés et les  \npoints 𝐵, 𝐶, 𝐷 sont également alignés dans le même ordre. \nLes droites (𝐴𝐵) et (𝐷𝐸) sont-elles parallèles ?  \n \n \n \n \nA \nB’ \nB \nC’ \nC \nC’ \nB’ \nA \nB \nC \nB \nC \n \nD \nE \nA \n3 \n4,5 \n6 \n4 \n\n5 \nYvan Monka – Académie de Strasbourg – www.maths-et-tiques.fr \nCorrection \n● D’une part : \n#!\n#( =\n)\n$ = 0,75 \n● D’autre part : \n#\"\n#% =\n$,'\n- = 0,75 \n \nDonc : \n#!\n#( =\n#\"\n#% \nDe plus les points 𝐴, 𝐶, 𝐸 sont alignés dans le même ordre que les points 𝐵, 𝐶, 𝐷. \nD’après la réciproque du théorème de Thalès, on peut conclure que les droites (𝐴𝐵) et (𝐷𝐸) \nsont parallèles. \n \nMéthode : Démontrer que deux droites ne sont pas parallèles \n \n Vidéo https://youtu.be/ovlhagzONlw \n \n                                         \n                                                            Les droites (𝑃𝑅) et (𝐷𝐸) sont-elles parallèles ? \n \n \n                                          \n \n \n \nCorrection \n \n• D’une part : \n#.\n#% =\n$\n- ≈0,67 \n• D’autre part : \n#/\n#( =\n+,'\n$ = 0,625 \nDonc :  \n#.\n#% ≠\n#/\n#(\n \nOn ne peut pas utiliser la réciproque du théorème de Thalès. \n(𝑃𝑅) et (𝐷𝐸) ne sont pas parallèles. \n \n \n \nLors d’un voyage en Egypte, Thalès de Milet (-624 ; -546) aurait mesuré la hauteur de la pyramide de \nKheops par un rapport de proportionnalité avec son ombre. \nCitons : « Le rapport que j’entretiens avec mon ombre est le même que celui que la pyramide entretient \navec la sienne. » \nPar une relation de proportionnalité, il obtient la hauteur de la pyramide grâce à la longueur de son \nombre. \nL'idée ingénieuse de Thalès est la suivante : « A l’instant où mon ombre sera égale à ma taille, l'ombre de \nla pyramide sera égale à sa hauteur. » \n \n \nHors du cadre de la classe, aucune reproduction, même partielle, autres que celles prévues à l'article L 122-5 du code de \nla propriété intellectuelle, ne peut être faite de ce site sans l'autorisation expresse de l'auteur. \nwww.maths-et-tiques.fr/index.php/mentions-legales \n \n\n\n\n“A\n\n\n\n^{\\mathrm{s}}\n\n\n\n$$ \\frac{\\partial\\Phi^{3}}{\\partial t} $$\n\n\n\n^{\\mathrm{s}}\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n^{\\mathrm{s}}\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n^{\\mathrm{s}}\n\n\n\n$$ \\frac{\\partial\\Phi^{3}}{\\partial t} $$\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n^{\\mathrm{s}}\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\n[image]\n\n\n\nPOUR NE PAS CONFONDRE :\n\nReciproque du\ntheoreme de Thales\n\n4 4\n\nDemontrer que\nCalculer des longueurs des droites\nsont paralleles\n\nTheoreme de Thales\n",
    "h1-2-les-regimes-totalitaires.pdf": "H1-2 LES REGIMES TOTALITAIRES  \nA LA RECHERCHE D’UNE NOUVELLE GEOPOLITIQUE EUROPEENNE \n \nIntroduction : La Première Guerre mondiale, par la brutalisation des sociétés, est un bouleversement profond pour \nles pays européens. Si en Russie, les révolutions aboutissent à la mise en place progressive d’un régime communiste, \nla Première Guerre mondiale fragilise les démocraties. Par ses traités, elle fait naître en Allemagne et en Italie des \nmouvements politiques antidémocratiques violents qui prospèrent sur les difficultés économiques et la montée du \ncommunisme. Des régimes que l’on qualifiera plus tard de totalitaires se mettent en place en Russie, en Italie et en \nAllemagne. \nProblématique : Pourquoi qualifier les régimes russe, allemand et italien de totalitaires et comment ont-ils fait \nbasculer l’Europe et le monde dans la guerre ?  \nI. \nDes racines et des pratiques communes. \nA. \nDes régimes issus de la brutalisation des sociétés. \nL’impact de la Grande Guerre : Le régime communiste qui s’installe progressivement en Russie est né dans le contexte \nde la Première Guerre mondiale, avec le soulèvement de la population et des mutineries qui aboutissent à la chute du \nTsar Nicolas II, remplacé par un gouvernement provisoire qui est à son tour renversé par un coup d’état construit par \nles Bolcheviks, mouvement minoritaire communiste dirigé par Lénine (octobre 1917). En Italie, Benito Mussolini prend \nles rênes du PNF, parti national fasciste dans un pays qui se sent trahi par la « victoire mutilée » et réclame les terres \nirredente qu’elle n’a pas obtenu à l’issue des traités de paix. Alors que les grèves se multiplient, Mussolini s’appuie sur \nles Squadre ou chemises noires (groupe paramilitaire majoritairement composé de soldats démobilisés qui mènent \ndes opérations violentes) pour créer un climat de violence favorable à sa nomination à la tête de l’Italie (marche sur \nRome en octobre 1922). En Allemagne, la République de Weimar, née en 1918, est considérée par les partis politiques \nd’extrême droite comme responsable de la défaite et de l’humiliation de Versailles. \nLe poids de la crise économique : L’autre racine commune à ces régimes totalitaires est la misère. En Russie, la \nPremière Guerre mondiale, puis la guerre civile qui oppose les communistes aux « Blancs » entre 1917 et 1922, créé \nles conditions nécessaires à l’établissement d’un régime violent dans un pays déjà marqué par une grande misère. La \ncrise économique très grave que connaît l’Italie après la Première Guerre mondiale est aussi un facteur expliquant \nl’arrivée au pouvoir de Mussolini. Mais c’est en Allemagne que le poids de la crise économique est le plus fort dans la \nmise en place du régime totalitaire. Si le NSDAP, le parti nazi est présent dès les années 20 en Allemagne, il reste \nmarginal et ne connaît pas de succès politiques majeurs avant l’arrivée de la crise économique de 1929 en Allemagne \nqui créé les conditions favorables à la nomination d’Hitler comme chancelier en janvier 1933, dans un contexte de \nviolence extrême organisée par les SA (sturmabteilung, organisation paramilitaire issue du parti nazi) et de lutte avec \nleurs principaux opposants, le parti communiste allemand. \nB. \nUne société contrôlée. \nUn culte du chef organisé par une propagande puissante : Dans les régimes totalitaires, la place du chef est centrale \net un véritable culte se met en place autour de ces guides, de ces leaders qu’il faut écouter et suivre aveuglément. De \nmanière différente, ils occupent chacun l’espace médiatique grâce à une propagande savamment orchestrée par des \nhommes comme Joseph Goebbels, le ministre allemand de l’information et de la Propagande. Le Duce, le Vojd ou le \nFührer sont omniprésents, ne se trompent jamais et se sacrifient pour leur nation, invitant les populations à faire de \nmême. De grandes manifestations sportives ou politiques sont organisées, mettant en scène cette adhésion populaire \ncomme à Nuremberg ou bien encore lors des festivités de la révolution d’octobre sur la Place Rouge à Moscou. \nDes sociétés encadrées : Dans les états totalitaires, l’individu doit s’effacer au nom de la construction d’une société \nnouvelle, d’un homme nouveau. Chaque moment de la vie sociale est encadré par une organisation issue du parti \nunique : les organisations de jeunesse (Jeunesses hitlériennes, Balilla et Avant-gardistes, Pionniers et Komsomols), le \n\ntravail (Front du travail, soviet), la vie sociale et politique au sein du parti qui devient un ascenseur social (PCUS, parti \nfasciste, parti nazi). En Allemagne, l’organisation Kraft durch Freude (la Force par la joie) organise les temps libres et \nles vacances des travailleurs.  L’ensemble de la vie politique est politisé et les résistances restent faibles, malgré \ncertaines tentatives au sein des milieux catholiques en Italie et en Allemagne et orthodoxes en URSS. \n \nC. \nUne économie dirigée. \nLe dirigisme d’état allemand et italien : En Italie, Mussolini lance des programmes ambitieux pour faire de son pays \nune grande puissance industrielle et agricole autour de l’assèchement des terres insalubres. Avec le contrecoup de la \ncrise économique, le dirigisme italien s’accentue. L’IRI (institut pour la reconstruction industrielle) est créé en 1933 et \ncontrôle une large part de l’industrie italienne. En Allemagne, le même schéma de politiques de grands travaux se met \nen place, mise en lumière par une propagande intensive. L’autre chemin choisi par l’Allemagne et l’Italie est celui de \nl’autarcie, c’est-à-dire d’une économie fermée. La bataille de l’emploi n’est gagnée en Allemagne que dans la cadre du \nréarmement (en 1939, les 2/3e du revenu national sont consacrés au réarmement) et au prix d’une baisse importante \ndu pouvoir d’achat et d’un pillage industriel des pays annexés. Mais le recul du chômage explique en partie l’adhésion \nde la population aux régimes totalitaires. \nLa politique économique de Staline : En 1929, Staline lance son pays dans une grande réforme économique : il s’agit \nde faire de l’URSS une grande puissance industrielle. Pour y arriver, l’État nationalise l’ensemble de l’économie et fixe \ndes objectifs de production (planification) à l’industrie lourde. Dans un pays largement agricole, l’agriculture est \nsacrifiée pour financer l’industrialisation forcée. La propriété privée est supprimée et les terres sont regroupées dans \ndes fermes collectives d’État (sovkhozes). La désorganisation totale du monde agricole entraîne des famines, \nattribuées aux koulaks (paysans opposés à la collectivisation) mais le régime glorifie les nouveaux héros comme \nStakhanov, mineur qui aurait produit 14 fois plus que les objectifs. \nII. \nDes idéologies différentes qui légitiment la violence. \nA. \nLe socialisme soviétique. \nLe socialisme soviétique : Le parti bolchevik qui prend le pouvoir à partir de 1917 sous la tutelle de Lénine s’appuie \nsur le communisme. Il s’agit d’une idéologie qui vise à la création d’une société égalitaire sans classe. Pour y parvenir, \nles ouvriers (les prolétaires) doivent faire la révolution et imposer des réformes : c’est la dictature du prolétariat. Pour \nles soviétiques, le communisme doit s’établir dans le monde entier et l’URSS doit aider à la mise en place d’une \nrévolution mondiale. C’est le rôle du Komintern, organisation internationale communiste dont le siège est à Moscou.  \nStaline, qui s’empare progressivement du pouvoir en 1927, pousse à la naissance de l’Homme nouveau. \nLa Terreur stalinienne au cœur de l’idéologie : Tous ceux qui s’opposent au pouvoir sont considérés comme des \nennemis de la classe ouvrière. Le pouvoir soviétique met en place dès 1918 des premiers camps dans lesquels il s’agit \nde « rééduquer » par le travail. Ce sont en fait des camps de travail forcé administré par le Goulag. Staline, qui arrive \nau pouvoir en éliminant ses opposants, s’appuie sur la police politique, le NKVD, pour traquer ceux qu’il appelle « les \nennemis de l’intérieur », en fait tous ceux qui s’opposent, même et surtout au sein du Parti Communiste. Entre 1936 \net 1938, la Grande Terreur s’abat sur l’URSS. La violence d’état devient systématique, des objectifs sont fixés dans les \nprovinces. Entre 1,5 et 2 millions de personnes sont arrêtées, condamnées à mort (750 000) ou envoyées dans des \ncamps. Les principaux opposants à Staline sont jugés en public pendant les Procès de Moscou, et sont généralement \ncondamnés à la peine de mort.  \nB. \nLe fascisme italien. \nDéfinir le fascisme : Avec les lois fascistissimes de 1925-1926, Mussolini met en place les outils nécessaires à \nl’encadrement de la société. Le fascisme se base sur une double référence au passé glorieux de l’Italie (L’Empire \nromain) et sur la volonté de construire un état moderne autour de son chef. Il s’appuie donc sur un fort nationalisme \n\net un rejet de la démocratie et du communisme. La culture de guerre, visible dans l’encadrement militaire de la société, \ntout comme dans la propagande après la guerre en Éthiopie, est un élément central du fascisme. Le fascisme intègre \nde manière incomplète un racisme d’état, affirmant la supériorité du peuple italien. En 1938, une série de lois \nantijuives complètent la définition du fascisme.  \nUne violence d’état antidémocratique : Si le nombre de victimes du régime de Mussolini n’est pas comparable avec \nl’Allemagne et l’URSS, l’Italie est un état policier dans lequel les opposants politiques sont systématiquement \npourchassés. Après l’assassinat de l’opposant Matteotti (1924) et les lois fascistissimes, Mussolini utilise la violence \npour asseoir son autorité. L’OVRA, police politique mise en place en 1927, traque les opposants politiques, notamment \nles communistes. Arrêtés, ils sont jugés et condamnés à mort ou à la déportation dans les îles Lipari. \nC. \nLe nazisme. \nLe racisme comme base idéologique : Développée par Hitler dans son ouvrage Mein Kampf, rédigé en 1924-1925, le \nnazisme se définit comme une révolution sociale qui doit permettre à la race aryenne de conserver sa supériorité. Il \nentend donc lutter contre tout ce qui pourrait affaiblir la race aryenne (handicapés, homosexuels…). Pour Hitler, les \nJuifs sont les principaux responsables de l’affaiblissement et de la défaite de 1918 (« coup de poignard dans le dos »), \nassociés aux communistes. L’antisémitisme est donc un fondement central du nazisme et les Juifs sont la cible du \nnazisme : boycott des magasins juifs, Lois de Nuremberg (1935) les privant de la nationalité allemande et interdisant \nles mariages entre Juifs et citoyens allemands, ou bien encore les mesures de 1938 leur interdisant d’exercer un \nnombre importants de métiers. \nLa violence comme outil idéologique : Hitler confisque le pouvoir grâce à la violence (incendie du Reichstag en février \n1933) et fait du NSDAP le seul parti autorisé. Devenu Reichsführer en 1934 à la mort du Président Hindenburg, Hitler \nmet en place de nombreux outils pour orchestrer la violence nazie : la Gestapo, police politique, arrête les opposants \ntandis que les SA puis les SS sont chargés de l’application violente des mesures nazies. Ainsi, utilisant comme prétexte \nun attentat à Paris contre un représentant nazi, les dirigeants nazis appellent les Allemands à se venger. C’est le début \ndu pogrom (mot russe signifiant la persécution des Juifs) orchestré par les SS. Dans la nuit du 9 au 10 novembre 1938, \nappelée la Nuit de Cristal, les magasins juifs sont détruits, les synagogues incendiées et les Juifs sont déportés dans les \ncamps de concentration dont certains ont ouvert dès 1933. \nIII. \nLe rêve d’un nouvel ordre européen. \nA. \nLe culte de la guerre. \nLa guerre au centre des idéologies : Dans les régimes totalitaires, le fonctionnement de base de la société est celui de \nl’embrigadement c’est-à-dire de la militarisation des organisations civiles : le port de l’uniforme dans les groupes \nparamilitaires (SS, squadre) et les organisations de jeunesse en témoignent. Dans les régimes fasciste et nazi, la guerre \nest un but en soi, un projet de société qui permettra de créer une nouvelle société. En URSS, la guerre est d’abord \ncelle contre les ennemis intérieurs. Mais elle est aussi glorifiée par le sacrifice pour assurer la victoire du communisme, \ncomme lors de la guerre civile entre 1917 et 1922 durant laquelle les forces européennes aidèrent les troupes \nblanches. La guerre est donc pour l’URSS nécessaire comme moyen de survie. \nLe refus de l’ordre international : L’un des enjeux centraux du régime nazi est de venger l’affront de Versailles en \nredonnant à l’Allemagne une place centrale en Europe. Il faut donc, par la guerre, se venger de la France et du Royaume \nUni. Mais il faut surtout pour Hitler donner à l’Allemagne « un espace vital » suffisant pour sa population par une \ncolonisation de l’Europe de l’Est et donc la disparition de la Pologne, de la Russie…L’Italie a une position plus ambigüe \nvis-à-vis de l’ordre international. Si elle quitte la SDN, elle cherche tout de même à rester proche de la France et du \nRoyaume Uni. Mais, devant les réticences occidentales, l’Italie opère un rapprochement avec l’Allemagne pour former \nl’Axe Rome-Berlin (1936), le nationalisme italien devant nécessairement passer par des annexions et des guerres. \nL’URSS prône la révolution communiste mondiale et le renversement des régimes bourgeois. Ses relations avec les \n\nautres puissances mondiales sont marquées par un forte défiance même si elle se rapproche de la France (adhésion à \nla SDN en 1934, accords avec la France). \nB. \nL’Espagne, lieu d’affrontement des totalitarismes \nUn pays déchiré par une guerre civile : Les élections de février 1936 voient la victoire en Espagne d’un Front Populaire \nregroupant les forces de gauche, dont les communistes (comme en France). Une partie de l’armée, basée au Maroc \nespagnol, se soulève sous la direction du Général Franco qui prend la tête des nationalistes contre les Républicains. La \nguerre dure trois ans et se termine par la victoire des nationalistes et la fuite en France des Républicains espagnols \n(600 000 morts). \n Un lieu d’affrontement idéologique : Si malgré la demande espagnole, la France de Léon Blum refuse d’intervenir \ndans le conflit et essaie d’imposer un embargo sur les armes, les régimes totalitaires s’engagent dans le conflit. L’URSS, \npar le biais du Komintern, organise et arme les Brigades Internationales qui réunit les volontaires du monde entier qui \ns’engagent aux côtés des Républicains espagnols. De leur côté, les régimes allemand et italien envoient des troupes et \ndu matériel pour aider les nationalistes de Franco, créant une alliance idéologique. Ils vont profiter de ce conflit pour \nexpérimenter leurs troupes et leur matériel comme lors du bombardement du village de Guernica par l’aviation \nallemande (1937) ou celui de Barcelone par l’aviation italienne (1938). \nC. \nLa marche vers la guerre. \nLa faiblesse des démocraties européennes : l’exemple espagnol montre qu’en Angleterre et en France, les \ngouvernements et les opinions publiques, traumatisées par la Première Guerre mondiale, sont très largement \npacifistes et en faveur de politiques d’apaisement et de renoncement. Dès 1936, face à la réoccupation militaire de la \nRhénanie, interdite par le Traité de Versailles, l’absence de réponse ferme franco-britannique illustre cette faiblesse. \nLorsque Hitler revendique les Sudètes, une région de la Tchécoslovaquie en 1938, Chamberlain (Royaume Uni) et \nDaladier (France) préfèrent sacrifier une allié militaire pour éviter de faire basculer l’Europe dans la guerre. C’est \nl’esprit de Munich (nom de la ville où a eu lieu la conférence). Même si la France a conscience que la guerre est proche, \nle Royaume Uni espère encore une conférence sur la paix. \nLes coups de force nazis et italiens : Profitant de l’absence de réaction des grandes puissances mondiales, Hitler, après \navoir réarmé son pays, se lance dans une politique d’expansion. En mars 1938, il réalise l’annexion de l’Autriche \nl’Anschluss au nom de la « Grande Allemagne » qui doit réunir tous les populations germanophones sous l’autorité \nnazie. En Septembre 1938, les Sudètes, province majoritairement germanophone en Tchécoslovaquie sont annexées \npar l’Allemagne avec l’accord de la France et du Royaume Uni (conférence de Munich). En mars 1939, Hitler s’empare \ndu reste de la Tchécoslovaquie et commence à revendiquer une partie de la Pologne. La France et le Royaume Uni \ncomprennent que la guerre est inévitable. Lorsque l’Allemagne, après avoir signé un pacte avec l’URSS se partageant \nla Pologne, se lance dans la guerre le 1e septembre 1939, les puissances occidentales se lancent à contre cœur dans la \nSeconde Guerre mondiale. \nConclusion : L’URSS, l’Italie fasciste et l’Allemagne nazie sont donc des régimes nés de la brutalisation des sociétés à \nl’issue des deux évènements du premier XXe siècle : la Première Guerre mondiale et la crise de 1929. S’appuyant sur \nun parti unique et rejetant les fondements démocratiques, ces trois régimes ont mis en place des méthodes de \ngouvernement communes pour atteindre des objectifs idéologiques différents : une dictature du prolétariat en URSS, \nun nationalisme guerrier en Italie et un nationalisme raciste et expansionniste en Allemagne. Ces trois pays ont eu en \ncommun de vouloir mettre en place une nouvelle géopolitique dans l’Europe des années 30, avec, comme moyen \nultime d’y parvenir la guerre face à des démocraties traumatisées par les sacrifices de la Grande Guerre. \n"
}