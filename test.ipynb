{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noe/Workspace/LLM-project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/noe/Workspace/LLM-project/.venv/lib/python3.12/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `dict[str, any]` but got `UniformParams` with value `UniformParams(noise_type=... 0.058823529411764705)])` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "[nltk_data] Downloading package punkt_tab to /home/noe/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2411\n"
     ]
    }
   ],
   "source": [
    "from ocr import PDFParser\n",
    "from ocr import OCR\n",
    "from summarizer import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "ocr = OCR()\n",
    "parser = PDFParser(ocr)\n",
    "\n",
    "text = parser.parse_pdf('/home/noe/Workspace/LLM-project/OCR6 - FINITO/pdfs/1 - Introduction to RL and MDPs.pdf')\n",
    "print(len(text.split()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Re', 'in', 'forcement', 'ĠLearning', 'Ċ', 'An', 'ĠIntroduction', 'Ċ', 'Ċ', 'Instruct', 'or', 'Ċ', 'H', 'uss', 'am', 'ĠAT', 'OU', 'I', 'Ċ', 'âĹı', 'Ċ', 'Software', 'ĠEngineer', 'Ġat', 'ĠVale', 'o', 'Ġ', 'Ċ', 'Cr', 'Ã©', 'te', 'il', 'Ġ(', 'France', ')', 'Ġsince', 'ĠNov', 'Ġ2022', 'Ċ', 'âĹı', 'Ċ', 'Ph', 'D', '-', 'C', 'if', 're', 'ĠR', 'ENA', 'ULT', 'Ġ&', 'Ġ', 'Ċ', 'G', 'ren', 'ob', 'le', '-', 'Al', 'pes', 'ĠUniversity', 'Ġ', 'Ċ', '(', '2019', '-', '20', '22', ')', 'Ċ', 'âĹı', 'Ċ', 'Special', 'ities', ':', 'ĠAutom', 'ated', 'ĠDriving', ',', 'Ġ', 'Ċ', 'Re', 'in', 'forcement', 'ĠLearning', ',', 'Ġ', 'Ċ', 'Aut', 'omatic', 'ĠControl', ',', 'ĠOptim', 'ization', 'Ċ', 'Ċ', 'Instruct', 'or', 'Ċ', 'Vict', 'or', 'ĠMOR', 'AND', 'Ċ', 'mor', 'and', '@', 'is', 'ir', '.', 'up', 'mc', '.', 'fr', 'Ċ', 'âĹı', 'Ċ', 'Ph', 'D', 'ĠStudent', 'Ġat', 'ĠIS', 'IR', 'Ġ(', 'S', 'or', 'bon', 'ne', 'Ġ-', 'Ġ', 'Ċ', 'C', 'NRS', ')', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'Expl', 'aining', 'Ġhow', 'ĠLL', 'Ms', 'Ġmanipulate', 'Ġ', 'Ċ', 'Know', 'ledge', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 't', 'ow', 'ards', 'ĠA', 'Is', 'Ġthat', 'Ġknow', 'Ġwhat', 'Ġthey', 'Ġ', 'Ċ', 'know', 'Ċ', 'âĹı', 'Ċ', 'Also', 'Ġout', 'Ġof', 'Ġa', 'ĠSchool', 'Ġof', 'ĠEngineering', 'Ġ!', 'Ċ', 'Feel', 'Ġfree', 'Ġto', 'Ġreach', 'Ġout', 'Ġat', 'Ġthe', 'Ġend', 'Ġof', 'Ġour', 'Ġ', 'Ċ', 's', 'essions', 'Ġ!', 'Ġ', 'Ċ', 'Ċ', 'Course', 'ĠContent', 'Ċ', '1', '.', 'Ċ', 'Introduction', 'Ġto', 'ĠRein', 'forcement', 'ĠLearning', 'Ċ', '2', '.', 'Ċ', 'Mark', 'ov', 'ĠDecision', 'ĠProcess', 'es', 'Ġ(', 'M', 'DP', 's', ')', 'Ċ', '3', '.', 'Ċ', 'Policy', 'Ġand', 'ĠValue', 'ĠFunctions', 'Ċ', '4', '.', 'Ċ', 'Dynamic', 'ĠProgramming', 'Ġ(', 'DP', ')', 'Ġfor', 'ĠRL', 'Ċ', '5', '.', 'Ċ', 'Model', '-', 'Free', 'Ġmethods', 'Ċ', '6', '.', 'Ċ', 'Value', 'ĠFunction', 'ĠApp', 'rox', 'imation', 'Ċ', '7', '.', 'Ċ', 'Policy', '-', 'Grad', 'ient', 'Ġand', 'ĠActor', '-', 'Crit', 'ic', 'ĠMethods', 'Ċ', '8', '.', 'Ċ', 'Deep', 'ĠRL', 'Ċ', '9', '.', 'Ċ', 'TP', 'ĠProject', 'Ċ', 'Ċ', 'Introduction', 'Ġto', 'ĠRein', 'forcement', 'Ġ', 'Ċ', 'Learning', 'Ġ(', 'RL', ')', 'Ċ', 'Ċ', 'Super', 'vised', 'ĠLearning', 'Ċ', 'Re', 'in', 'forcement', 'ĠLearning', 'Ċ', 'Un', 'super', 'vised', 'ĠLearning', 'Ċ', 'Train', 'Ġwith', 'Ġlabeled', 'Ġdata', 'Ċ', 'Train', 'Ġwith', 'Ġunl', 'abel', 'ed', 'Ġdata', 'Ċ', 'Cl', 'ust', 'ering', 'Ċ', 'Train', 'Ġwith', 'Ġenvironment', 'Ġ', 'Ċ', 'exper', 'ience', 'Ċ', 'Reg', 'ression', 'Ċ', 'Class', 'i', 'ï', '¬', 'ģ', 'cation', 'Ċ', '6', 'Ċ', 'Types', 'Ġof', 'ĠLearning', 'Ċ', 'Ċ', 'Super', 'vised', 'ĠLearning', 'Ġ', 'Ċ', 'Model', 'Ċ', 'Input', 's', ':', 'Ċ', 'Features', 'Ġ/', 'ĠStates', 'Ċ', 'Pred', 'icted', 'ĠOutput', 's', ':', 'Ċ', 'Value', '/', 'ĠClass', 'Ċ', 'Training', 'Ġtarget', ':', 'Ċ', 'Target', 'ĠOutput', 'Ċ', 'âĹı', 'Ċ', 'Error', ':', 'ĠTarget', 'ĠOutput', 'Ġ-', 'ĠPred', 'icted', 'ĠOutput', 'Ċ', 'âĹı', 'Ċ', 'Object', 'ive', ':', 'ĠMin', 'imize', 'Ġthe', 'Ġerror', 'Ġbetween', 'Ġthe', 'Ġtarget', 'Ġand', 'Ġthe', 'Ġpredicted', 'Ġoutput', 'Ċ', '7', 'Ċ', 'Super', 'vised', 'ĠLearning', 'Ċ', 'Ċ', 'Super', 'vised', 'ĠLearning', 'Ċ', 'Ċ', 'Re', 'in', 'forcement', 'ĠLearning', 'Ċ', 'Re', 'in', 'forcement', 'Ġ', 'Ċ', 'Learning', 'ĠModel', 'Ċ', 'Input', 's', ':', 'Ċ', 'Features', 'Ġ/', 'ĠStates', 'Ċ', 'Pred', 'icted', 'ĠOutput', 's', ':', 'Ċ', 'A', 'ctions', 'Ċ', 'E', 'val', 'uation', ':', 'Ċ', 'Rew', 'ards', 'Ġ/', 'ĠPen', 'alties', 'Ċ', 'âĹı', 'Ċ', 'Error', ':', 'ĠAwards', 'Ġ-', 'ĠPen', 'alties', 'Ġ', 'Ċ', 'âĹı', 'Ċ', 'Object', 'ive', ':', 'ĠMaxim', 'ize', 'Ġthe', 'Ġawards', 'Ġand', 'Ġdecrease', 'Ġpenalties', 'Ġas', 'Ġmuch', 'Ġas', 'Ġpossible', 'Ċ', '9', 'Ċ', 'Ċ', 'Re', 'in', 'forcement', 'ĠLearning', 'Ċ', 'Ċ', 'Examples', 'Ġof', 'ĠRewards', 'Ġ[', '1', ']', 'Ċ', 'âĹı', 'Ċ', 'Fly', 'Ġstunt', 'Ġmanoeuv', 'res', 'Ġin', 'Ġa', 'Ġhelicopter', 'Ċ', 'âĹ', 'ĭ', 'Ċ', '+', 've', 'Ġreward', 'Ġfor', 'Ġfollowing', 'Ġdesired', 'Ġtrajectory', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'âĪĴ', 've', 'Ġreward', 'Ġfor', 'Ġcrashing', 'Ċ', 'âĹı', 'Ċ', 'Man', 'age', 'Ġan', 'Ġinvestment', 'Ġportfolio', 'Ċ', 'âĹ', 'ĭ', 'Ċ', '+', 've', 'Ġreward', 'Ġfor', 'Ġeach', 'Ġ$', 'Ġin', 'Ġbank', 'Ċ', 'âĹı', 'Ċ', 'Control', 'Ġa', 'Ġpower', 'Ġstation', 'Ċ', 'âĹ', 'ĭ', 'Ċ', '+', 've', 'Ġreward', 'Ġfor', 'Ġproducing', 'Ġpower', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'âĪĴ', 've', 'Ġreward', 'Ġfor', 'Ġexceeding', 'Ġsafety', 'Ġthresholds', 'Ċ', 'âĹı', 'Ċ', 'Make', 'Ġa', 'Ġhumanoid', 'Ġrobot', 'Ġwalk', 'Ċ', 'âĹ', 'ĭ', 'Ċ', '+', 've', 'Ġreward', 'Ġfor', 'Ġforward', 'Ġmotion', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'âĪĴ', 've', 'Ġreward', 'Ġfor', 'Ġfalling', 'Ġover', 'Ċ', 'âĹı', 'Ċ', 'Play', 'Ġmany', 'Ġdifferent', 'ĠAtari', 'Ġgames', 'Ġbetter', 'Ġthan', 'Ġhumans', 'Ċ', 'âĹ', 'ĭ', 'Ċ', '+', '/', 'âĪĴ', 've', 'Ġreward', 'Ġfor', 'Ġincreasing', '/', 'dec', 're', 'asing', 'Ġscore', 'Ċ', '11', 'Ċ', 'Ċ', 'Agent', 'Ġand', 'ĠEnvironment', 'Ċ', 'A', 'ctions', 'Ġ', 'Ġ', 'ĠA', '(', 't', ')', 'Ċ', 'Obs', 'erv', 'ations', 'Ġ', 'Ġ', 'ĠO', '(', 't', ')', 'Ġ', 'Ċ', 'Rew', 'ards', 'Ġ', 'Ġ', 'ĠR', '(', 't', ')', 'Ġ', 'Ċ', 'Agent', 'Ċ', 'Environment', 'Ċ', 'At', 'Ġstep', 'Ġt', ':', 'Ġ', 'Ċ', 'The', 'ĠAgent', ':', 'Ċ', 'âĹı', 'Ċ', 'Re', 'ce', 'ives', 'ĠO', '(', 't', ')', 'Ċ', 'âĹı', 'Ċ', 'Re', 'ce', 'ives', 'ĠR', '(', 't', ')', 'Ċ', 'âĹı', 'Ċ', 'Exec', 'utes', 'ĠA', '(', 't', ')', 'Ċ', 'The', 'ĠEnvironment', ':', 'Ċ', 'âĹı', 'Ċ', 'Re', 'ce', 'ives', 'ĠA', '(', 't', ')', 'Ċ', 'âĹı', 'Ċ', 'Em', 'its', 'ĠO', '(', 't', '+', '1', ')', 'Ċ', 'âĹı', 'Ċ', 'Em', 'its', 'ĠR', '(', 't', '+', '1', ')', 'Ċ', 't', '++', 'Ċ', '12', 'Ċ', 'Ċ', 'F', 'ully', 'ĠObserv', 'able', 'ĠEnvironment', 'Ċ', 'Obs', 'erv', 'ations', 'Ġ', 'Ġ', 'ĠO', '(', 't', ')', 'Ġ', 'Ċ', 'Agent', 'Ċ', 'Environment', 'Ċ', 'âĹı', 'Ċ', 'Environment', 'Ġobservations', 'Ġ=', 'ĠAgent', 'Ġ', 'Ċ', 'state', 'Ċ', 'âĹı', 'Ċ', 'This', 'Ġis', 'Ġassumed', 'Ġin', 'ĠMark', 'ov', 'ĠDecision', 'Ġ', 'Ċ', 'Process', 'Ġ(', 'M', 'DP', ')', 'Ċ', '13', 'Ċ', 'Ċ', 'Part', 'ially', 'ĠObserv', 'able', 'ĠEnvironment', 'Ċ', 'Obs', 'erv', 'ations', 'Ġ', 'Ġ', 'ĠO', '(', 't', ')', 'Ġ', 'Ċ', 'Agent', 'Ċ', 'Environment', 'Ċ', 'âĹı', 'Ċ', 'Environment', 'Ġobservations', 'Ġâī', 'ł', 'ĠAgent', 'Ġstate', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'A', 'Ġdrone', 'Ġnavigating', 'Ġa', 'Ġforest', 'Ġonly', 'Ġsees', 'Ġ', 'Ċ', 'near', 'by', 'Ġobstacles', '.', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'A', 'Ġhealthcare', 'Ġagent', 'Ġobserves', 'Ġpatient', 'Ġ', 'Ċ', 'sym', 'ptoms', 'Ġbut', 'Ġnot', 'Ġthe', 'Ġunderlying', 'Ġ', 'Ċ', 'd', 'ise', 'ase', '.', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'A', 'Ġself', '-', 'driving', 'Ġcar', 'Ġdetects', 'Ġnearby', 'Ġ', 'Ċ', 'veh', 'icles', 'Ġbut', 'Ġnot', 'Ġhidden', 'Ġpedestrians', '.', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'A', 'Ġweather', 'Ġforecasting', 'Ġmodel', 'Ġ', 'Ċ', 'ob', 'serv', 'es', 'Ġrecent', 'Ġconditions', 'Ġbut', 'Ġnot', 'Ġ', 'Ċ', 'future', 'Ġpatterns', 'Ċ', 'âĹı', 'Ċ', 'This', 'Ġis', 'Ġcalled', 'ĠPart', 'ially', 'ĠObserv', 'able', 'ĠMark', 'ov', 'Ġ', 'Ċ', 'Dec', 'ision', 'ĠProcess', 'Ġ(', 'P', 'OM', 'DP', ')', 'Ċ', '(', 'Missing', 'Ġinfo', ')', 'Ċ', '14', 'Ċ', 'Ċ', 'Re', 'in', 'forcement', 'ĠLearning', 'Ċ', '15', 'Ċ', 'Agent', ':', 'ĠThe', 'Ġsystem', 'Ġthat', 'Ġtakes', 'Ġ', 'Ċ', 'actions', 'Ġto', 'Ġbe', 'Ġtrained', '.', 'Ċ', 'Environment', ':', 'ĠThe', 'Ġexternal', 'Ġ', 'Ċ', 'system', 'Ġwith', 'Ġwhich', 'Ġthe', 'Ġagent', 'Ġ', 'Ċ', 'inter', 'acts', '.', 'Ċ', 'State', ':', 'ĠThe', 'Ġinformation', 'Ġ', 'Ċ', 'required', 'Ġby', 'Ġthe'], ['Ġagent', 'Ġto', 'Ġtake', 'Ġ', 'Ċ', 'an', 'Ġaction', '.', 'ĠThis', 'Ġinfo', 'Ġis', 'Ġobserved', 'Ġ', 'Ċ', 'from', 'Ġthe', 'Ġenvironment', '.', 'Ċ', 'Action', ':', 'ĠThe', 'Ġdecision', 'Ġor', 'Ġ', 'Ċ', 'move', 'Ġthat', 'Ġthe', 'Ġagent', 'Ġmakes', 'Ġ', 'Ċ', 'at', 'Ġa', 'Ġparticular', 'Ġstate', 'Ċ', 'Reward', ':', 'ĠFeedback', 'Ġreceived', 'Ġ', 'Ċ', 'by', 'Ġthe', 'Ġagent', 'Ġto', 'Ġevaluate', 'Ġthe', 'Ġ', 'Ċ', 't', 'aken', 'Ġaction', 'Ġunder', 'Ġa', 'Ġcertain', 'Ġ', 'Ċ', 'state', '.', 'Ċ', 'General', 'ĠArchitecture', 'Ċ', 'Ċ', '16', 'Ċ', 'Re', 'in', 'forcement', 'ĠLearning', 'Ċ', 'Ċ', 'Policy', 'Ċ', 'RL', 'ĠAgent', 'Ċ', 'A', 'Ġpolicy', 'Ġde', 'ï', '¬', 'ģ', 'nes', 'Ġthe', 'Ġagent', 'âĢ', 'Ļ', 's', 'Ġbehavior', 'Ġin', 'Ġthe', 'Ġenvironment', '.', 'ĠIt', 'Ġ', 'Ċ', 'rep', 'resents', 'Ġa', 'Ġmapping', 'Ġfrom', 'Ġstates', 'Ġto', 'Ġactions', ',', 'Ġfor', 'Ġexample', ':', 'Ċ', 'âĹı', 'Ċ', 'D', 'eter', 'ministic', 'Ġpolicy', ':', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ,', 'Ġ', 'Ċ', 'where', 'Ġthe', 'Ġaction', 'Ġa', 'Ġis', 'Ġchosen', 'Ġdirectly', 'Ġbased', 'Ġon', 'Ġstate', 'Ġs', '.', 'Ċ', 'âĹı', 'Ċ', 'St', 'och', 'astic', 'Ġpolicy', ':', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ,', 'Ċ', 'where', 'Ġthe', 'Ġpolicy', 'Ġgives', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġtaking', 'Ġaction', 'Ġ', 'Ċ', 'a', 'Ġgiven', 'Ġstate', 'Ġs', '.', 'Ċ', '17', 'Ċ', 'Ċ', 'Value', 'ĠFunction', 'Ċ', 'RL', 'ĠAgent', 'Ċ', 'A', 'Ġvalue', 'Ġfunction', 'Ġ', 'Ċ', 'âĹı', 'Ċ', 'est', 'imates', 'Ġthe', 'Ġexpected', 'Ġfuture', 'Ġreward', 'Ġ', 'Ċ', 'âĹı', 'Ċ', 'ass', 'esses', 'Ġthe', 'Ġquality', 'Ġof', 'Ġstates', ',', 'Ġhelping', 'Ġto', 'Ġdetermine', 'Ġthe', 'Ġbest', 'Ġactions', 'Ġto', 'Ġ', 'Ċ', 'take', '.', 'Ġ', 'Ċ', 'For', 'Ġexample', ',', 'Ġthe', 'Ġstate', 'Ġvalue', 'Ġunder', 'Ġpolicy', 'Ġ', 'ðĿ', 'ľ', 'ĭ', 'Ġis', 'Ġgiven', 'Ġby', ':', 'Ċ', 'This', 'Ġequation', 'Ġexpresses', 'Ġthe', 'Ġexpected', 'Ġsum', 'Ġof', 'Ġdiscounted', 'Ġrewards', 'Ġstarting', 'Ġfrom', 'Ġ', 'Ċ', 'state', 'Ġ', 'ðĿ', 'ĳ', 'ł', '.', 'Ċ', '18', 'Ċ', 'Ċ', 'Value', 'ĠFunction', 'Ċ', 'RL', 'ĠAgent', 'Ċ', 'A', 'Ġvalue', 'Ġfunction', 'Ġ', 'Ċ', 'âĹı', 'Ċ', 'est', 'imates', 'Ġthe', 'Ġexpected', 'Ġfuture', 'Ġreward', 'Ġ', 'Ċ', 'âĹı', 'Ċ', 'ass', 'esses', 'Ġthe', 'Ġquality', 'Ġof', 'Ġstates', ',', 'Ġhelping', 'Ġto', 'Ġdetermine', 'Ġthe', 'Ġbest', 'Ġactions', 'Ġto', 'Ġ', 'Ċ', 'take', '.', 'Ġ', 'Ċ', 'For', 'Ġexample', ',', 'Ġthe', 'Ġstate', 'Ġvalue', 'Ġunder', 'Ġpolicy', 'Ġ', 'ðĿ', 'ľ', 'ĭ', 'Ġis', 'Ġgiven', 'Ġby', ':', 'Ċ', 'This', 'Ġequation', 'Ġexpresses', 'Ġthe', 'Ġexpected', 'Ġsum', 'Ġof', 'Ġdiscounted', 'Ġrewards', 'Ġstarting', 'Ġfrom', 'Ġ', 'Ċ', 'state', 'Ġ', 'ðĿ', 'ĳ', 'ł', '.', 'Ċ', '19', 'Ċ', 'Ċ', 'Value', 'ĠFunction', 'Ċ', 'RL', 'ĠAgent', 'Ċ', '20', 'Ċ', 'Î³', 'âĪ', 'Ī', '[', '0', ',', '1', ']:', 'Ċ', 'âĹı', 'Ċ', 'If', 'ĠÎ', '³', '=', '0', ',', 'Ġthe', 'Ġagent', 'Ġfocuses', 'Ġsolely', 'Ġon', 'Ġimmediate', 'Ġrewards', '.', 'Ċ', 'âĹı', 'Ċ', 'If', 'ĠÎ', '³', '=', '1', ',', 'Ġfuture', 'Ġrewards', 'Ġare', 'Ġvalued', 'Ġequally', 'Ġto', 'Ġimmediate', 'Ġrewards', '.', 'Ċ', 'Ċ', 'Model', 'Ċ', 'RL', 'ĠAgent', 'Ċ', 'A', 'Ġmodel', 'Ġforecasts', 'Ġthe', 'Ġenvironment', \"'s\", 'Ġnext', 'Ġstate', 'Ġand', 'Ġexpected', 'Ġreward', ':', 'Ċ', 'âĹı', 'Ċ', 'ðĿ', 'ĳ', 'ĥ', 'Ġrepresents', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġthe', 'Ġnext', 'Ġstate', 'Ġgiven', 'Ġthe', 'Ġcurrent', 'Ġstate', 'Ġand', 'Ġ', 'Ċ', 'action', ':', 'Ċ', 'âĹı', 'Ċ', 'ðĿ', 'ĳ', 'ħ', 'Ġrepresents', 'Ġthe', 'Ġexpected', 'Ġimmediate', 'Ġreward', 'Ġgiven', 'Ġthe', 'Ġcurrent', 'Ġstate', 'Ġand', 'Ġ', 'Ċ', 'action', ':', 'Ċ', '21', 'Ċ', 'Ċ', 'States', ',', 'ĠActions', ',', 'ĠRewards', 'Ċ', 'Example', ':', 'ĠMaze', 'Ġ[', '1', ']', 'Ċ', 'âĹı', 'Ċ', 'States', ':', 'ĠAgent', 'âĢ', 'Ļ', 's', 'Ġlocation', 'Ċ', 'âĹı', 'Ċ', 'A', 'ctions', ':', 'ĠRight', ',', 'ĠLeft', ',', 'ĠUp', ',', 'ĠDown', 'Ċ', 'âĹı', 'Ċ', 'Rew', 'ards', ':', 'Ġ-', '1', 'Ġper', 'Ġtime', '-', 'step', 'Ċ', '22', 'Ċ', 'Ċ', 'Policy', 'Ċ', 'Example', ':', 'ĠMaze', 'Ġ[', '1', ']', 'Ċ', 'Ar', 'rows', 'Ġrepresent', 'Ġpolicy', 'ĠÏ', 'Ģ', '(', 's', ')', 'Ġfor', 'Ġ', 'Ċ', 'each', 'Ġstate', 'Ġs', 'Ċ', '23', 'Ċ', 'Ċ', 'Example', ':', 'ĠMaze', 'Ġ[', '1', ']', 'Ċ', 'Numbers', 'Ġrepresent', 'Ġvalue', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġof', 'Ġ', 'Ċ', 'each', 'Ġstate', 'Ġs', 'Ċ', '24', 'Ċ', 'Ċ', 'Different', 'ĠTypes', 'Ċ', 'RL', 'ĠAgents', 'Ċ', 'â', 'ŀ', 'Ķ', 'Ċ', 'Value', '-', 'based', ':', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'No', 'ĠPolicy', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'Value', 'ĠFunction', 'Ċ', 'â', 'ŀ', 'Ķ', 'Ċ', 'Policy', '-', 'based', ':', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'Policy', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'No', 'ĠValue', 'ĠFunction', 'Ċ', 'â', 'ŀ', 'Ķ', 'Ċ', 'Actor', '-', 'Crit', 'ic', ':', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'Policy', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'Value', 'ĠFunction', 'Ċ', 'â', 'ŀ', 'Ķ', 'Ċ', 'Model', '-', 'free', ':', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'Policy', 'Ġand', '/', 'or', 'ĠValue', 'ĠFunction', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'No', 'ĠModel', 'Ċ', 'â', 'ŀ', 'Ķ', 'Ċ', 'Model', '-', 'based', ':', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'Policy', 'Ġand', '/', 'or', 'ĠValue', 'ĠFunction', 'Ċ', 'âĹ', 'Ĩ', 'Ċ', 'Model', 'Ċ', '25', 'Ċ', 'Ċ', 'Mark', 'ov', 'ĠDecision', 'ĠProcess', 'es', 'Ġ(', 'M', 'DP', 's', ')', 'Ċ', 'Ċ', 'Mark', 'ov', 'ĠProcess', 'Ċ', 'âĹı', 'Ċ', 'A', 'ĠMark', 'ov', 'ĠProcess', 'Ġis', 'Ġa', 'Ġmemory', 'less', 'Ġprocess', 'Ġwhere', 'Ġthe', 'Ġfuture', 'Ġstate', 'Ġdepends', 'Ġonly', 'Ġ', 'Ċ', 'on', 'Ġthe', 'Ġcurrent', 'Ġstate', 'Ġand', 'Ġnot', 'Ġon', 'Ġany', 'Ġpast', 'Ġstates', '.', 'Ċ', 'âĹı', 'Ċ', 'Form', 'ally', ',', 'Ġa', 'ĠMark', 'ov', 'ĠProcess', 'Ġis', 'Ġa', 'Ġtuple', ':', 'ĠM', '=(', 'S', ',', 'P', ')', 'Ċ', 'Where', ':', 'Ċ', 'âĹı', 'Ċ', 'S', ':', 'ĠA', 'Ġï', '¬', 'ģ', 'n', 'ite', 'Ġset', 'Ġof', 'Ġstates', '.', 'Ċ', 'âĹı', 'Ċ', 'P', ':', 'ĠTransition', 'Ġprobabilities', 'Ġbetween', 'Ġstates', ',', 'Ġde', 'ï', '¬', 'ģ', 'ned', 'Ġas', ':', 'Ċ', '27', 'Ċ', 'Ċ', 'The', 'ĠMark', 'ov', 'ĠProperty', 'Ċ', 'âĹı', 'Ċ', 'Mark', 'ov', 'Ġproperty', ':', 'ĠFuture', 'Ġdepends', 'Ġonly', 'Ġon', 'Ġthe', 'Ġpresent', ',', 'Ġnot', 'Ġpast', 'Ġstates', 'Ċ', 'âĹı', 'Ċ', 'Sim', 'pl', 'i', 'ï', '¬', 'ģ', 'es', 'Ġstate', 'Ġtransition', 'Ġmodeling', 'Ċ', '28', 'Ċ', 'Ċ', 'âĹı', 'Ċ', 'A', 'ĠMark', 'ov', 'ĠReward', 'ĠProcess', 'Ġis', 'Ġa', 'ĠMark', 'ov', 'ĠProcess', 'Ġwith', 'Ġadded', 'Ġrewards', '.', 'Ċ', 'âĹı', 'Ċ', 'It', 'Ġis', 'Ġrepresented', 'Ġas', 'Ġa', 'Ġtuple', ':', 'ĠMR', '=(', 'S', ',', 'P', ',', 'R', ',', 'Î³', ')', 'Ċ', 'Where', ':', 'Ċ', 'âĹı', 'Ċ', 'R', '(', 's', '):', 'ĠReward', 'Ġfunction', 'Ġproviding', 'Ġthe', 'Ġexpected', 'Ġreward', 'Ġat', 'Ġeach', 'Ġstate', 'Ġs', ',', 'Ġ', 'Ċ', 'âĹı', 'Ċ', 'Î³', ':', 'ĠDiscount', 'Ġfactor', ',', 'Ġcontrolling', 'Ġthe', 'Ġimportance', 'Ġof', 'Ġfuture', 'Ġrewards', '.', 'Ċ', '29', 'Ċ', 'Mark', 'ov', 'ĠReward', 'ĠProcess', 'Ċ', 'Ċ', 'C', 'um', 'ulative', 'ĠReward', 'Ġ-', 'ĠGain', 'Ċ', 'Mark', 'ov', 'ĠReward', 'ĠProcess', 'Ċ', 'âĹı', 'Ċ', 'C', 'um', 'ulative', 'ĠReward', 'ĠG', '(', 't', ')', 'Ġ:', 'ĠEx', 'pected', 'Ġcumulative', 'Ġreward', 'Ġfrom', 'Ġstate', 'Ġs'], ['Ċ', '30', 'Ċ', 'âĹı', 'Ċ', '(', 'State', '-)', 'Value', 'ĠFunction', 'Ġv', '(', 's', ')', 'Ġ:', 'ĠEx', 'pected', 'Ġstate', '-', 'value', 'Ġof', 'Ġstate', 'Ġs', 'Ċ', 'Ċ', 'State', '-', 'Value', 'ĠFunction', 'Ċ', 'Bell', 'man', 'ĠEqu', 'ation', 'Ċ', 'âĹı', 'Ċ', 'The', 'Ġstate', '-', 'value', 'Ġfunction', 'Ġcan', 'Ġbe', 'Ġpresented', 'Ġas', 'Ġan', 'Ġimmediate', 'Ġreward', 'Ġand', 'Ġfuture', 'Ġreward', 'Ġ', 'Ċ', 'as', 'Ġfollows', ':', 'Ċ', 'PR', 'OO', 'F', '?', 'Ċ', '31', 'Ċ', 'Ċ', 'Proof', 'Ċ', 'Bell', 'man', 'ĠEqu', 'ation', 'Ċ', 'St', 'och', 'astic', 'ĠE', 'q', '.', 'Ċ', '?', 'Ċ', '32', 'Ċ', 'Ċ', 'Example', 'Ċ', 'Ċ', 'Example', ':', 'ĠStudent', 'ĠMR', 'P', 'Ġ(', 'P', ',', 'ĠS', ',', 'ĠR', ')', 'Ġ[', '1', ']', 'Ġ', 'Ċ', '34', 'Ċ', 'Ċ', 'Disc', 'ount', 'Ġfactor', 'Ġeffect', 'Ċ', 'Example', ':', 'ĠStudent', 'ĠMR', 'P', 'Ġ(', 'P', ',', 'ĠS', ',', 'ĠR', ')', 'Ġ[', '1', ']', 'Ġ', 'Ġ', 'Ċ', '35', 'Ċ', 'Ċ', 'Disc', 'ount', 'Ġfactor', 'Ġeffect', 'Ċ', 'Example', ':', 'ĠStudent', 'ĠMR', 'P', 'Ġ(', 'P', ',', 'ĠS', ',', 'ĠR', ')', 'Ġ[', '1', ']', 'Ġ', 'Ġ', 'Ċ', '36', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ċ', 'Ċ', 'Disc', 'ount', 'Ġfactor', 'Ġeffect', 'Ċ', 'Example', ':', 'ĠStudent', 'ĠMR', 'P', 'Ġ(', 'P', ',', 'ĠS', ',', 'ĠR', ')', 'Ġ[', '1', ']', 'Ġ', 'Ġ', 'Ċ', '38', 'Ċ', '?', 'Ċ', 'Ċ', 'Example', 'Ġof', 'ĠBell', 'man', 'âĢ', 'Ļ', 's', 'Ġequation', 'Ċ', 'Example', ':', 'ĠStudent', 'ĠMR', 'P', 'Ġ(', 'P', ',', 'ĠS', ',', 'ĠR', ')', 'Ġ[', '1', ']', 'Ġ', 'Ġ', 'Ċ', '39', 'Ċ', 'Ċ', 'MR', 'P', 'ĠâĨĴ', 'ĠM', 'DP', 'Ċ', '(', 'P', ',', 'ĠS', ',', 'ĠR', ')', 'ĠâĨĴ', 'Ġ(', 'P', ',', 'ĠS', ',', 'ĠA', ',', 'ĠR', ')', 'Ċ', 'Ċ', 'Mark', 'ov', 'ĠDecision', 'ĠProcess', 'Ġ(', 'M', 'DP', ')', 'Ċ', 'ĠA', 'ĠMark', 'ov', 'Ġdecision', 'Ġprocess', 'Ġis', 'Ġa', 'Ġ4', '-', 't', 'uple', 'Ġ(', 'S', ',', 'ĠA', ',', 'ĠP', ',', 'ĠR', '):', 'Ċ', 'Note', ':', 'ĠA', 'Ġï', '¬', 'ģ', 'n', 'ite', 'ĠM', 'DP', 'Ġis', 'Ġan', 'ĠM', 'DP', 'Ġwith', 'Ġï', '¬', 'ģ', 'n', 'ite', 'Ġstate', ',', 'Ġaction', ',', 'Ġand', 'Ġreward', 'Ġ', 'Ċ', 'sets', '.', 'ĠMuch', 'Ġof', 'Ġthe', 'Ġcurrent', 'Ġtheory', 'Ġof', 'Ġreinforcement', 'Ġlearning', 'Ġis', 'Ġ', 'Ċ', 'restricted', 'Ġto', 'Ġï', '¬', 'ģ', 'n', 'ite', 'ĠM', 'DP', 's', '.', 'Ċ', 'âĹı', 'Ċ', 'States', 'Ġ(', 'S', '):', 'ĠDesc', 'ribe', 'Ġenvironment', 'Ġsituations', 'Ċ', 'âĹı', 'Ċ', 'A', 'ctions', 'Ġ(', 'A', '):', 'ĠCho', 'ices', 'Ġavailable', 'Ġto', 'Ġthe', 'Ġagent', 'Ċ', 'âĹı', 'Ċ', 'Rew', 'ards', 'Ġ(', 'R', '):', 'ĠIm', 'mediate', 'Ġfeedback', 'Ġfor', 'Ġactions', 'Ċ', 'âĹı', 'Ċ', 'Trans', 'ition', 'ĠProb', 'abilities', 'Ġ(', 'P', '):', 'ĠLike', 'lihood', 'Ġof', 'Ġreaching', 'Ġa', 'Ġ', 'Ċ', 'new', 'Ġstate', 'Ċ', '41', 'Ċ', 'Ċ', 'State', 'ĠTrans', 'itions', 'Ġ-', 'ĠPolicy', 'Ċ', 'Mark', 'ov', 'ĠDecision', 'ĠProcess', 'Ċ', 'âĹı', 'Ċ', 'Trans', 'ition', 'Ġprobability', ':', 'ĠP', '(', 's', 'âĢ²', 'âĪ', '£', 's', ',', 'a', ')', 'Ċ', 'âĹı', 'Ċ', 'Mod', 'els', 'Ġprobability', 'Ġof', 'Ġmoving', 'Ġto', 'Ġs', 'âĢ²', 'Ġfrom', 'Ġs', 'Ġafter', 'Ġaction', 'Ġa', 'Ċ', '42', 'Ċ', 'Ċ', 'Reward', 'ĠFunction', 'Ġand', 'ĠPolicy', 'Ċ', 'Mark', 'ov', 'ĠDecision', 'ĠProcess', 'Ċ', 'âĹı', 'Ċ', 'Reward', 'Ġfunction', 'ĠR', '(', 's', ',', 'a', '):', 'ĠIm', 'mediate', 'Ġfeedback', 'Ċ', 'âĹı', 'Ċ', 'Pos', 'itive', 'Ġrewards', 'Ġencourage', 'Ġactions', ';', 'Ġnegative', 'Ġprevent', 'Ġactions', 'Ċ', 'âĹı', 'Ċ', 'D', 'eter', 'ministic', 'Ġpolicy', ':', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ,', 'Ġ', 'Ċ', 'where', 'Ġthe', 'Ġaction', 'Ġa', 'Ġis', 'Ġchosen', 'Ġdirectly', 'Ġbased', 'Ġon', 'Ġstate', 'Ġs', '.', 'Ċ', 'âĹı', 'Ċ', 'St', 'och', 'astic', 'Ġpolicy', ':', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ,', 'Ċ', 'where', 'Ġthe', 'Ġpolicy', 'Ġgives', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġtaking', 'Ġaction', 'Ġa', 'Ġgiven', 'Ġ', 'Ċ', 'state', 'Ġs', '.', 'Ċ', '43', 'Ċ', 'Ċ', 'P', 'olic', 'ies', 'Ċ', 'Mark', 'ov', 'ĠDecision', 'ĠProcess', 'Ċ', 'âĹı', 'Ċ', 'D', 'eter', 'ministic', 'Ġpolicy', ':', 'Ġ', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ċ', 'where', 'Ġthe', 'Ġaction', 'Ġa', 'Ġis', 'Ġchosen', 'Ġdirectly', 'Ġbased', 'Ġon', 'Ġstate', 'Ġs', '.', 'Ċ', 'âĹı', 'Ċ', 'St', 'och', 'astic', 'Ġpolicy', ':', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ċ', 'where', 'Ġthe', 'Ġpolicy', 'Ġgives', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġtaking', 'Ġaction', 'Ġa', 'Ġgiven', 'Ġstate', 'Ġs', '.', 'Ċ', '44', 'Ċ', 'Ċ', 'Example', ':', 'ĠStudent', 'ĠM', 'DP', 'Ġ(', 'P', ',', 'ĠS', ',', 'ĠA', ',', 'ĠR', ')', 'Ġ[', '1', ']', 'Ġ', 'Ġ', 'Ċ', '45', 'Ċ', 'Ċ', 'Value', 'ĠFunctions', 'Ċ', 'Mark', 'ov', 'ĠDecision', 'ĠProcess', 'Ċ', 'âĹı', 'Ċ', 'State', '-', 'value', 'Ġfunction', 'Ġ:', 'ĠEx', 'pected', 'Ġcumulative', 'Ġreward', 'Ġfrom', 'Ġstate', 'Ġs', 'Ġunder', 'Ġpolicy', 'ĠÏ', 'Ģ', 'Ċ', '46', 'Ċ', 'âĹı', 'Ċ', 'Action', '-', 'value', 'Ġfunction', ':', 'ĠEx', 'pected', 'Ġreward', 'Ġof', 'Ġtaking', 'Ġaction', 'Ġa', 'Ġin', 'Ġstate', 'Ġs', 'Ġunder', 'Ġpolicy', 'ĠÏ', 'Ģ', 'Ċ', 'Ċ', 'State', '-', 'Value', 'ĠFunction', 'Ċ', 'Mark', 'ov', 'ĠDecision', 'ĠProcess', 'Ċ', '47', 'Ċ', 'Ċ', 'Bell', 'man', 'ĠExpect', 'ation', 'ĠEqu', 'ation', 'Ċ', '48', 'Ċ', 'âĹı', 'Ċ', 'State', '-', 'value', 'Ġfunction', 'Ġ:', 'ĠEx', 'pected', 'Ġcumulative', 'Ġreward', 'Ġfrom', 'Ġstate', 'Ġs', 'Ġunder', 'Ġpolicy', 'ĠÏ', 'Ģ', 'Ċ', 'âĹı', 'Ċ', 'Action', '-', 'value', 'Ġfunction', ':', 'ĠEx', 'pected', 'Ġreward', 'Ġof', 'Ġtaking', 'Ġaction', 'Ġa', 'Ġin', 'Ġstate', 'Ġs', 'Ġunder', 'Ġpolicy', 'ĠÏ', 'Ģ', 'Ċ', 'Ċ', 'Bell', 'man', 'ĠExpect', 'ation', 'ĠEqu', 'ation', 'Ġ[', '1', ']', 'Ċ', '49', 'Ċ', 'Ċ', 'Bell', 'man', 'ĠExpect', 'ation', 'ĠEqu', 'ation', 'Ġ[', '1', ']', 'Ċ', '50', 'Ċ', 'Ċ', 'Bell', 'man', 'ĠExpect', 'ation', 'ĠEqu', 'ation', 'Ġ[', '1', ']', 'Ċ', '51', 'Ċ', 'Ċ', 'Bell', 'man', 'ĠExpect', 'ation', 'ĠEqu', 'ation', 'Ġ[', '1', ']', 'Ċ', '52', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ċ', 'Ċ', 'Example', ':', 'ĠStudent', 'ĠM', 'DP', 'Ċ', '54', 'Ċ', '?', 'Ċ', 'Ċ', 'Example', ':', 'ĠStudent', 'ĠM', 'DP', 'Ċ', '55', 'Ċ', 'Ċ', 'State', '-', 'Value', 'Ġand', 'ĠAction', '-', 'Value', 'ĠFunctions', 'Ċ', 'Bell', 'man', 'ĠOptim', 'ality', 'Ċ', 'âĹı', 'Ċ', 'The', 'Ġoptimal', 'Ġstate', '-', 'value', 'Ġfunction', 'Ċ', 'âĹı', 'Ċ', 'Opt', 'imal', 'Ġaction', '-', 'value', 'Ġfunction', 'Ċ', '56', 'Ċ', 'Ċ', 'Ex', 'ercise', ':', 'ĠOpt', 'imal', 'ĠState', '-', 'Value', 'ĠFunction', 'Ġ[', '1', ']', 'Ċ', '57', 'Ċ', 'Ċ', 'Ex', 'ercise', ':', 'ĠOpt', 'imal', 'ĠAction', '-', 'Value', 'ĠFunction', 'Ġ[', '1', ']', 'Ċ'], ['58', 'Ċ', 'Ċ', 'Find', 'Ġan', 'ĠOpt', 'imal', 'ĠPolicy', 'Ċ', 'âĹı', 'Ċ', 'An', 'Ġoptimal', 'Ġpolicy', 'ĠÏ', 'Ģ', 'âĪ', 'Ĺ', 'Ġcan', 'Ġbe', 'Ġdetermined', 'Ġby', 'Ġselecting', 'Ġactions', 'Ġthat', 'Ġ', 'Ċ', 'max', 'imize', 'Ġthe', 'Ġoptimal', 'Ġaction', '-', 'value', 'Ġfunction', 'Ġq', 'âĪ', 'Ĺ', '(', 's', ',', 'a', ').', 'ĠThe', 'Ġoptimal', 'Ġpolicy', 'ĠÏ', 'Ģ', 'âĪ', 'Ĺ', 'Ċ', '(', 'a', 'âĪ', '£', 's', ')', 'Ġis', 'Ġde', 'ï', '¬', 'ģ', 'ned', 'Ġas', ':', 'Ċ', 'âĹı', 'Ċ', 'For', 'Ġany', 'ĠM', 'DP', ',', 'Ġthere', 'Ġis', 'Ġalways', 'Ġa', 'Ġdeter', 'ministic', 'Ġoptimal', 'Ġpolicy', '.', 'ĠIf', 'Ġq', 'âĪ', 'Ĺ', '(', 's', ',', 'a', ')', 'Ġis', 'Ġ', 'Ċ', 'known', ',', 'Ġwe', 'Ġcan', 'Ġdirectly', 'Ġderive', 'Ġthe', 'Ġoptimal', 'Ġpolicy', 'Ġfrom', 'Ġit', '.', 'Ċ', '59', 'Ċ', 'Ċ', 'Ex', 'erc', 'ises', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ1', ':', 'ĠUnderstanding', 'ĠPolicies', 'Ċ', 'Question', ':', 'Ċ', 'Let', 'ĠS', '={', 's', '1', ',', 's', '2', '}', 'Ġbe', 'Ġa', 'Ġset', 'Ġof', 'Ġtwo', 'Ġstates', 'Ġand', 'ĠA', '={', 'a', '1', ',', 'a', '2', '}', 'Ġbe', 'Ġa', 'Ġset', 'Ġof', 'Ġtwo', 'Ġactions', '.', 'ĠSuppose', 'Ġa', 'Ġ', 'Ċ', 'st', 'och', 'astic', 'Ġpolicy', 'ĠÏ', 'Ģ', 'Ġis', 'Ġde', 'ï', '¬', 'ģ', 'ned', 'Ġas', 'Ġfollows', ':', 'Ċ', '1', '.', 'Ċ', 'What', 'Ġis', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġtaking', 'Ġaction', 'Ġa', '2', 'Ġ', 'Ġin', 'Ġstate', 'Ġs', '1', 'Ġ', 'Ġunder', 'Ġthis', 'Ġpolicy', '?', 'Ċ', '2', '.', 'Ċ', 'If', 'Ġthe', 'Ġagent', 'Ġis', 'Ġin', 'Ġstate', 'Ġs', '2', 'Ġ,', 'Ġwhat', 'Ġis', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġtaking', 'Ġaction', 'Ġa', '1', 'Ġ', 'Ġunder', 'Ġthis', 'Ġ', 'Ċ', 'policy', '?', 'Ċ', '61', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ1', ':', 'ĠUnderstanding', 'ĠPolicies', 'Ċ', 'Solution', ':', 'Ċ', '1', '.', 'Ċ', 'The', 'Ġprobability', 'Ġof', 'Ġtaking', 'Ġaction', 'Ġa', '2', 'Ġ', 'Ġin', 'Ġstate', 'Ġs', '1', 'Ġ', 'Ġis', 'Ġgiven', 'Ġdirectly', 'Ġby', 'ĠÏ', 'Ģ', '(', 'a', '2', 'Ġ|', 's', '1', 'Ġ)', '=', '0', '.', '3', 'Ċ', '2', '.', 'Ċ', 'The', 'Ġprobability', 'Ġof', 'Ġtaking', 'Ġaction', 'Ġa', '1', 'Ġ', 'Ġin', 'Ġstate', 'Ġs', '2', 'Ġ', 'Ġis', 'Ġgiven', 'Ġby', 'ĠÏ', 'Ģ', '(', 'a', '1', 'ĠâĪ', '£', 's', '2', 'Ġ)', '=', '0', '.', '4', 'Ċ', '62', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ2', ':', 'ĠState', '-', 'Value', 'ĠFunction', 'Ċ', 'Question', ':', 'Ċ', 'Consider', 'Ġa', 'Ġsimple', 'ĠM', 'DP', 'Ġwith', 'Ġtwo', 'Ġstates', 'Ġs', '1', 'Ġ', 'Ġand', 'Ġs', '2', 'Ġ', 'Ġand', 'Ġa', 'Ġsingle', 'Ġaction', 'Ġa', 'Ġwith', 'Ġthe', 'Ġ', 'Ċ', 'follow', 'ing', 'Ġreward', 'Ġstructure', ':', 'Ċ', 'âĹı', 'Ċ', 'Starting', 'Ġfrom', 'Ġs', '1', 'Ġ', 'Ġand', 'Ġtaking', 'Ġaction', 'Ġa', ',', 'Ġthe', 'Ġagent', 'Ġmoves', 'Ġto', 'Ġs', '2', 'Ġ', 'Ġwith', 'Ġa', 'Ġreward', 'Ġof', 'Ġ5', '.', 'Ċ', 'âĹı', 'Ċ', 'Starting', 'Ġfrom', 'Ġs', '2', 'Ġ', 'Ġand', 'Ġtaking', 'Ġaction', 'Ġa', ',', 'Ġthe', 'Ġagent', 'Ġstays', 'Ġin', 'Ġs', '2', 'Ġ', 'Ġand', 'Ġreceives', 'Ġa', 'Ġ', 'Ċ', 're', 'ward', 'Ġof', 'Ġ3', '.', 'Ċ', 'Assuming', 'Ġa', 'Ġdiscount', 'Ġfactor', 'ĠÎ', '³', '=', '0', '.', '9', 'Ġand', 'Ġa', 'Ġdeter', 'ministic', 'Ġpolicy', 'Ġwhere', 'Ġaction', 'Ġa', 'Ġis', 'Ġ', 'Ċ', 'always', 'Ġtaken', ',', 'Ġcompute', 'Ġthe', 'Ġvalue', 'Ġof', 'Ġeach', 'Ġstate', 'Ġv', '(', 's', '1', 'Ġ)', 'Ġand', 'Ġv', '(', 's', '2', 'Ġ).', 'Ċ', '63', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ2', ':', 'ĠState', '-', 'Value', 'ĠFunction', 'Ċ', 'Solution', ':', 'Ċ', 'The', 'ĠBell', 'man', 'Ġequation', 'Ġfor', 'Ġthe', 'Ġvalue', 'Ġof', 'Ġeach', 'Ġstate', 'Ġs', 'Ġis', ':', 'Ċ', '1', '.', 'Ċ', 'For', 'Ġs', '2', 'Ġ:', 'Ċ', 'S', 'olving', 'Ġfor', 'Ġv', '(', 's', '2', 'Ġ)', 'ĠâĨĴ', 'Ġv', '(', 's', '2', ')=', '30', 'Ċ', '2', '.', 'Ċ', 'For', 'Ġs', '1', 'Ġ:', 'Ċ', 'Thus', ',', 'Ġv', '(', 's', '1', ')=', '32', 'Ġand', 'Ġv', '(', 's', '2', ')=', '30', '.', 'Ċ', '64', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ3', ':', 'ĠAction', '-', 'Value', 'ĠFunction', 'Ċ', 'Question', ':', 'Ċ', 'Using', 'Ġthe', 'Ġsame', 'ĠM', 'DP', 'Ġsetup', 'Ġas', 'Ġin', 'ĠExercise', 'Ġ2', ',', 'Ġcalculate', 'Ġthe', 'Ġaction', '-', 'value', 'Ġq', '(', 's', '1', 'Ġ,', 'a', ')', 'Ġand', 'Ġ', 'Ċ', 'q', '(', 's', '2', 'Ġ,', 'a', ')', 'Ġfor', 'Ġeach', 'Ġstate', '-', 'action', 'Ġpair', '.', 'Ċ', '65', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ3', ':', 'ĠAction', '-', 'Value', 'ĠFunction', 'Ċ', 'Solution', ':', 'Ċ', 'The', 'ĠBell', 'man', 'Ġequation', 'Ġfor', 'Ġthe', 'Ġaction', '-', 'value', 'Ġfunction', 'Ġis', ':', 'Ċ', 'Using', 'Ġthe', 'Ġstate', 'Ġvalues', 'Ġcalculated', 'Ġin', 'ĠExercise', 'Ġ2', ':', 'Ċ', '66', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ4', ':', 'ĠBell', 'man', 'ĠOptim', 'ality', 'ĠEqu', 'ation', 'Ċ', 'Question', ':', 'Ċ', 'Supp', 'ose', 'Ġwe', 'Ġhave', 'Ġan', 'ĠM', 'DP', 'Ġwith', 'Ġthree', 'Ġstates', 'ĠS', '={', 's', '1', ',', 's', '2', ',', 's', '3', '}', 'Ġand', 'Ġtwo', 'Ġactions', 'ĠA', '={', 'a', '1', ',', 'a', '2', '}.', 'Ġ', 'Ċ', 'The', 'Ġreward', 'Ġfunction', 'Ġand', 'Ġtransitions', 'Ġare', 'Ġgiven', 'Ġbelow', ':', 'Ċ', 'âĹı', 'Ċ', 'From', 'Ġs', '1', 'Ġ', 'Ġtaking', 'Ġa', '1', 'Ġ', 'Ġleads', 'Ġto', 'Ġs', '2', 'Ġ', 'Ġwith', 'Ġreward', 'Ġ4', '.', 'Ċ', 'âĹı', 'Ċ', 'From', 'Ġs', '1', 'Ġ', 'Ġtaking', 'Ġa', '2', 'Ġ', 'Ġleads', 'Ġto', 'Ġs', '3', 'Ġ', 'Ġwith', 'Ġreward', 'Ġ2', '.', 'Ċ', 'âĹı', 'Ċ', 'From', 'Ġs', '2', 'Ġ', 'Ġtaking', 'Ġa', '1', 'Ġ', 'Ġleads', 'Ġto', 'Ġs', '3', 'Ġ', 'Ġwith', 'Ġreward', 'Ġ5', '.', 'Ċ', 'âĹı', 'Ċ', 'From', 'Ġs', '3', 'Ġ', 'Ġtaking', 'Ġa', '1', 'Ġ', 'Ġor', 'Ġa', '2', 'Ġ', 'Ġleads', 'Ġback', 'Ġto', 'Ġs', '3', 'Ġ', 'Ġwith', 'Ġreward', 'Ġ3', '.', 'Ċ', 'Assuming', 'Ġa', 'Ġdiscount', 'Ġfactor', 'ĠÎ', '³', '=', '0', '.', '9', ',', 'Ġwrite', 'Ġthe', 'ĠBell', 'man', 'Ġoptim', 'ality', 'Ġequation', 'Ġfor', 'Ġv', 'âĪ', 'Ĺ', '(', 's', '1', 'Ġ).', 'Ċ', '67', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ4', ':', 'ĠBell', 'man', 'ĠOptim', 'ality', 'ĠEqu', 'ation', 'Ċ', '68', 'Ċ', 'Solution', ':', 'Ċ', 'The', 'ĠBell', 'man', 'Ġoptim', 'ality', 'Ġequation', 'Ġfor', 'Ġthe', 'Ġstate', '-', 'value', 'Ġfunction', 'Ġis', ':', 'Ċ', 'Sub', 'st', 'it', 'uting', 'Ġthe', 'Ġrewards', ':', 'Ċ', 'To', 'Ġsolve', 'Ġthis', ',', 'Ġwe', 'Ġwould', 'Ġneed', 'Ġthe', 'Ġvalues', 'Ġof', 'Ġv', '*', '(', 's', '2', ')', 'Ġand', 'Ġv', '*', '(', 's', '3', '),', 'Ġwhich', 'Ġcan', 'Ġbe', 'Ġcalculated', 'Ġ', 'Ċ', 'rec', 'urs', 'ively', 'Ġby', 'Ġapplying', 'Ġthe', 'ĠBell', 'man', 'Ġoptim', 'ality', 'Ġequation', 'Ġto', 'Ġeach', 'Ġstate', '.', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ4', ':', 'ĠBell', 'man', 'ĠOptim', 'ality', 'ĠEqu', 'ation', 'Ċ', '69', 'Ċ', 'Solution', ':', 'Ċ', 'The', 'Ġoptimal', 'Ġvalues', 'Ġfor', 'Ġeach', 'Ġstate', 'Ġare', ':', 'Ġ', 'Ċ', 'âĹı', 'Ċ', 'v', '*', '(', 's', '1', ')', 'Ġ=', 'Ġ32', '.', '8', 'Ċ', 'âĹı', 'Ċ', 'v', '*', '(', 's', '2', ')', 'Ġ=', 'Ġ32', 'Ċ', 'âĹı', 'Ċ', 'v', '*', '(', 's', '3', ')', 'Ġ=', 'Ġ30', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ5', ':', 'ĠOpt', 'imal', 'ĠPolicy', 'ĠDer', 'ivation', 'Ċ', 'Question', ':', 'Ċ', 'If', 'Ġthe', 'Ġoptimal', 'Ġaction', '-', 'value', 'Ġfunction', 'Ġq', 'âĪ', 'Ĺ', '(', 's', ',', 'a', ')', 'Ġfor', 'Ġsome', 'Ġstate', 'Ġs', 'Ġis', 'Ġgiven', 'Ġby', ':', 'Ċ', 'âĹı', 'Ċ', 'q', 'âĪ', 'Ĺ', '(', 's', ',', 'a', '1', 'Ġ)', '=', '12', 'Ċ', 'âĹı', 'Ċ'], ['q', 'âĪ', 'Ĺ', '(', 's', ',', 'a', '2', 'Ġ)', '=', '10', 'Ċ', 'What', 'Ġis', 'Ġthe', 'Ġoptimal', 'Ġpolicy', 'ĠÏ', 'Ģ', 'âĪ', 'Ĺ', '(', 'a', 'âĪ', '£', 's', ')?', 'Ċ', '70', 'Ċ', 'Ċ', 'Ex', 'ercise', 'Ġ5', ':', 'ĠOpt', 'imal', 'ĠPolicy', 'ĠDer', 'ivation', 'Ċ', 'Solution', ':', 'Ċ', 'The', 'Ġoptimal', 'Ġpolicy', 'ĠÏ', 'Ģ', 'âĪ', 'Ĺ', '(', 'a', 'âĪ', '£', 's', ')', 'Ġchooses', 'Ġthe', 'Ġaction', 'Ġthat', 'Ġmaxim', 'izes', 'Ġq', 'âĪ', 'Ĺ', '(', 's', ',', 'a', ').', 'Ċ', 'So', ':', 'Ċ', 'Ġ', 'Ċ', 'Thus', ',', 'Ġthe', 'Ġoptimal', 'Ġpolicy', 'Ġis', 'Ġto', 'Ġalways', 'Ġchoose', 'Ġaction', 'Ġa', '1', 'Ġ', 'Ġin', 'Ġstate', 'Ġs', ',', 'Ġsince', 'Ġq', 'âĪ', 'Ĺ', '(', 's', ',', 'a', '1', 'Ġ)', '>', 'q', 'âĪ', 'Ĺ', 'Ċ', '(', 's', ',', 'a', '2', 'Ġ).', 'Ċ', '71', 'Ċ', 'Ċ', 'Expl', 'oration', 'Ġ&', 'ĠExpl', 'o', 'itation', 'Ċ', 'Ċ', 'Expl', 'oration', 'Ġvs', '.', 'ĠExpl', 'o', 'itation', 'Ċ', 'âĹı', 'Ċ', 'In', 'ĠRL', ',', 'Ġthe', 'Ġagent', 'Ġfaces', 'Ġa', 'Ġdilemma', 'Ġbetween', ':', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'Expl', 'oration', ':', 'ĠTrying', 'Ġnew', 'Ġactions', 'Ġto', 'Ġdiscover', 'Ġvaluable', 'Ġ', 'Ċ', 'out', 'comes', '.', 'Ġ(', 'can', 'Ġbe', 'Ġharmful', 'âĢ¦)', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'Expl', 'o', 'itation', ':', 'ĠCho', 'osing', 'Ġactions', 'Ġthat', 'Ġhave', 'Ġyielded', 'Ġhigh', 'Ġ', 'Ċ', 're', 'wards', 'Ġin', 'Ġthe', 'Ġpast', '.', 'Ċ', '73', 'Ċ', 'âĹı', 'Ċ', 'Goal', ':', 'ĠBalance', 'Ġexploration', 'Ġand', 'Ġexploitation', 'Ġto', 'Ġmaximize', 'Ġrewards', 'Ġover', 'Ġtime', '.', 'Ċ', 'âĹı', 'Ċ', 'Chall', 'enge', ':', 'ĠToo', 'Ġmuch', 'Ġexploration', 'Ġcan', 'Ġdelay', 'Ġachieving', 'Ġrewards', ',', 'Ġwhile', 'Ġtoo', 'Ġ', 'Ċ', 'much', 'Ġexploitation', 'Ġcan', 'Ġlead', 'Ġto', 'Ġsub', 'opt', 'imal', 'Ġlong', '-', 'term', 'Ġresults', '.', 'Ċ', 'Ċ', 'Expl', 'oration', ':', 'ĠDiscover', 'ing', 'ĠNew', 'ĠOpportun', 'ities', 'Ċ', 'âĹı', 'Ċ', 'Example', 'Ġ1', 'Ġ-', 'ĠA', 'Ġrobot', 'Ġnavigating', 'Ġa', 'Ġmaze', ':', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'The', 'Ġrobot', 'Ġtries', 'Ġunfamiliar', 'Ġpaths', 'Ġto', 'Ġlocate', 'Ġshorter', 'Ġroutes', 'Ġor', 'Ġmore', 'Ġvaluable', 'Ġ', 'Ċ', 're', 'wards', '.', 'Ċ', 'âĹı', 'Ċ', 'Example', 'Ġ2', 'Ġ-', 'ĠA', 'Ġrecommendation', 'Ġsystem', ':', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'Occ', 'asionally', 'Ġrecommends', 'Ġnew', ',', 'Ġlesser', '-', 'known', 'Ġproducts', 'Ġto', 'Ġa', 'Ġuser', 'Ġto', 'Ġ', 'Ċ', 'learn', 'Ġtheir', 'Ġinterests', '.', 'Ċ', 'âĹı', 'Ċ', 'B', 'ene', 'ï', '¬', 'ģ', 't', ':', 'ĠExploration', 'Ġcan', 'Ġuncover', 'Ġhigher', 'Ġrewards', 'Ġthat', 'Ġaren', 'âĢ', 'Ļ', 't', 'Ġimmediately', 'Ġ', 'Ċ', 'ob', 'vious', '.', 'Ċ', '74', 'Ċ', 'Ċ', 'Expl', 'o', 'itation', ':', 'ĠLever', 'aging', 'ĠKnown', 'ĠInformation', 'Ċ', 'âĹı', 'Ċ', 'Example', 'Ġ1', 'Ġ-', 'ĠA', 'Ġtrading', 'Ġagent', ':', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'Select', 's', 'Ġstocks', 'Ġit', 'Ġhas', 'Ġpreviously', 'Ġident', 'i', 'ï', '¬', 'ģ', 'ed', 'Ġas', 'Ġpro', 'ï', '¬', 'ģ', 'table', ',', 'Ġpriorit', 'izing', 'Ġ', 'Ċ', 'cons', 'ist', 'ency', 'Ġover', 'Ġdiscovering', 'Ġnew', 'Ġoptions', '.', 'Ċ', 'âĹı', 'Ċ', 'B', 'ene', 'ï', '¬', 'ģ', 't', ':', 'ĠExpl', 'o', 'itation', 'Ġcapital', 'izes', 'Ġon', 'Ġknown', 'Ġsuccesses', ',', 'Ġensuring', 'Ġsteady', 'Ġrewards', '.', 'Ċ', 'âĹı', 'Ċ', 'Example', 'Ġ2', 'Ġ-', 'ĠA', 'Ġgame', '-', 'playing', 'ĠAI', ':', 'Ċ', 'âĹ', 'ĭ', 'Ċ', 'Repe', 'ats', 'Ġa', 'Ġhigh', '-', 're', 'ward', 'Ġmove', 'Ġ(', 'e', '.', 'g', '.,', 'Ġa', 'Ġchess', 'Ġopening', ')', 'Ġthat', 'Ġhas', 'Ġled', 'Ġto', 'Ġvictories', 'Ġ', 'Ċ', 'in', 'Ġpast', 'Ġgames', '.', 'Ċ', '75', 'Ċ', 'Ċ', 'Any', 'ĠQuestions', 'Ġ?', 'Ġ', 'Ċ', 'Don', 'âĢ', 'Ļ', 't', 'Ġhesitate', 'Ġto', 'Ġcontact', 'Ġme', 'Ċ', 'mor', 'and', '@', 'is', 'ir', '.', 'pm', 'c', '.', 'fr', 'Ċ', 'Ċ', 'Any', 'ĠQuestions', 'Ġ?', 'Ġ', 'Ċ', 'Contact', 'Ġus', 'Ġ!', 'Ċ', 'h', 'uss', 'am', '.', 'at', 'ou', 'i', '@', 'vale', 'o', '.', 'com', 'Ċ', 'Ċ', 'References', 'Ċ', '78', 'Ċ', '[', '1', ']', 'ĠDavid', 'ĠSilver', ',', 'ĠLect', 'ures', 'Ġon', 'ĠRein', 'forcement', 'ĠLearning', ',', 'Ġ2015', 'Ċ', '[', '2', ']', 'ĠRein', 'forcement', 'ĠLearning', 'Ġand', 'ĠAdvanced', 'ĠDeep', 'ĠLearning', 'Ġ(', 'S', 'or', 'bon', 'ne', ')', 'Ġ-', 'ĠOlivier', 'Ġ', 'Ċ', 'S', 'ig', 'aud', 'Ċ', '[', '3', ']', 'ĠSutton', ',', 'ĠR', '.', 'ĠS', '.', 'Ġand', 'ĠBart', 'o', ',', 'ĠA', '.', 'ĠG', '.', 'Ġ(', '2018', '),', 'ĠRein', 'forcement', 'ĠLearning', ':', 'ĠAn', 'ĠIntroduction', 'Ġ', 'Ċ', '(', 'Second', 'Ġedition', ').', 'ĠMIT', 'ĠPress', 'Ċ', 'Ġ', 'Ċ', 'O', 'liv', 'ier', 'ĠSig', 'aud', 'ĠYoutube', 'ĠChannel', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'left', '|', '\\\\', 'pro', 'd', '_{', 'i', '_{', '\\\\', 'math', 'rm', '{', 'e', '},', '\\\\', 'math', 'rm', '{', 'e', '},', '\\\\', 'math', 'rm', '{', '=', '^', '{\\\\', 'math', 'rm', '{', 'su', 'ue', 'ost', '}}', '\\\\', 'pro', 'd', '_{', 'i', '=', '\\\\', 'd', 'ots', '}\\\\', 'right', '|', '_{', '-', '{\\\\', 'math', 'rm', '{', 't', '}}', '=-', '\\\\', 'math', 'rm', '{', '=', '}', '-', '\\\\', 'math', 'rm', '{', '=', '}', '-', '\\\\', 'math', 'rm', '{', '=', '}', '-', '\\\\', 'math', 'rm', '{', '=', '}', '-', '\\\\', 'math', 'rm', '{', '=', '}}', '^', '{\\\\', 'math', 'rm', '{', 'sun', 'ere', '\\\\', ',', '\\\\', ',', '}}', '\\\\', 'left', '|', '\\\\', 'sum', '_{', 'i', '=', '\\\\', 'math', 'rm', '{', '=', '}}', '^', '{\\\\', 'math', 'rm', '{', 'su', 'are', '\\\\', ',', '\\\\', ',', '}}', '\\\\', 'right', '|', '_{', '+', '}\\\\', 'math', 'rm', '{', '~', '}\\\\', 'le', 'q', '{\\\\', 'frac', '{', '}{', '\\\\', 'sq', 'rt', '{\\\\', 'left', '|', '\\\\', 'left', '.', '\\\\', 'oper', 'at', 'orn', 'ame', '{', 's', 'us', 'umen', '}\\\\', 'math', 'rm', '{', 'ment', '}\\\\', 'right', '|', '_{', '-', '{\\\\', 'math', 'rm', '{', 'inton', '\\\\', 'right', '|', '}\\\\', 'right', '|', '=', '{\\\\', 'frac', '{\\\\', 'math', 'rm', '{', 'inton', 'h', '(', 's', 'ument', ')', '}}', '{\\\\', 'sq', 'rt', '{\\\\', 'left', '|', '\\\\', 'frac', '{\\\\', 'math', 'rm', '{', 's', 'ust', 'on', '}}', '{\\\\', 'sq', 'rt', '{\\\\', 'left', '(', '1', '+', '\\\\', 'math', 'rm', '{', 's', 'us', 'ume', '}}', '-', '\\\\', 'math', 'rm', '{', '-', '}', '-', '1', '-', '\\\\', 'math', 'rm', '{', '-', '}', '-', '1', '-', '1', '-', '1', '-', '1', '-', '1', '-', '1', '-', '1', '}', '-', '1', '\\\\', 'right', ')', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '{\\\\', 'math', 'f', 'rak', '{', 'v', '}}', '^', '{\\\\', 'chi', '_{', '*', '}', '^', '{\\\\', 'chi', '}}', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'begin', '{', 'array', '}{', 'c', '}{', '{\\\\', 'math', 'rm', '{', 'D', 'eter', 'rip', 'irit', 'ic', '}\\\\', ';', '\\\\', 'math', 'rm', '{', 'Policy', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\'], ['math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{\\\\', 'end', '{', 'array', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'begin', '{', 'array', '}{', 'c', '}{', '{\\\\', 'math', 'rm', '{', 'St', 'och', 'astic', '}\\\\', ';', '\\\\', 'math', 'rm', '{', 'Policy', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{\\\\', 'end', '{', 'array', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '{\\\\', 'math', 'f', 'rak', '{', 'i', 'Ġn', '}}', '_{', 'i', '}', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', 'v', '_{', '\\\\', 'pi', '}', '(', 's', ')=', '\\\\', 'math', 'bb', '{', 'R', '}', '_{', '\\\\', 'pi', '}\\\\', 'left', '[', 'R', '_{', 't', '+', '1', '}', '+', '\\\\', 'gam', 'ma', 'ĠR', '_{', 't', '+', '2', '}', '+', '\\\\', 'gam', 'ma', '^{', '2', '}', 'R', '_{', 't', '+', '3', '}', '+', '\\\\', 'cd', 'ot', '\\\\', 'cd', 'ot', '\\\\', 'cd', 'ot', '\\\\', 'right', ']', 'S', '_{', 't', '}', '=', 's', '\\\\', 'right', ']', 'ĊĊ', 'Ċ', 'Ċ', 'v', '_{', '\\\\', 'pi', '}', '(', 's', ')=', '\\\\', 'math', 'bb', '{', 'R', '}', '_{', '\\\\', 'pi', '}\\\\', 'left', '[', 'R', '_{', 't', '+', '1', '}', '+', '\\\\', 'gam', 'ma', 'ĠR', '_{', 't', '+', '2', '}', '+', '\\\\', 'gam', 'ma', '^{', '2', '}', 'R', '_{', 't', '+', '3', '}', '+', '\\\\', 'cd', 'ot', '\\\\', 'cd', 'ot', '\\\\', 'cd', 'ot', '\\\\', 'right', ']', 'S', '_{', 't', '}', '=', 's', '\\\\', 'right', ']', 'ĊĊ', 'Ċ', 'Ċ', 'v', '_{', '\\\\', 'pi', '}', '(', 's', ')=', '\\\\', 'math', 'bb', '{', 'R', '}', '_{', '\\\\', 'pi', '}\\\\', 'left', '[', 'R', '_{', 't', '+', '1', '}', '+', '\\\\', 'gam', 'ma', 'ĠR', '_{', 't', '+', '2', '}', '+', '\\\\', 'gam', 'ma', '^{', '2', '}', 'R', '_{', 't', '+', '3', '}', '+', '\\\\', 'cd', 'ot', '\\\\', 'cd', 'ot', '\\\\', 'cd', 'ot', '\\\\', 'right', ']', 'S', '_{', 't', '}', '=', 's', '\\\\', 'right', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', 'R', '_{', 's', '}', '^{', 'a', '}', '=', '\\\\', 'math', 'bb', '{', 'E', '}\\\\', 'left', '[', 'R', '_{', 't', '+', '1', '}\\\\', 'mid', 'ĠS', '_{', 't', '}', '=', 's', ',', 'A', '_{', 't', '}', '=', 'a', '\\\\', 'right', ']', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'over', 'line', '{{', '\\\\', 'oper', 'at', 'orn', 'ame', '{', 't', '}{', '\\\\', 'math', 'bf', '{', 'Z', '}}', '\\\\', 'lambda', '}}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'script', 'style', '{\\\\', 'frac', '{\\\\', 'Gam', 'ma', '^{', '*', '}\\\\', 'Gam', 'ma', '_{', '1', '}', '^', '{\\\\', 'prime', '}}', '{\\\\', 'Delta', '\\\\', 'L', 'amb', 'da', '_{', '1', '}', '^{', '2', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'frac', '{', 'e', '\\\\', 'math', 'bf', '{', 'c', '}\\\\', 't', 'au', '^{', '2', '}}', '{\\\\', 'Delta', '\\\\', 'math', 'bf', '{', 'k', '}', '_{', '1', '}', '^{', '2', '}\\\\', 'math', 'bf', '{', 'k', '}', '_{', '1', '}', '^{', '2', '}}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', 'U', '_{', '\\\\', 'pi', '}\\\\', '!', '\\\\', 'left', '(', 'S', '\\\\', 'right', ')', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', 'D', '\\\\', 'Big', 'l', '({', '\\\\', 'bf', 'ĠS', '}', '_{', 't', '+', '1', '}\\\\', 'vert', '{\\\\', 'bf', 'ĠS', '}', '_{', 't', '}\\\\', 'B', 'igr', ')\\\\', 'impl', 'ies', '\\\\', 'left', '.', 'D', '\\\\', 'Big', 'l', '({', '\\\\', 'bf', 'ĠS', '}', '_{', 't', '+', '1', '}\\\\', 'Big', 'l', '\\\\', 'vert', '{\\\\', 'cal', 'ĠS', '}', '_{', '1', '},', '\\\\', 'Ġ{\\\\', 'bf', 'ĠS', '}', '_{', '2', '},', '\\\\', 'ld', 'ots', '\\\\', 'right', '.', '\\\\', 'cd', 'ot', '\\\\', 'right', '.', '\\\\', 'cd', 'ot', '\\\\', 'left', '.', '\\\\', 'right', '.', '\\\\', 'right', '.', '\\\\', 'right', '.', '\\\\', 'ge', '\\\\', 'left', '.', '\\\\', 'left', '.', 'S', '_{', 't', '}\\\\', 'right', ')', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', 'P', '_{', 's', ',', 's', '^', '{\\\\', 'prime', '}}', '^{', 'a', '}\\\\', 'long', 'right', 'arrow', 'ĠP', '{\\\\', 'big', '(', '}', 'S', '_{', 't', '+', '1', '}\\\\', 'under', 'line', '1', '\\\\', 'under', 'line', '1', '\\\\', 'under', 'line', '1', '\\\\', 'under', 'line', '-', '\\\\', 'under', 'lines', ',', '\\\\', 'under', 'line', 'A', '_{', 't', '}\\\\', 'under', 'line', '-', '\\\\', 'under', 'lines', '\\\\', 'under', 'line', '-', '\\\\', 'under', 'line', 'a', '\\\\', 'big', ')', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'left', '.', '\\\\', 'frac', '{\\\\', 'l', 'angle', 'ĠE', '_{', 'Q', 'Ġe', 'Ġe', '}', '^{', '2', '}\\\\', 'r', 'angle', '\\\\', 'stack', 'rel', '{\\\\', 'left', 'arrow', '}{', '\\\\', 'long', 'right', 'arrow', '}\\\\', 'frac', '{\\\\', 'l', 'angle', 'ĠE', '_{', 'g', 'Ġe', '}', '^{', '2', '}', '|', 'E', '_{', 'e', '}', '^{', '2', '}\\\\', 'stack', 'rel', '{\\\\', 'long', 'right', 'arrow', '}{', '\\\\', 'the', 'ta', '}', '_{', 'e', '}', '^{', '2', '}\\\\', 'stack', 'rel', '{\\\\', 'long', 'right', 'arrow', '}{', '\\\\', 'the', 'ta', '}', '_{', 'g', 'Ġa', '}', '^{', '2', '}\\\\', 'stack', 'rel', '{\\\\', 'long', 'right', 'arrow', '}{', '\\\\', 'long', 'right', 'arrow', '}\\\\', 'frac', '{\\\\', 'pi', '}{', '\\\\', 'the', 'ta', '}', '_{', 'e', '}', '^{', '2', '}\\\\', '!', '\\\\', 'ps', 'i', '=', 'M', '\\\\', 'right', '\\\\', 'r', 'angle', '}\\\\', 'frac', '{\\\\', 'pi', '}{', '\\\\', 'the', 'ta', '}\\\\', 'frac', '{\\\\', 'math', 'rm', '{', 'G', '}', '_{', 'e', '}', '^{', '2'], ['}}', '{\\\\', 'math', 'rm', '{', '~', '.}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'begin', '{', 'array', '}{', 'c', '}{', '{\\\\', 'math', 'rm', '{', 'D', 'eter', 'rin', 'ir', 'iz', 'tic', '~', 'Policy', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\', 'end', '{', 'array', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'begin', '{', 'array', '}{', 'c', '}{', '{\\\\', 'oper', 'at', 'orn', 'ame', '{', 'St', 'och', 'astic', '}\\\\', 'oper', 'at', 'orn', 'ame', '{', 'Policy', '\\\\\\\\', 'Ġ{{', 'm', '(\\\\', 'partial', '|', 'S', ')=', 'P', '[', 'A', '_{', 't', '}', '=', '{\\\\', 'math', 'cal', '{', 'B', '}}', '_{', 't', '}', '=', '{\\\\', 'math', 'cal', '{', 'S', '}}', ']', '}}', '\\\\', 'end', '{', 'array', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'begin', '{', 'array', '}{', 'c', '}{', '{\\\\', 'math', 'rm', '{', 'D', 'eter', 'rip', 'irit', 'ic', '}\\\\', ';', '{\\\\', 'math', 'rm', '{', 'Policy', '}', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\\\\\', 'Ġ{{', '\\\\', 'math', 'rm', '{', '~', '\\\\\\\\', 'Ġ{{', '\\\\', 'dot', '{\\\\', 'cal', 'ĠA', '}', '=', '\\\\', 'over', 'line', '\\\\', 'cal', 'ĠT', '(', 'S', ')', '}}', '\\\\', 'end', '{', 'array', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'begin', '{', 'array', '}{', 'c', '}{', '{\\\\', 'oper', 'at', 'orn', 'ame', '{', 'St', 'och', 'astic', '}\\\\', 'oper', 'at', 'orn', 'ame', '{', 'Policy', '\\\\\\\\', 'Ġ{{', 'm', '(\\\\', 'partial', '|', 'S', ')=', 'P', '[', 'A', '_{', 't', '}', '=', '{\\\\', 'math', 'cal', '{', 'B', '}}', '_{', 't', '}', '=', '{\\\\', 'math', 'cal', '{', 'S', '}}', ']', '}}', '\\\\', 'end', '{', 'array', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'begin', '{', 'array', '}{', 'l', '}{', '{\\\\', 'stack', 'rel', '{\\\\', 'script', 'style', 'Ġs', '\\\\', 'to', 'Ġs', '}{', '\\\\', 'sq', 'rt', '{', 's', '}}', '\\\\', 'sum', '\\\\', 'sum', '\\\\', 'sum', '}}', '\\\\\\\\', 'Ġ{{', '\\\\', 'left', '(\\\\', 'sum', '_{', 'j', '=', 's', '=', '1', '}', '^{', 'p', '-', 's', '-', 's', '}\\\\', 'left', '(\\\\', 'sum', '_{', 'j', '=', 's', '=', 's', '}', '^{', 's', '=', 's', '}\\\\', 'Big', '(\\\\', 'sum', '_{', 'j', '=', 's', '}', '^{', 's', '=', 's', '}\\\\', 'Big', ')\\\\', 'over', 'brace', '{\\\\', 'sum', '_{', 'j', '=', 's', '}', '^{', 's', '=', 's', '}\\\\', 'big', 'cap', '}', '^', '{\\\\', 'script', 'style', 'Ġs', '\\\\', 'to', 'Ġs', '}\\\\', 'right', ')\\\\', 'sum', '_{', 'j', '=', 's', '\\\\\\\\', 'Ġ{{', '\\\\', 'left', '(\\\\', 'sum', '_{', 'j', '=', 's', '=', 's', '}', '^{', 's', '-', 's', '}\\\\', 'Big', '(\\\\', 'sum', '_{', 'j', '=', 's', '}', '^{', 's', '=', 's', '}\\\\', 'Big', ')\\\\', 'sum', '_{', 's', '=', 's', '}', '^{', 's', '=', 's', '}\\\\', 'pro', 'd', '_{', 'j', '=', 's', '}', '^{', 's', '=', 's', '}\\\\', 'pro', 'd', '_{', 's', '=', 's', '}', '^{', 's', '=', 's', '}\\\\', 'right', ')', '}}', '\\\\', 'end', '{', 'array', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', 'q', '_{', '\\\\', 'pi', '}', '(', 's', ',', 'a', ')=', '\\\\', 'math', 'bb', '{', 'E', '}', '_{', '\\\\', 'pi', '}\\\\', 'left', '[', '\\\\', 'sum', '_{', 'k', '=', '0', '}', '^', '{\\\\', 'in', 'fty', '}\\\\', 'gam', 'ma', '^{', 'k', '}', 'R', '_{', 't', '+', 'k', '+', '1', '}\\\\', 'mid', 'ĠS', '_{', 't', '}', '=', 's', ',', 'A', '_{', 't', '}', '=', 'a', '\\\\', 'right', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', 'v', '_{', '\\\\', 'pi', '}', '(', 's', ')\\\\', 'stack', 'rel', '{', 'v', '_{', '\\\\', 'pi', '}', '(', 's', ')\\\\', 'long', 'left', 'right', 'arrow', 'Ġs', '}{', '\\\\', 'sq', 'rt', '{\\\\', 'sum', '}}', '\\\\', 'left', '.', '\\\\', 'begin', '{', 'array', '}{', 'c', '}{', '{', '}}', '\\\\\\\\', 'Ġ{{', '}}', '\\\\\\\\', 'Ġ{{', '}}', '\\\\\\\\', 'Ġ{{', '}}', '\\\\\\\\', 'Ġ{{', 'v', '_{', '\\\\', 'pi', '}', '(', 's', ',', 'a', ')\\\\', 'long', 'left', 'right', 'arrow', 'Ġa', '}}', '\\\\', 'end', '{', 'array', '}\\\\', 'right', '.', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'begin', '{', 'array', '}{', 'c', '}{', '{', 'q', '_{', '\\\\', 'pi', '}', '(', 's', ',', 'a', ')\\\\', 'stack', 'rel', '{\\\\', 'left', 'arrow', '}{', '\\\\', 'left', 'arrow', '}', 's', ',', 'a', '_{', '\\\\', 's', 'igma', '},', 'a', '_{', '\\\\', 'gam', 'ma', '}\\\\', 'display', 'style', '\\\\', 'l', 'angle', '\\\\', 'left', '.', '\\\\', 'n', 'ab', 'la', '_{', '\\\\', 'mu', '}\\\\', 'right', '.', '^', '{\\\\', 'in', 'fty', '\\\\\\\\', 'Ġ{{', '\\\\', 'q', 'quad', '\\\\', 'left', '.', 'r', '\\\\', 'right', '.', '}}', '\\\\\\\\', 'Ġ{{', '\\\\', 'q', 'quad', '\\\\', 'q', 'quad', '\\\\', 'left', '.', 'q', '_{', '\\\\', 'pi', '}', '(', 's', '^', '{\\\\', 'prime', '}', ')\\\\', 'stack', 'rel', '{\\\\', 'left', 'arrow', '}{', '\\\\', 'sim', '}', 'e', '^', '{\\\\', 'prime', '}\\\\', 'right', '.', '\\\\', 'sum', '_{', 's', '^', '{\\\\', 'prime', '}}', '\\\\', 'Gam', 'ma', '_{', '\\\\', 'pi', '}', '(', 's', '^', '{\\\\', 'prime', '})', '}}', '\\\\', 'end', '{', 'array', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'begin', '{', 'array', '}{', 'l', '}{', '{\\\\', 'mp', '\\\\', 'left', '(\\\\', 'sum', '_{', 'i', '=', '1', '}', '^{', 'n', '}\\\\', 'right', ')\\\\', 'quad', '{\\\\', 'sq', 'rt', '[]', '{\\\\', 'sim', '}', '\\\\\\\\', 'Ġ{{', '\\\\', 'left', '(\\\\', 'sum', '_{', 'i', '=', '1', '}', '^{', 'n', '+', '1', '}\\\\', 'over', 'brace', '{\\\\', 'frac', '{', 'n', '}{', 'i', '+', 'i', '}}', '^{', 'n', '+', '}\\\\', 'big', 'cap', '_{', 'i', '=', '1', '}', '^{', 'n', '}\\\\', 'over', 'brace', '{', 'i', '^', '{\\\\', 'prime', '}}', '^{', 'n', '}\\\\', 'over', 'brace', '{', 'i', '^', '{\\\\', 'prime', '}}', '^{', 'n', '}\\\\', 'over', 'brace', '{', 'i', '^', '{\\\\', 'prime', '}}', '^{', 'n', '\\\\\\\\', 'Ġ{{', '\\\\', 'left', '(\\\\', 'sum', '_{', 'i', '=', '1', '}', '^{', 'n', '+', 'i', '}\\\\', 'left', '(\\\\', 'sum', '_{', 'i', '=', '1', '}', '^{', 'n', '}\\\\', 'over', 'brace', '{', 'i', '^'], ['{\\\\', 'prime', '}}', '^{', 'n', '}\\\\', 'over', 'brace', '{', 'i', '^', '{\\\\', 'prime', '}}', '^{', 'n', '}\\\\', 'right', ')', '^', '{\\\\', 'prime', '\\\\', 'end', '{', 'array', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'ĊĊ', 'Ċ', 'Ċ', '$$', 'Ġ\\\\', 'Ġ_', '{\\\\', 'frac', '{\\\\', 'beta', '}{', '\\\\', 'L', 'amb', 'da', '_{', '\\\\', 'math', 'rm', '{', 'S', '}', 'Ġ$$', 'ĊĊ', 'Ċ', 'Ċ', '[', 'image', ']', 'Ċ']]\n"
     ]
    }
   ],
   "source": [
    "from summarizer import *\n",
    "chunks = get_chunks(text, 1000)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Reinforcement Learning\\nAn Introduction\\n\\nInstructor\\nHussam ATOUI\\n●\\nSoftware Engineer at Valeo \\nCréteil (France) since Nov 2022\\n●\\nPhD-Cifre RENAULT & \\nGrenoble-Alpes University \\n(2019-2022)\\n●\\nSpecialities: Automated Driving, \\nReinforcement Learning, \\nAutomatic Control, Optimization\\n\\nInstructor\\nVictor MORAND\\nmorand@isir.upmc.fr\\n●\\nPhD Student at ISIR (Sorbonne - \\nCNRS)\\n○\\nExplaining how LLMs manipulate \\nKnowledge\\n○\\ntowards AIs that know what they \\nknow\\n●\\nAlso out of a School of Engineering ! Feel free to reach out at the end of our \\nsessions ! Course Content\\n1. Introduction to Reinforcement Learning\\n2. Markov Decision Processes (MDPs)\\n3. Policy and Value Functions\\n4. Dynamic Programming (DP) for RL\\n5. Model-Free methods\\n6. Value Function Approximation\\n7. Policy-Gradient and Actor-Critic Methods\\n8. Deep RL\\n9.', 'TP Project\\n\\nIntroduction to Reinforcement \\nLearning (RL)\\n\\nSupervised Learning\\nReinforcement Learning\\nUnsupervised Learning\\nTrain with labeled data\\nTrain with unlabeled data\\nClustering\\nTrain with environment \\nexperience\\nRegression\\nClassiﬁcation\\n6\\nTypes of Learning\\n\\nSupervised Learning \\nModel\\nInputs:\\nFeatures / States\\nPredicted Outputs:\\nValue/ Class\\nTraining target:\\nTarget Output\\n●\\nError: Target Output - Predicted Output\\n●\\nObjective: Minimize the error between the target and the predicted output\\n7\\nSupervised Learning\\n\\nSupervised Learning\\n\\nReinforcement Learning\\nReinforcement \\nLearning Model\\nInputs:\\nFeatures / States\\nPredicted Outputs:\\nActions\\nEvaluation:\\nRewards / Penalties\\n●\\nError: Awards - Penalties \\n●\\nObjective: Maximize the awards and decrease penalties as much as possible\\n9\\n\\nReinforcement Learning\\n\\nExamples of Rewards [1]\\n●\\nFly stunt manoeuvres in a helicopter\\n○\\n+ve reward for following desired trajectory\\n○\\n−ve reward for crashing\\n●\\nManage an investment portfolio\\n○\\n+ve reward for each $ in bank\\n●\\nControl a power station\\n○\\n+ve reward for producing power\\n○\\n−ve reward for exceeding safety thresholds\\n●\\nMake a humanoid robot walk\\n○\\n+ve reward for forward motion\\n○\\n−ve reward for falling over\\n●\\nPlay many different Atari games better than humans\\n○\\n+/−ve reward for increasing/decreasing score\\n11\\n\\nAgent and Environment\\nActions   A(t)\\nObservations   O(t) \\nRewards   R(t) \\nAgent\\nEnvironment\\nAt step t: \\nThe Agent:\\n●\\nReceives O(t)\\n●\\nReceives R(t)\\n●\\nExecutes A(t)\\nThe Environment:\\n●\\nReceives A(t)\\n●\\nEmits O(t+1)\\n●\\nEmits R(t+1)\\nt++\\n12\\n\\nFully Observable Environment\\nObservations   O(t) \\nAgent\\nEnvironment\\n●\\nEnvironment observations = Agent \\nstate\\n●\\nThis is assumed in Markov Decision \\nProcess (MDP)\\n13\\n\\nPartially Observable Environment\\nObservations   O(t) \\nAgent\\nEnvironment\\n●\\nEnvironment observations ≠ Agent state\\n○\\nA drone navigating a forest only sees \\nnearby obstacles.', '○\\nA healthcare agent observes patient \\nsymptoms but not the underlying \\ndisease. ○\\nA self-driving car detects nearby \\nvehicles but not hidden pedestrians. ○\\nA weather forecasting model \\nobserves recent conditions but not \\nfuture patterns\\n●\\nThis is called Partially Observable Markov \\nDecision Process (POMDP)\\n(Missing info)\\n14\\n\\nReinforcement Learning\\n15\\nAgent: The system that takes \\nactions to be trained. Environment: The external \\nsystem with which the agent \\ninteracts. State: The information \\nrequired by the agent to take \\nan action. This info is observed \\nfrom the environment. Action: The decision or \\nmove that the agent makes \\nat a particular state\\nReward: Feedback received \\nby the agent to evaluate the \\ntaken action under a certain \\nstate. General Architecture\\n\\n16\\nReinforcement Learning\\n\\nPolicy\\nRL Agent\\nA policy deﬁnes the agent’s behavior in the environment.', 'It \\nrepresents a mapping from states to actions, for example:\\n●\\nDeterministic policy:                   , \\nwhere the action a is chosen directly based on state s.\\n●\\nStochastic policy:                                                   ,\\nwhere the policy gives the probability of taking action \\na given state s.\\n17\\n\\nValue Function\\nRL Agent\\nA value function \\n●\\nestimates the expected future reward \\n●\\nassesses the quality of states, helping to determine the best actions to \\ntake. For example, the state value under policy 𝜋 is given by:\\nThis equation expresses the expected sum of discounted rewards starting from \\nstate 𝑠. 18\\n\\nValue Function\\nRL Agent\\nA value function \\n●\\nestimates the expected future reward \\n●\\nassesses the quality of states, helping to determine the best actions to \\ntake. For example, the state value under policy 𝜋 is given by:\\nThis equation expresses the expected sum of discounted rewards starting from \\nstate 𝑠.', '19\\n\\nValue Function\\nRL Agent\\n20\\nγ∈[0,1]:\\n●\\nIf γ=0, the agent focuses solely on immediate rewards. ●\\nIf γ=1, future rewards are valued equally to immediate rewards.', \"Model\\nRL Agent\\nA model forecasts the environment's next state and expected reward:\\n●\\n𝑃 represents the probability of the next state given the current state and \\naction:\\n●\\n𝑅 represents the expected immediate reward given the current state and \\naction:\\n21\\n\\nStates, Actions, Rewards\\nExample: Maze [1]\\n●\\nStates: Agent’s location\\n●\\nActions: Right, Left, Up, Down\\n●\\nRewards: -1 per time-step\\n22\\n\\nPolicy\\nExample: Maze [1]\\nArrows represent policy π(s) for \\neach state s\\n23\\n\\nExample: Maze [1]\\nNumbers represent value           of \\neach state s\\n24\\n\\nDifferent Types\\nRL Agents\\n➔\\nValue-based:\\n◆\\nNo Policy\\n◆\\nValue Function\\n➔\\nPolicy-based:\\n◆\\nPolicy\\n◆\\nNo Value Function\\n➔\\nActor-Critic:\\n◆\\nPolicy\\n◆\\nValue Function\\n➔\\nModel-free:\\n◆\\nPolicy and/or Value Function\\n◆\\nNo Model\\n➔\\nModel-based:\\n◆\\nPolicy and/or Value Function\\n◆\\nModel\\n25\\n\\nMarkov Decision Processes (MDPs)\\n\\nMarkov Process\\n●\\nA Markov Process is a memoryless process where the future state depends only \\non the current state and not on any past states.\", '●\\nFormally, a Markov Process is a tuple: M=(S,P)\\nWhere:\\n●\\nS: A ﬁnite set of states. ●\\nP: Transition probabilities between states, deﬁned as:\\n27\\n\\nThe Markov Property\\n●\\nMarkov property: Future depends only on the present, not past states\\n●\\nSimpliﬁes state transition modeling\\n28\\n\\n●\\nA Markov Reward Process is a Markov Process with added rewards. ●\\nIt is represented as a tuple: MR=(S,P,R,γ)\\nWhere:\\n●\\nR(s): Reward function providing the expected reward at each state s, \\n●\\nγ: Discount factor, controlling the importance of future rewards. 29\\nMarkov Reward Process\\n\\nCumulative Reward - Gain\\nMarkov Reward Process\\n●\\nCumulative Reward G(t) : Expected cumulative reward from state s\\n30\\n●\\n(State-)Value Function v(s) : Expected state-value of state s\\n\\nState-Value Function\\nBellman Equation\\n●\\nThe state-value function can be presented as an immediate reward and future reward \\nas follows:\\nPROOF? 31\\n\\nProof\\nBellman Equation\\nStochastic Eq. ?', '32\\n\\nExample\\n\\nExample: Student MRP (P, S, R) [1] \\n34\\n\\nDiscount factor effect\\nExample: Student MRP (P, S, R) [1]  \\n35\\n\\nDiscount factor effect\\nExample: Student MRP (P, S, R) [1]  \\n36\\n\\nExercise\\n\\nDiscount factor effect\\nExample: Student MRP (P, S, R) [1]  \\n38\\n? Example of Bellman’s equation\\nExample: Student MRP (P, S, R) [1]  \\n39\\n\\nMRP → MDP\\n(P, S, R) → (P, S, A, R)\\n\\nMarkov Decision Process (MDP)\\n A Markov decision process is a 4-tuple (S, A, P, R):\\nNote: A ﬁnite MDP is an MDP with ﬁnite state, action, and reward \\nsets. Much of the current theory of reinforcement learning is \\nrestricted to ﬁnite MDPs.', '●\\nStates (S): Describe environment situations\\n●\\nActions (A): Choices available to the agent\\n●\\nRewards (R): Immediate feedback for actions\\n●\\nTransition Probabilities (P): Likelihood of reaching a \\nnew state\\n41\\n\\nState Transitions - Policy\\nMarkov Decision Process\\n●\\nTransition probability: P(s′∣s,a)\\n●\\nModels probability of moving to s′ from s after action a\\n42\\n\\nReward Function and Policy\\nMarkov Decision Process\\n●\\nReward function R(s,a): Immediate feedback\\n●\\nPositive rewards encourage actions; negative prevent actions\\n●\\nDeterministic policy:                   , \\nwhere the action a is chosen directly based on state s.\\n●\\nStochastic policy:                                                   ,\\nwhere the policy gives the probability of taking action a given \\nstate s.\\n43\\n\\nPolicies\\nMarkov Decision Process\\n●\\nDeterministic policy: \\n                  \\nwhere the action a is chosen directly based on state s.\\n●\\nStochastic policy:         \\n                                          \\nwhere the policy gives the probability of taking action a given state s.\\n44\\n\\nExample: Student MDP (P, S, A, R) [1]  \\n45\\n\\nValue Functions\\nMarkov Decision Process\\n●\\nState-value function : Expected cumulative reward from state s under policy π\\n46\\n●\\nAction-value function: Expected reward of taking action a in state s under policy π\\n\\nState-Value Function\\nMarkov Decision Process\\n47\\n\\nBellman Expectation Equation\\n48\\n●\\nState-value function : Expected cumulative reward from state s under policy π\\n●\\nAction-value function: Expected reward of taking action a in state s under policy π\\n\\nBellman Expectation Equation [1]\\n49\\n\\nBellman Expectation Equation [1]\\n50\\n\\nBellman Expectation Equation [1]\\n51\\n\\nBellman Expectation Equation [1]\\n52\\n\\nExercise\\n\\nExample: Student MDP\\n54\\n?', 'Example: Student MDP\\n55\\n\\nState-Value and Action-Value Functions\\nBellman Optimality\\n●\\nThe optimal state-value function\\n●\\nOptimal action-value function\\n56\\n\\nExercise: Optimal State-Value Function [1]\\n57\\n\\nExercise: Optimal Action-Value Function [1]\\n58\\n\\nFind an Optimal Policy\\n●\\nAn optimal policy π∗ can be determined by selecting actions that \\nmaximize the optimal action-value function q∗(s,a). The optimal policy π∗\\n(a∣s) is deﬁned as:\\n●\\nFor any MDP, there is always a deterministic optimal policy. If q∗(s,a) is \\nknown, we can directly derive the optimal policy from it. 59\\n\\nExercises\\n\\nExercise 1: Understanding Policies\\nQuestion:\\nLet S={s1,s2} be a set of two states and A={a1,a2} be a set of two actions. Suppose a \\nstochastic policy π is deﬁned as follows:\\n1. What is the probability of taking action a2  in state s1  under this policy? 2. If the agent is in state s2 , what is the probability of taking action a1  under this \\npolicy?', '61\\n\\nExercise 1: Understanding Policies\\nSolution:\\n1. The probability of taking action a2  in state s1  is given directly by π(a2 |s1 )=0.3\\n2. The probability of taking action a1  in state s2  is given by π(a1 ∣s2 )=0.4\\n62\\n\\nExercise 2: State-Value Function\\nQuestion:\\nConsider a simple MDP with two states s1  and s2  and a single action a with the \\nfollowing reward structure:\\n●\\nStarting from s1  and taking action a, the agent moves to s2  with a reward of 5. ●\\nStarting from s2  and taking action a, the agent stays in s2  and receives a \\nreward of 3. Assuming a discount factor γ=0.9 and a deterministic policy where action a is \\nalways taken, compute the value of each state v(s1 ) and v(s2 ). 63\\n\\nExercise 2: State-Value Function\\nSolution:\\nThe Bellman equation for the value of each state s is:\\n1. For s2 :\\nSolving for v(s2 ) → v(s2)=30\\n2. For s1 :\\nThus, v(s1)=32 and v(s2)=30.', '64\\n\\nExercise 3: Action-Value Function\\nQuestion:\\nUsing the same MDP setup as in Exercise 2, calculate the action-value q(s1 ,a) and \\nq(s2 ,a) for each state-action pair. 65\\n\\nExercise 3: Action-Value Function\\nSolution:\\nThe Bellman equation for the action-value function is:\\nUsing the state values calculated in Exercise 2:\\n66\\n\\nExercise 4: Bellman Optimality Equation\\nQuestion:\\nSuppose we have an MDP with three states S={s1,s2,s3} and two actions A={a1,a2}. The reward function and transitions are given below:\\n●\\nFrom s1  taking a1  leads to s2  with reward 4. ●\\nFrom s1  taking a2  leads to s3  with reward 2. ●\\nFrom s2  taking a1  leads to s3  with reward 5. ●\\nFrom s3  taking a1  or a2  leads back to s3  with reward 3. Assuming a discount factor γ=0.9, write the Bellman optimality equation for v∗(s1 ).', '67\\n\\nExercise 4: Bellman Optimality Equation\\n68\\nSolution:\\nThe Bellman optimality equation for the state-value function is:\\nSubstituting the rewards:\\nTo solve this, we would need the values of v*(s2) and v*(s3), which can be calculated \\nrecursively by applying the Bellman optimality equation to each state. Exercise 4: Bellman Optimality Equation\\n69\\nSolution:\\nThe optimal values for each state are: \\n●\\nv*(s1) = 32.8\\n●\\nv*(s2) = 32\\n●\\nv*(s3) = 30\\n\\nExercise 5: Optimal Policy Derivation\\nQuestion:\\nIf the optimal action-value function q∗(s,a) for some state s is given by:\\n●\\nq∗(s,a1 )=12\\n●\\nq∗(s,a2 )=10\\nWhat is the optimal policy π∗(a∣s)? 70\\n\\nExercise 5: Optimal Policy Derivation\\nSolution:\\nThe optimal policy π∗(a∣s) chooses the action that maximizes q∗(s,a).', 'So:\\n \\nThus, the optimal policy is to always choose action a1  in state s, since q∗(s,a1 )>q∗\\n(s,a2 ). 71\\n\\nExploration & Exploitation\\n\\nExploration vs. Exploitation\\n●\\nIn RL, the agent faces a dilemma between:\\n○\\nExploration: Trying new actions to discover valuable \\noutcomes. (can be harmful…)\\n○\\nExploitation: Choosing actions that have yielded high \\nrewards in the past. 73\\n●\\nGoal: Balance exploration and exploitation to maximize rewards over time. ●\\nChallenge: Too much exploration can delay achieving rewards, while too \\nmuch exploitation can lead to suboptimal long-term results. Exploration: Discovering New Opportunities\\n●\\nExample 1 - A robot navigating a maze:\\n○\\nThe robot tries unfamiliar paths to locate shorter routes or more valuable \\nrewards. ●\\nExample 2 - A recommendation system:\\n○\\nOccasionally recommends new, lesser-known products to a user to \\nlearn their interests. ●\\nBeneﬁt: Exploration can uncover higher rewards that aren’t immediately \\nobvious.', '74\\n\\nExploitation: Leveraging Known Information\\n●\\nExample 1 - A trading agent:\\n○\\nSelects stocks it has previously identiﬁed as proﬁtable, prioritizing \\nconsistency over discovering new options. ●\\nBeneﬁt: Exploitation capitalizes on known successes, ensuring steady rewards. ●\\nExample 2 - A game-playing AI:\\n○\\nRepeats a high-reward move (e.g., a chess opening) that has led to victories \\nin past games. 75\\n\\nAny Questions ? Don’t hesitate to contact me\\nmorand@isir.pmc.fr\\n\\nAny Questions ? Contact us ! hussam.atoui@valeo.com\\n\\nReferences\\n78\\n[1] David Silver, Lectures on Reinforcement Learning, 2015\\n[2] Reinforcement Learning and Advanced Deep Learning (Sorbonne) - Olivier \\nSigaud\\n[3] Sutton, R. S. and Barto, A. G. (2018), Reinforcement Learning: An Introduction \\n(Second edition).', 'MIT Press\\n \\nOlivier Sigaud Youtube Channel\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n$$ \\\\left|\\\\prod_{i_{\\\\mathrm{e},\\\\mathrm{e},\\\\mathrm{=^{\\\\mathrm{suueost}}\\\\prod_{i=\\\\dots}\\\\right|_{-{\\\\mathrm{t}}=-\\\\mathrm{=}-\\\\mathrm{=}-\\\\mathrm{=}-\\\\mathrm{=}-\\\\mathrm{=}}^{\\\\mathrm{sunere\\\\,\\\\,}}\\\\left|\\\\sum_{i=\\\\mathrm{=}}^{\\\\mathrm{suare\\\\,\\\\,}}\\\\right|_{+}\\\\mathrm{~}\\\\leq{\\\\frac{}{\\\\sqrt{\\\\left|\\\\left.\\\\operatorname{susumen}\\\\mathrm{ment}\\\\right|_{-{\\\\mathrm{inton\\\\right|}\\\\right|={\\\\frac{\\\\mathrm{intonh(sument)}}{\\\\sqrt{\\\\left|\\\\frac{\\\\mathrm{suston}}{\\\\sqrt{\\\\left(1+\\\\mathrm{susume}}-\\\\mathrm{-}-1-\\\\mathrm{-}-1-1-1-1-1-1-1}-1\\\\right) $$\\n\\n\\n\\n{\\\\mathfrak{v}}^{\\\\chi_{*}^{\\\\chi}}\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n$$ \\\\begin{array}{c}{{\\\\mathrm{Deterripiritic}\\\\;\\\\mathrm{Policy\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\end{array} $$\\n\\n\\n\\n$$ \\\\begin{array}{c}{{\\\\mathrm{Stochastic}\\\\;\\\\mathrm{Policy\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\\\\\ {{\\\\mathrm{\\\\end{array} $$\\n\\n\\n\\n{\\\\mathfrak{i n}}_{i}\\n\\n\\n\\n[image]\\n\\n\\n\\nv_{\\\\pi}(s)=\\\\mathbb{R}_{\\\\pi}\\\\left[R_{t+1}+\\\\gamma R_{t+2}+\\\\gamma^{2}R_{t+3}+\\\\cdot\\\\cdot\\\\cdot\\\\right]S_{t}=s\\\\right]\\n\\n\\n\\nv_{\\\\pi}(s)=\\\\mathbb{R}_{\\\\pi}\\\\left[R_{t+1}+\\\\gamma R_{t+2}+\\\\gamma^{2}R_{t+3}+\\\\cdot\\\\cdot\\\\cdot\\\\right]S_{t}=s\\\\right]\\n\\n\\n\\nv_{\\\\pi}(s)=\\\\mathbb{R}_{\\\\pi}\\\\left[R_{t+1}+\\\\gamma R_{t+2}+\\\\gamma^{2}R_{t+3}+\\\\cdot\\\\cdot\\\\cdot\\\\right]S_{t}=s\\\\right]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\nR_{s}^{a}=\\\\mathbb{E}\\\\left[R_{t+1}\\\\mid S_{t}=s,A_{t}=a\\\\right]\\n\\n\\n\\n$$ \\\\overline{{\\\\operatorname{t}{\\\\mathbf{Z}}\\\\lambda}} $$\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n$$ \\\\scriptstyle{\\\\frac{\\\\Gamma^{*}\\\\Gamma_{1}^{\\\\prime}}{\\\\Delta\\\\Lambda_{1}^{2 $$\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n$$ \\\\frac{e\\\\mathbf{c}\\\\tau^{2}}{\\\\Delta\\\\mathbf{k}_{1}^{2}\\\\mathbf{k}_{1}^{2}} $$\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\nU_{\\\\pi}\\\\!\\\\left(S\\\\right)\\n\\n\\n\\n[image]\\n\\n\\n\\nD\\\\Bigl({\\\\bf S}_{t+1}\\\\vert{\\\\bf S}_{t}\\\\Bigr)\\\\implies\\\\left.D\\\\Bigl({\\\\bf S}_{t+1}\\\\Bigl\\\\vert{\\\\cal S}_{1},\\\\ {\\\\bf S}_{2},\\\\ldots\\\\right.\\\\cdot\\\\right.\\\\cdot\\\\left.\\\\right.\\\\right.\\\\right.\\\\ge\\\\left.\\\\left.S_{t}\\\\right)\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\nP_{s,s^{\\\\prime}}^{a}\\\\longrightarrow P{\\\\big(}S_{t+1}\\\\underline1\\\\underline1\\\\underline1\\\\underline-\\\\underlines,\\\\underlineA_{t}\\\\underline-\\\\underlines\\\\underline-\\\\underlinea\\\\big)\\n\\n\\n\\n[image]\\n\\n\\n\\n$$ \\\\left.\\\\frac{\\\\langle E_{Q e e}^{2}\\\\rangle\\\\stackrel{\\\\leftarrow}{\\\\longrightarrow}\\\\frac{\\\\langle E_{g e}^{2}|E_{e}^{2}\\\\stackrel{\\\\longrightarrow}{\\\\theta}_{e}^{2}\\\\stackrel{\\\\longrightarrow}{\\\\theta}_{g a}^{2}\\\\stackrel{\\\\longrightarrow}{\\\\longrightarrow}\\\\frac{\\\\pi}{\\\\theta}_{e}^{2}\\\\!\\\\psi=M\\\\right\\\\rangle}\\\\frac{\\\\pi}{\\\\theta}\\\\frac{\\\\mathrm{G}_{e}^{2}}{\\\\mathrm{~.}', '$$\\n\\n\\n\\n$$ \\\\begin{array}{c}{{\\\\mathrm{Deterriniriztic~Policy\\\\\\\\ {{\\\\mathrm{~\\\\\\\\ {{\\\\mathrm{~\\\\\\\\ {{\\\\mathrm{~\\\\\\\\ {{\\\\mathrm{~\\\\\\\\ {{\\\\mathrm{~\\\\\\\\ {{\\\\mathrm{~\\\\end{array} $$\\n\\n\\n\\n$$ \\\\begin{array}{c}{{\\\\operatorname{Stochastic}\\\\operatorname{Policy\\\\\\\\ {{m(\\\\partial|S)=P[A_{t}={\\\\mathcal{B}}_{t}={\\\\mathcal{S}}]}}\\\\end{array} $$\\n\\n\\n\\n[image]\\n\\n\\n\\n$$ \\\\begin{array}{c}{{\\\\mathrm{Deterripiritic}\\\\;{\\\\mathrm{Policy}\\\\\\\\ {{\\\\mathrm{~\\\\\\\\ {{\\\\mathrm{~\\\\\\\\ {{\\\\mathrm{~\\\\\\\\ {{\\\\mathrm{~\\\\\\\\ {{\\\\dot{\\\\cal A}=\\\\overline\\\\cal T(S)}}\\\\end{array} $$\\n\\n\\n\\n$$ \\\\begin{array}{c}{{\\\\operatorname{Stochastic}\\\\operatorname{Policy\\\\\\\\ {{m(\\\\partial|S)=P[A_{t}={\\\\mathcal{B}}_{t}={\\\\mathcal{S}}]}}\\\\end{array} $$\\n\\n\\n\\n$$ \\\\begin{array}{l}{{\\\\stackrel{\\\\scriptstyle s\\\\to s}{\\\\sqrt{s}}\\\\sum\\\\sum\\\\sum}}\\\\\\\\ {{\\\\left(\\\\sum_{j=s=1}^{p-s-s}\\\\left(\\\\sum_{j=s=s}^{s=s}\\\\Big(\\\\sum_{j=s}^{s=s}\\\\Big)\\\\overbrace{\\\\sum_{j=s}^{s=s}\\\\bigcap}^{\\\\scriptstyle s\\\\to s}\\\\right)\\\\sum_{j=s\\\\\\\\ {{\\\\left(\\\\sum_{j=s=s}^{s-s}\\\\Big(\\\\sum_{j=s}^{s=s}\\\\Big)\\\\sum_{s=s}^{s=s}\\\\prod_{j=s}^{s=s}\\\\prod_{s=s}^{s=s}\\\\right)}}\\\\end{array} $$\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\nq_{\\\\pi}(s,a)=\\\\mathbb{E}_{\\\\pi}\\\\left[\\\\sum_{k=0}^{\\\\infty}\\\\gamma^{k}R_{t+k+1}\\\\mid S_{t}=s,A_{t}=a\\\\right]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\nv_{\\\\pi}(s)\\\\stackrel{v_{\\\\pi}(s)\\\\longleftrightarrow s}{\\\\sqrt{\\\\sum}}\\\\left.\\\\begin{array}{c}{{}}\\\\\\\\ {{}}\\\\\\\\ {{}}\\\\\\\\ {{}}\\\\\\\\ {{v_{\\\\pi}(s,a)\\\\longleftrightarrow a}}\\\\end{array}\\\\right.', '$$ \\\\begin{array}{c}{{q_{\\\\pi}(s,a)\\\\stackrel{\\\\leftarrow}{\\\\leftarrow}s,a_{\\\\sigma},a_{\\\\gamma}\\\\displaystyle\\\\langle\\\\left.\\\\nabla_{\\\\mu}\\\\right.^{\\\\infty\\\\\\\\ {{\\\\qquad\\\\left.r\\\\right.', '}}\\\\\\\\ {{\\\\qquad\\\\qquad\\\\left.q_{\\\\pi}(s^{\\\\prime})\\\\stackrel{\\\\leftarrow}{\\\\sim}e^{\\\\prime}\\\\right.\\\\sum_{s^{\\\\prime}}\\\\Gamma_{\\\\pi}(s^{\\\\prime})}}\\\\end{array} $$\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n$$ \\\\begin{array}{l}{{\\\\mp\\\\left(\\\\sum_{i=1}^{n}\\\\right)\\\\quad{\\\\sqrt[]{\\\\sim}\\\\\\\\ {{\\\\left(\\\\sum_{i=1}^{n+1}\\\\overbrace{\\\\frac{n}{i+i}}^{n+}\\\\bigcap_{i=1}^{n}\\\\overbrace{i^{\\\\prime}}^{n}\\\\overbrace{i^{\\\\prime}}^{n}\\\\overbrace{i^{\\\\prime}}^{n\\\\\\\\ {{\\\\left(\\\\sum_{i=1}^{n+i}\\\\left(\\\\sum_{i=1}^{n}\\\\overbrace{i^{\\\\prime}}^{n}\\\\overbrace{i^{\\\\prime}}^{n}\\\\right)^{\\\\prime\\\\end{array} $$\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n[image]\\n\\n\\n\\n$$ \\\\ _{\\\\frac{\\\\beta}{\\\\Lambda_{\\\\mathrm{S} $$\\n\\n\\n\\n[image]']\n"
     ]
    }
   ],
   "source": [
    "from summarizer import *\n",
    "chunks_context = chunks_context(text,300)\n",
    "print(chunks_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "resume_chunked_context() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m texte\u001b[38;5;241m=\u001b[39m\u001b[43mresume_chunked_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(texte)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texte))\n",
      "\u001b[0;31mTypeError\u001b[0m: resume_chunked_context() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "texte=resume_chunked_context(chunks_context,tokenizer,model=model)\n",
    "print(texte)\n",
    "print(len(texte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " 'Introduction',\n",
       " 'to',\n",
       " 'Reinforcement',\n",
       " 'Learning.',\n",
       " 'An',\n",
       " 'Introduction',\n",
       " 'to',\n",
       " 'Markov',\n",
       " 'Decision',\n",
       " 'Processes.',\n",
       " 'Policy',\n",
       " 'and',\n",
       " 'Value',\n",
       " 'Functions.',\n",
       " 'Dynamic',\n",
       " 'Programming',\n",
       " '(DP)',\n",
       " 'for',\n",
       " 'RL.',\n",
       " 'Deep',\n",
       " 'RL.',\n",
       " 'TP',\n",
       " 'Project:',\n",
       " 'Reinforcement',\n",
       " 'Learning',\n",
       " '(RL)',\n",
       " 'The',\n",
       " 'project',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'minimize',\n",
       " 'the',\n",
       " 'error',\n",
       " 'between',\n",
       " 'the',\n",
       " 'target',\n",
       " 'and',\n",
       " 'the',\n",
       " 'predicted',\n",
       " 'output',\n",
       " 'of',\n",
       " 'the',\n",
       " 'training.',\n",
       " 'The',\n",
       " 'project',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'R-theory',\n",
       " 'of',\n",
       " 'reinforcement',\n",
       " 'learning.',\n",
       " 'Partially',\n",
       " 'Observable',\n",
       " 'Markov',\n",
       " 'Decision',\n",
       " 'Process',\n",
       " '(POMDP)(Missing',\n",
       " 'info)Reinforcement',\n",
       " 'Learning',\n",
       " '(RL)',\n",
       " 'Agent:',\n",
       " 'The',\n",
       " 'system',\n",
       " 'that',\n",
       " 'takes',\n",
       " 'the',\n",
       " 'actions',\n",
       " 'to',\n",
       " 'be',\n",
       " 'trained.',\n",
       " 'Environment:',\n",
       " 'The',\n",
       " 'external',\n",
       " 'system',\n",
       " 'with',\n",
       " 'which',\n",
       " 'the',\n",
       " 'agent',\n",
       " 'interacts.',\n",
       " 'State:',\n",
       " 'The',\n",
       " 'information',\n",
       " 'required',\n",
       " 'by',\n",
       " 'the',\n",
       " 'agent',\n",
       " 'to',\n",
       " 'take',\n",
       " 'an',\n",
       " 'action.',\n",
       " 'Reward:',\n",
       " 'Feedback',\n",
       " 'received',\n",
       " 'from',\n",
       " 'the',\n",
       " 'environment',\n",
       " 'to',\n",
       " 'evaluate',\n",
       " 'action.',\n",
       " 'It',\n",
       " 'represents',\n",
       " 'a',\n",
       " 'mapping',\n",
       " 'from',\n",
       " 'states',\n",
       " 'to',\n",
       " 'actions,',\n",
       " 'for',\n",
       " 'example:.',\n",
       " 'The',\n",
       " 'state',\n",
       " 'value',\n",
       " 'under',\n",
       " 'policy',\n",
       " '𝜋',\n",
       " 'is',\n",
       " 'given',\n",
       " 'by:.',\n",
       " 'This',\n",
       " 'equation',\n",
       " 'expresses',\n",
       " 'the',\n",
       " 'expected',\n",
       " 'sum',\n",
       " 'of',\n",
       " 'discounted',\n",
       " 'rewards',\n",
       " 'starting',\n",
       " 'from',\n",
       " '.',\n",
       " 'state',\n",
       " '𝑠.',\n",
       " 'For',\n",
       " 'example,',\n",
       " 'the',\n",
       " 'state',\n",
       " 'value',\n",
       " 'of',\n",
       " 'is',\n",
       " '‘’”“”,',\n",
       " 'where',\n",
       " '‘',\n",
       " '’',\n",
       " 'is',\n",
       " 'the',\n",
       " 'state’s',\n",
       " 'value.”.',\n",
       " '20γ∉[0,1]:774.',\n",
       " 'If',\n",
       " 'γ=0,',\n",
       " 'the',\n",
       " 'agent',\n",
       " 'focuses',\n",
       " 'solely',\n",
       " 'on',\n",
       " 'immediate',\n",
       " 'rewards.',\n",
       " 'If',\n",
       " 'γ=1,',\n",
       " 'future',\n",
       " 'rewards',\n",
       " 'are',\n",
       " 'valued',\n",
       " 'equally',\n",
       " 'to',\n",
       " 'immediate',\n",
       " 'reward.',\n",
       " 'A',\n",
       " 'model',\n",
       " 'forecasts',\n",
       " 'the',\n",
       " \"environment's\",\n",
       " 'next',\n",
       " 'state',\n",
       " 'and',\n",
       " 'expected',\n",
       " 'reward.',\n",
       " 'Markov',\n",
       " 'Process',\n",
       " 'is',\n",
       " 'a',\n",
       " 'memoryless',\n",
       " 'process',\n",
       " 'where',\n",
       " 'the',\n",
       " 'future',\n",
       " 'state',\n",
       " 'depends',\n",
       " 'only',\n",
       " 'on',\n",
       " 'the',\n",
       " 'current',\n",
       " 'state',\n",
       " 'and',\n",
       " 'not',\n",
       " 'on',\n",
       " 'any',\n",
       " 'past',\n",
       " 'states.',\n",
       " 'A',\n",
       " 'Markov',\n",
       " 'Reward',\n",
       " 'Process',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Markov',\n",
       " 'Process',\n",
       " 'with',\n",
       " 'added',\n",
       " 'rewards.',\n",
       " 'Markov',\n",
       " 'Property:',\n",
       " 'Future',\n",
       " 'depends',\n",
       " 'only',\n",
       " 'on',\n",
       " 'the',\n",
       " 'present,',\n",
       " 'not',\n",
       " 'past',\n",
       " 'states.',\n",
       " 'State-value',\n",
       " 'function',\n",
       " 'can',\n",
       " 'be',\n",
       " 'presented',\n",
       " 'as',\n",
       " 'an',\n",
       " 'immediate',\n",
       " 'reward',\n",
       " 'and',\n",
       " 'future',\n",
       " 'reward.',\n",
       " 'A',\n",
       " 'Markov',\n",
       " 'decision',\n",
       " 'process',\n",
       " '(MDP)',\n",
       " 'is',\n",
       " 'a',\n",
       " '4-tuple',\n",
       " '(S,',\n",
       " 'A,',\n",
       " 'P,',\n",
       " 'R)',\n",
       " 'with',\n",
       " 'state,',\n",
       " 'action,',\n",
       " 'and',\n",
       " 'reward',\n",
       " 'sets.',\n",
       " 'Much',\n",
       " 'of',\n",
       " 'the',\n",
       " 'current',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'reinforcement',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'restricted',\n",
       " 'to',\n",
       " 'ﬁnite',\n",
       " 'MDPs.',\n",
       " 'Actions',\n",
       " '(A):',\n",
       " 'Choices',\n",
       " 'available',\n",
       " 'to',\n",
       " 'the',\n",
       " 'agent;',\n",
       " 'Rewards',\n",
       " '(R):',\n",
       " 'Immediate',\n",
       " 'feedback',\n",
       " 'for',\n",
       " 'actions;',\n",
       " 'Transition',\n",
       " 'Probabilities',\n",
       " '(P):',\n",
       " 'Likelihood',\n",
       " 'of',\n",
       " 'reaching',\n",
       " 'a',\n",
       " 'new',\n",
       " 'state.',\n",
       " 'Policy',\n",
       " '(P)',\n",
       " 'is',\n",
       " 'the',\n",
       " 'policy',\n",
       " 'that',\n",
       " 'gives',\n",
       " 'the',\n",
       " 'probability',\n",
       " 'of',\n",
       " 'taking',\n",
       " 'action',\n",
       " 'a',\n",
       " 'given',\n",
       " 'state',\n",
       " 's.',\n",
       " 'For',\n",
       " 'any',\n",
       " 'MDP,',\n",
       " 'there',\n",
       " 'is',\n",
       " 'always',\n",
       " 'a',\n",
       " 'deterministic',\n",
       " 'optimal',\n",
       " 'policy.',\n",
       " 'An',\n",
       " 'optimal',\n",
       " 'policy',\n",
       " 'can',\n",
       " 'be',\n",
       " 'determined',\n",
       " 'by',\n",
       " 'selecting',\n",
       " 'actions',\n",
       " 'thatmaximize',\n",
       " 'the',\n",
       " 'optimal',\n",
       " 'action-value',\n",
       " 'function',\n",
       " 'q∗(s,a)',\n",
       " 'Consider',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'MDP',\n",
       " 'with',\n",
       " 'two',\n",
       " 'states',\n",
       " 's1',\n",
       " 'and',\n",
       " 's2',\n",
       " 'and',\n",
       " 'a',\n",
       " 'single',\n",
       " 'action',\n",
       " 'a',\n",
       " 'with',\n",
       " 'the',\n",
       " 'following',\n",
       " 'reward',\n",
       " 'structure.',\n",
       " 'The',\n",
       " 'Bellman',\n",
       " 'equation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'each',\n",
       " 'state',\n",
       " 's',\n",
       " 'is:.',\n",
       " 'For',\n",
       " 's2',\n",
       " ':.Solving',\n",
       " 'for',\n",
       " 'v(s2',\n",
       " ')',\n",
       " '→',\n",
       " 'v',\n",
       " '(s2)',\n",
       " '=',\n",
       " '30.',\n",
       " 'Using',\n",
       " 'the',\n",
       " 'same',\n",
       " 'MDP',\n",
       " 'setup',\n",
       " 'as',\n",
       " 'in',\n",
       " 'Exercise',\n",
       " '2,',\n",
       " 'calculate',\n",
       " 'the',\n",
       " 'action-value',\n",
       " 'q(s1',\n",
       " ',a)',\n",
       " 'for',\n",
       " 'each',\n",
       " 'state-action',\n",
       " 'pair.',\n",
       " 'Assuming',\n",
       " 'a',\n",
       " 'discount',\n",
       " 'factor',\n",
       " 'γ=0.9,',\n",
       " 'write',\n",
       " 'the',\n",
       " 'Bellman',\n",
       " 'optimality',\n",
       " 'equation',\n",
       " 'for',\n",
       " 'v∗(',\n",
       " 's1',\n",
       " ').',\n",
       " 'The',\n",
       " 'Bellman',\n",
       " 'optimality',\n",
       " 'equation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'state-value',\n",
       " 'function',\n",
       " 'is:.Substituting',\n",
       " 'the',\n",
       " 'rewards:.',\n",
       " 'The',\n",
       " 'optimal',\n",
       " 'values',\n",
       " 'for',\n",
       " 'each',\n",
       " 'state',\n",
       " 'are:',\n",
       " '32.8',\n",
       " 'and',\n",
       " '30.',\n",
       " 'The',\n",
       " 'optimal',\n",
       " 'policy',\n",
       " 'is',\n",
       " 'to',\n",
       " 'always',\n",
       " 'choose',\n",
       " 'action',\n",
       " 'a1',\n",
       " 'in',\n",
       " 'state',\n",
       " 's,',\n",
       " 'since',\n",
       " 'q∗(s,a1',\n",
       " ')>q∗',\n",
       " '(',\n",
       " 's,a2',\n",
       " '),',\n",
       " '71.',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'is',\n",
       " 'to',\n",
       " 'balance',\n",
       " 'exploration',\n",
       " 'and',\n",
       " 'exploitation',\n",
       " 'to',\n",
       " 'maximize',\n",
       " 'rewards',\n",
       " 'over',\n",
       " 'time.',\n",
       " 'Exploitation:',\n",
       " 'Leveraging',\n",
       " 'Known',\n",
       " 'Information',\n",
       " 'and',\n",
       " 'Leveraging',\n",
       " 'Information.',\n",
       " 'Exploitation',\n",
       " 'capitalizes',\n",
       " 'on',\n",
       " 'known',\n",
       " 'successes,',\n",
       " 'ensuring',\n",
       " 'steady',\n",
       " 'rewards.',\n",
       " 'A',\n",
       " 'game-playing',\n",
       " 'AI:Repeats',\n",
       " 'a',\n",
       " 'high-reward',\n",
       " 'move',\n",
       " 'that',\n",
       " 'has',\n",
       " 'led',\n",
       " 'to',\n",
       " 'victories',\n",
       " 'in',\n",
       " 'past',\n",
       " 'games.',\n",
       " 'Olivier',\n",
       " 'Sigaud’s',\n",
       " 'Youtube',\n",
       " 'Channel',\n",
       " 'is',\n",
       " 'at:',\n",
       " 'http://www.livier-sigaud.com/.',\n",
       " 'His',\n",
       " 'Facebook',\n",
       " 'page',\n",
       " 'is:',\n",
       " 'http:www.facebook.com/livier-',\n",
       " 'Sigaud.',\n",
       " 'His',\n",
       " 'Twitter',\n",
       " 'account',\n",
       " 'is:',\n",
       " '@LivierSigaud,',\n",
       " 'and',\n",
       " 'he',\n",
       " 'has',\n",
       " 'a',\n",
       " 'YouTube',\n",
       " 'channel',\n",
       " 'at:',\n",
       " 'www.llivier.com.',\n",
       " '[image]',\n",
       " 'U.S.',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'following',\n",
       " 'terms:',\n",
       " 'v.v.v_{\\\\pi}(s)',\n",
       " 'and',\n",
       " 'v.',\n",
       " 'v.',\n",
       " 'U.',\n",
       " 'v',\n",
       " '(s,',\n",
       " 's,',\n",
       " 's)',\n",
       " 'to',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'words.',\n",
       " '$$',\n",
       " 'rejects',\n",
       " 'rejected',\n",
       " 'rejection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'word',\n",
       " '“reject’s”',\n",
       " 'and',\n",
       " '”rejected’',\n",
       " 'with',\n",
       " 'the',\n",
       " 'word',\n",
       " '“rejection’’.',\n",
       " '$$',\n",
       " '\\\\begin{array}{c}{{q_{\\\\pi}(s,a)}\\\\stackrel{\\\\leftarrow}{\\\\leftarrow}s,',\n",
       " 'a',\n",
       " '’pi’sqrt{\\\\bigotimes’,',\n",
       " '‘v’(s),',\n",
       " '‘s,’',\n",
       " '‘a’;',\n",
       " '‘b’:',\n",
       " '‘V’',\n",
       " '(s,',\n",
       " 'v),',\n",
       " '’c’',\n",
       " ':',\n",
       " '‘S’.',\n",
       " '‘p’',\n",
       " '=',\n",
       " '“S”;',\n",
       " '“p”',\n",
       " ':',\n",
       " '“V”:',\n",
       " '“P”']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texte.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(    texte.split()    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
